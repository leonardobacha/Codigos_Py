{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "#rede_neural = MLPClassifier(max_iter=1000, verbose = True, tol = 0.0001, hidden_layer_sizes = (2,2))\n",
    "#rede_neural.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['A', 'D', 'A', 'A', 'C', 'C', 'A', 'A', 'C', 'B', 'C', 'C', 'A',\n",
       "       'D', 'A', 'A', 'C', 'A', 'A', 'A', 'A', 'D', 'A', 'A', 'C', 'C',\n",
       "       'A', 'A', 'C', 'A', 'B', 'D', 'C', 'A', 'C', 'D', 'A', 'A', 'B',\n",
       "       'A', 'A', 'A', 'C', 'A', 'C', 'A', 'C', 'A', 'A', 'A', 'B', 'A',\n",
       "       'D', 'A', 'A', 'A', 'A', 'A', 'A', 'B', 'A', 'A', 'A', 'A', 'A',\n",
       "       'A', 'C', 'A', 'B', 'D', 'C', 'D', 'B', 'A', 'A', 'A', 'A', 'A',\n",
       "       'A', 'D', 'A', 'A', 'A', 'A', 'C', 'B', 'D', 'A', 'A', 'A', 'D',\n",
       "       'A'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_excel(\"Base_de_dados_IBP.xlsx\",sheet_name = \"classie\") #Regressão,ensemble,pycaret\n",
    "#dataset = litoral.drop('Município',axis=1)\n",
    "array = dataset.values\n",
    "X = array[:, 1:4].astype(float)\n",
    "Y = array[:, 4]\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separação em conjuntos de treino e teste\n",
    "\n",
    "test_size = 0.20\n",
    "seed = 7\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X, Y, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parâmetros\n",
    "num_folds = 10\n",
    "scoring = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criação dos modelos\n",
    "models = []\n",
    "#models.append(('LR', LogisticRegression(solver='newton-cg')))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "models.append(('MLP', MLPClassifier(max_iter=1000, verbose = True, tol = 0.0001, hidden_layer_sizes = (2,2))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN: 0.505357 (0.169229)\n",
      "CART: 0.367857 (0.173352)\n",
      "NB: 0.223214 (0.118518)\n",
      "SVM: 0.558929 (0.231875)\n",
      "Iteration 1, loss = 16.08102095\n",
      "Iteration 2, loss = 16.08102094\n",
      "Iteration 3, loss = 16.08102094\n",
      "Iteration 4, loss = 16.08102093\n",
      "Iteration 5, loss = 16.08102093\n",
      "Iteration 6, loss = 16.08102093\n",
      "Iteration 7, loss = 16.08102092\n",
      "Iteration 8, loss = 16.08102092\n",
      "Iteration 9, loss = 16.08102091\n",
      "Iteration 10, loss = 16.08102091\n",
      "Iteration 11, loss = 16.08102091\n",
      "Iteration 12, loss = 16.08102090\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 2.01764662\n",
      "Iteration 2, loss = 2.01634261\n",
      "Iteration 3, loss = 2.01503755\n",
      "Iteration 4, loss = 2.01373142\n",
      "Iteration 5, loss = 2.01242422\n",
      "Iteration 6, loss = 2.01111594\n",
      "Iteration 7, loss = 2.00980656\n",
      "Iteration 8, loss = 2.00849606\n",
      "Iteration 9, loss = 2.00718444\n",
      "Iteration 10, loss = 2.00587167\n",
      "Iteration 11, loss = 2.00455774\n",
      "Iteration 12, loss = 2.00324262\n",
      "Iteration 13, loss = 2.00192631\n",
      "Iteration 14, loss = 2.00060879\n",
      "Iteration 15, loss = 1.99929003\n",
      "Iteration 16, loss = 1.99797002\n",
      "Iteration 17, loss = 1.99664874\n",
      "Iteration 18, loss = 1.99532617\n",
      "Iteration 19, loss = 1.99400229\n",
      "Iteration 20, loss = 1.99267709\n",
      "Iteration 21, loss = 1.99135054\n",
      "Iteration 22, loss = 1.99002264\n",
      "Iteration 23, loss = 1.98869335\n",
      "Iteration 24, loss = 1.98736267\n",
      "Iteration 25, loss = 1.98603057\n",
      "Iteration 26, loss = 1.98469704\n",
      "Iteration 27, loss = 1.98336207\n",
      "Iteration 28, loss = 1.98202564\n",
      "Iteration 29, loss = 1.98068772\n",
      "Iteration 30, loss = 1.97934832\n",
      "Iteration 31, loss = 1.97800741\n",
      "Iteration 32, loss = 1.97666498\n",
      "Iteration 33, loss = 1.97532102\n",
      "Iteration 34, loss = 1.97397551\n",
      "Iteration 35, loss = 1.97262844\n",
      "Iteration 36, loss = 1.97127980\n",
      "Iteration 37, loss = 1.96992958\n",
      "Iteration 38, loss = 1.96857777\n",
      "Iteration 39, loss = 1.96722435\n",
      "Iteration 40, loss = 1.96586932\n",
      "Iteration 41, loss = 1.96451268\n",
      "Iteration 42, loss = 1.96315440\n",
      "Iteration 43, loss = 1.96179449\n",
      "Iteration 44, loss = 1.96043293\n",
      "Iteration 45, loss = 1.95906972\n",
      "Iteration 46, loss = 1.95770486\n",
      "Iteration 47, loss = 1.95633833\n",
      "Iteration 48, loss = 1.95497013\n",
      "Iteration 49, loss = 1.95360025\n",
      "Iteration 50, loss = 1.95222870\n",
      "Iteration 51, loss = 1.95085547\n",
      "Iteration 52, loss = 1.94948055\n",
      "Iteration 53, loss = 1.94810394\n",
      "Iteration 54, loss = 1.94672564\n",
      "Iteration 55, loss = 1.94534565\n",
      "Iteration 56, loss = 1.94396397\n",
      "Iteration 57, loss = 1.94258059\n",
      "Iteration 58, loss = 1.94119551\n",
      "Iteration 59, loss = 1.93980873\n",
      "Iteration 60, loss = 1.93842026\n",
      "Iteration 61, loss = 1.93703010\n",
      "Iteration 62, loss = 1.93563823\n",
      "Iteration 63, loss = 1.93424468\n",
      "Iteration 64, loss = 1.93284943\n",
      "Iteration 65, loss = 1.93145249\n",
      "Iteration 66, loss = 1.93005387\n",
      "Iteration 67, loss = 1.92865355\n",
      "Iteration 68, loss = 1.92725156\n",
      "Iteration 69, loss = 1.92584789\n",
      "Iteration 70, loss = 1.92444255\n",
      "Iteration 71, loss = 1.92303553\n",
      "Iteration 72, loss = 1.92162685\n",
      "Iteration 73, loss = 1.92021651\n",
      "Iteration 74, loss = 1.91880451\n",
      "Iteration 75, loss = 1.91739086\n",
      "Iteration 76, loss = 1.91597557\n",
      "Iteration 77, loss = 1.91455864\n",
      "Iteration 78, loss = 1.91314008\n",
      "Iteration 79, loss = 1.91171989\n",
      "Iteration 80, loss = 1.91029809\n",
      "Iteration 81, loss = 1.90887467\n",
      "Iteration 82, loss = 1.90744965\n",
      "Iteration 83, loss = 1.90602304\n",
      "Iteration 84, loss = 1.90459484\n",
      "Iteration 85, loss = 1.90316506\n",
      "Iteration 86, loss = 1.90173371\n",
      "Iteration 87, loss = 1.90030080\n",
      "Iteration 88, loss = 1.89886634\n",
      "Iteration 89, loss = 1.89743033\n",
      "Iteration 90, loss = 1.89599279\n",
      "Iteration 91, loss = 1.89455373\n",
      "Iteration 92, loss = 1.89311316\n",
      "Iteration 93, loss = 1.89167108\n",
      "Iteration 94, loss = 1.89022751\n",
      "Iteration 95, loss = 1.88878246\n",
      "Iteration 96, loss = 1.88733594\n",
      "Iteration 97, loss = 1.88588797\n",
      "Iteration 98, loss = 1.88443854\n",
      "Iteration 99, loss = 1.88298768\n",
      "Iteration 100, loss = 1.88153539\n",
      "Iteration 101, loss = 1.88008170\n",
      "Iteration 102, loss = 1.87862660\n",
      "Iteration 103, loss = 1.87717012\n",
      "Iteration 104, loss = 1.87571226\n",
      "Iteration 105, loss = 1.87425304\n",
      "Iteration 106, loss = 1.87279248\n",
      "Iteration 107, loss = 1.87133057\n",
      "Iteration 108, loss = 1.86986735\n",
      "Iteration 109, loss = 1.86840282\n",
      "Iteration 110, loss = 1.86693699\n",
      "Iteration 111, loss = 1.86546988\n",
      "Iteration 112, loss = 1.86400151\n",
      "Iteration 113, loss = 1.86253188\n",
      "Iteration 114, loss = 1.86106101\n",
      "Iteration 115, loss = 1.85958892\n",
      "Iteration 116, loss = 1.85811563\n",
      "Iteration 117, loss = 1.85664113\n",
      "Iteration 118, loss = 1.85516546\n",
      "Iteration 119, loss = 1.85368862\n",
      "Iteration 120, loss = 1.85221063\n",
      "Iteration 121, loss = 1.85073151\n",
      "Iteration 122, loss = 1.84925127\n",
      "Iteration 123, loss = 1.84776993\n",
      "Iteration 124, loss = 1.84628750\n",
      "Iteration 125, loss = 1.84480399\n",
      "Iteration 126, loss = 1.84331943\n",
      "Iteration 127, loss = 1.84183383\n",
      "Iteration 128, loss = 1.84034720\n",
      "Iteration 129, loss = 1.83885957\n",
      "Iteration 130, loss = 1.83737094\n",
      "Iteration 131, loss = 1.83588134\n",
      "Iteration 132, loss = 1.83439078\n",
      "Iteration 133, loss = 1.83289927\n",
      "Iteration 134, loss = 1.83140684\n",
      "Iteration 135, loss = 1.82991349\n",
      "Iteration 136, loss = 1.82841925\n",
      "Iteration 137, loss = 1.82692414\n",
      "Iteration 138, loss = 1.82542816\n",
      "Iteration 139, loss = 1.82393134\n",
      "Iteration 140, loss = 1.82243370\n",
      "Iteration 141, loss = 1.82093524\n",
      "Iteration 142, loss = 1.81943599\n",
      "Iteration 143, loss = 1.81793596\n",
      "Iteration 144, loss = 1.81643518\n",
      "Iteration 145, loss = 1.81493365\n",
      "Iteration 146, loss = 1.81343140\n",
      "Iteration 147, loss = 1.81192843\n",
      "Iteration 148, loss = 1.81042478\n",
      "Iteration 149, loss = 1.80892045\n",
      "Iteration 150, loss = 1.80741546\n",
      "Iteration 151, loss = 1.80590984\n",
      "Iteration 152, loss = 1.80440359\n",
      "Iteration 153, loss = 1.80289673\n",
      "Iteration 154, loss = 1.80138929\n",
      "Iteration 155, loss = 1.79988127\n",
      "Iteration 156, loss = 1.79837269\n",
      "Iteration 157, loss = 1.79686358\n",
      "Iteration 158, loss = 1.79535395\n",
      "Iteration 159, loss = 1.79384381\n",
      "Iteration 160, loss = 1.79233318\n",
      "Iteration 161, loss = 1.79082209\n",
      "Iteration 162, loss = 1.78931054\n",
      "Iteration 163, loss = 1.78779855\n",
      "Iteration 164, loss = 1.78628614\n",
      "Iteration 165, loss = 1.78477333\n",
      "Iteration 166, loss = 1.78326013\n",
      "Iteration 167, loss = 1.78174656\n",
      "Iteration 168, loss = 1.78023263\n",
      "Iteration 169, loss = 1.77871837\n",
      "Iteration 170, loss = 1.77720378\n",
      "Iteration 171, loss = 1.77568889\n",
      "Iteration 172, loss = 1.77417371\n",
      "Iteration 173, loss = 1.77265825\n",
      "Iteration 174, loss = 1.77114254\n",
      "Iteration 175, loss = 1.76962659\n",
      "Iteration 176, loss = 1.76811041\n",
      "Iteration 177, loss = 1.76659402\n",
      "Iteration 178, loss = 1.76507744\n",
      "Iteration 179, loss = 1.76356067\n",
      "Iteration 180, loss = 1.76204375\n",
      "Iteration 181, loss = 1.76052667\n",
      "Iteration 182, loss = 1.75900946\n",
      "Iteration 183, loss = 1.75749213\n",
      "Iteration 184, loss = 1.75597470\n",
      "Iteration 185, loss = 1.75445718\n",
      "Iteration 186, loss = 1.75293959\n",
      "Iteration 187, loss = 1.75142193\n",
      "Iteration 188, loss = 1.74990423\n",
      "Iteration 189, loss = 1.74838650\n",
      "Iteration 190, loss = 1.74686875\n",
      "Iteration 191, loss = 1.74535100\n",
      "Iteration 192, loss = 1.74383325\n",
      "Iteration 193, loss = 1.74231553\n",
      "Iteration 194, loss = 1.74079785\n",
      "Iteration 195, loss = 1.73928022\n",
      "Iteration 196, loss = 1.73776265\n",
      "Iteration 197, loss = 1.73624515\n",
      "Iteration 198, loss = 1.73472774\n",
      "Iteration 199, loss = 1.73321044\n",
      "Iteration 200, loss = 1.73169324\n",
      "Iteration 201, loss = 1.73017618\n",
      "Iteration 202, loss = 1.72865924\n",
      "Iteration 203, loss = 1.72714246\n",
      "Iteration 204, loss = 1.72562584\n",
      "Iteration 205, loss = 1.72410939\n",
      "Iteration 206, loss = 1.72259312\n",
      "Iteration 207, loss = 1.72107704\n",
      "Iteration 208, loss = 1.71956117\n",
      "Iteration 209, loss = 1.71804551\n",
      "Iteration 210, loss = 1.71653008\n",
      "Iteration 211, loss = 1.71501488\n",
      "Iteration 212, loss = 1.71349993\n",
      "Iteration 213, loss = 1.71198523\n",
      "Iteration 214, loss = 1.71047079\n",
      "Iteration 215, loss = 1.70895663\n",
      "Iteration 216, loss = 1.70744275\n",
      "Iteration 217, loss = 1.70592915\n",
      "Iteration 218, loss = 1.70441586\n",
      "Iteration 219, loss = 1.70290287\n",
      "Iteration 220, loss = 1.70139020\n",
      "Iteration 221, loss = 1.69987785\n",
      "Iteration 222, loss = 1.69836583\n",
      "Iteration 223, loss = 1.69685415\n",
      "Iteration 224, loss = 1.69534281\n",
      "Iteration 225, loss = 1.69383182\n",
      "Iteration 226, loss = 1.69232119\n",
      "Iteration 227, loss = 1.69081093\n",
      "Iteration 228, loss = 1.68930103\n",
      "Iteration 229, loss = 1.68779151\n",
      "Iteration 230, loss = 1.68628237\n",
      "Iteration 231, loss = 1.68477362\n",
      "Iteration 232, loss = 1.68326526\n",
      "Iteration 233, loss = 1.68175729\n",
      "Iteration 234, loss = 1.68024973\n",
      "Iteration 235, loss = 1.67874257\n",
      "Iteration 236, loss = 1.67723582\n",
      "Iteration 237, loss = 1.67572949\n",
      "Iteration 238, loss = 1.67422357\n",
      "Iteration 239, loss = 1.67271808\n",
      "Iteration 240, loss = 1.67121301\n",
      "Iteration 241, loss = 1.66970837\n",
      "Iteration 242, loss = 1.66820416\n",
      "Iteration 243, loss = 1.66670038\n",
      "Iteration 244, loss = 1.66519703\n",
      "Iteration 245, loss = 1.66369413\n",
      "Iteration 246, loss = 1.66219166\n",
      "Iteration 247, loss = 1.66068964\n",
      "Iteration 248, loss = 1.65918805\n",
      "Iteration 249, loss = 1.65768692\n",
      "Iteration 250, loss = 1.65618623\n",
      "Iteration 251, loss = 1.65468598\n",
      "Iteration 252, loss = 1.65318618\n",
      "Iteration 253, loss = 1.65168684\n",
      "Iteration 254, loss = 1.65018794\n",
      "Iteration 255, loss = 1.64868949\n",
      "Iteration 256, loss = 1.64719149\n",
      "Iteration 257, loss = 1.64569395\n",
      "Iteration 258, loss = 1.64419685\n",
      "Iteration 259, loss = 1.64270020\n",
      "Iteration 260, loss = 1.64120401\n",
      "Iteration 261, loss = 1.63970826\n",
      "Iteration 262, loss = 1.63821296\n",
      "Iteration 263, loss = 1.63671812\n",
      "Iteration 264, loss = 1.63522372\n",
      "Iteration 265, loss = 1.63372977\n",
      "Iteration 266, loss = 1.63223627\n",
      "Iteration 267, loss = 1.63074322\n",
      "Iteration 268, loss = 1.62925061\n",
      "Iteration 269, loss = 1.62775845\n",
      "Iteration 270, loss = 1.62626673\n",
      "Iteration 271, loss = 1.62477546\n",
      "Iteration 272, loss = 1.62328464\n",
      "Iteration 273, loss = 1.62179425\n",
      "Iteration 274, loss = 1.62030432\n",
      "Iteration 275, loss = 1.61881482\n",
      "Iteration 276, loss = 1.61732577\n",
      "Iteration 277, loss = 1.61583715\n",
      "Iteration 278, loss = 1.61434898\n",
      "Iteration 279, loss = 1.61286125\n",
      "Iteration 280, loss = 1.61137397\n",
      "Iteration 281, loss = 1.60988712\n",
      "Iteration 282, loss = 1.60840072\n",
      "Iteration 283, loss = 1.60691476\n",
      "Iteration 284, loss = 1.60542924\n",
      "Iteration 285, loss = 1.60394417\n",
      "Iteration 286, loss = 1.60245954\n",
      "Iteration 287, loss = 1.60097535\n",
      "Iteration 288, loss = 1.59949161\n",
      "Iteration 289, loss = 1.59800832\n",
      "Iteration 290, loss = 1.59652548\n",
      "Iteration 291, loss = 1.59504310\n",
      "Iteration 292, loss = 1.59356116\n",
      "Iteration 293, loss = 1.59207969\n",
      "Iteration 294, loss = 1.59059867\n",
      "Iteration 295, loss = 1.58911812\n",
      "Iteration 296, loss = 1.58763803\n",
      "Iteration 297, loss = 1.58615841\n",
      "Iteration 298, loss = 1.58467926\n",
      "Iteration 299, loss = 1.58320059\n",
      "Iteration 300, loss = 1.58172240\n",
      "Iteration 301, loss = 1.58024470\n",
      "Iteration 302, loss = 1.57876749\n",
      "Iteration 303, loss = 1.57729078\n",
      "Iteration 304, loss = 1.57581456\n",
      "Iteration 305, loss = 1.57433886\n",
      "Iteration 306, loss = 1.57286367\n",
      "Iteration 307, loss = 1.57138901\n",
      "Iteration 308, loss = 1.56991487\n",
      "Iteration 309, loss = 1.56844127\n",
      "Iteration 310, loss = 1.56696821\n",
      "Iteration 311, loss = 1.56549570\n",
      "Iteration 312, loss = 1.56402375\n",
      "Iteration 313, loss = 1.56255237\n",
      "Iteration 314, loss = 1.56108157\n",
      "Iteration 315, loss = 1.55961136\n",
      "Iteration 316, loss = 1.55814175\n",
      "Iteration 317, loss = 1.55667274\n",
      "Iteration 318, loss = 1.55520435\n",
      "Iteration 319, loss = 1.55373658\n",
      "Iteration 320, loss = 1.55226946\n",
      "Iteration 321, loss = 1.55080299\n",
      "Iteration 322, loss = 1.54933718\n",
      "Iteration 323, loss = 1.54787205\n",
      "Iteration 324, loss = 1.54640760\n",
      "Iteration 325, loss = 1.54494386\n",
      "Iteration 326, loss = 1.54348082\n",
      "Iteration 327, loss = 1.54201852\n",
      "Iteration 328, loss = 1.54055696\n",
      "Iteration 329, loss = 1.53909615\n",
      "Iteration 330, loss = 1.53763611\n",
      "Iteration 331, loss = 1.53617685\n",
      "Iteration 332, loss = 1.53471839\n",
      "Iteration 333, loss = 1.53326075\n",
      "Iteration 334, loss = 1.53180393\n",
      "Iteration 335, loss = 1.53034796\n",
      "Iteration 336, loss = 1.52889285\n",
      "Iteration 337, loss = 1.52743862\n",
      "Iteration 338, loss = 1.52598529\n",
      "Iteration 339, loss = 1.52453286\n",
      "Iteration 340, loss = 1.52308137\n",
      "Iteration 341, loss = 1.52163082\n",
      "Iteration 342, loss = 1.52018123\n",
      "Iteration 343, loss = 1.51873263\n",
      "Iteration 344, loss = 1.51728502\n",
      "Iteration 345, loss = 1.51583843\n",
      "Iteration 346, loss = 1.51439288\n",
      "Iteration 347, loss = 1.51294838\n",
      "Iteration 348, loss = 1.51150496\n",
      "Iteration 349, loss = 1.51006263\n",
      "Iteration 350, loss = 1.50862142\n",
      "Iteration 351, loss = 1.50718133\n",
      "Iteration 352, loss = 1.50574240\n",
      "Iteration 353, loss = 1.50430464\n",
      "Iteration 354, loss = 1.50286807\n",
      "Iteration 355, loss = 1.50143272\n",
      "Iteration 356, loss = 1.49999860\n",
      "Iteration 357, loss = 1.49856573\n",
      "Iteration 358, loss = 1.49713414\n",
      "Iteration 359, loss = 1.49570384\n",
      "Iteration 360, loss = 1.49427486\n",
      "Iteration 361, loss = 1.49284722\n",
      "Iteration 362, loss = 1.49142093\n",
      "Iteration 363, loss = 1.48999603\n",
      "Iteration 364, loss = 1.48857253\n",
      "Iteration 365, loss = 1.48715046\n",
      "Iteration 366, loss = 1.48572983\n",
      "Iteration 367, loss = 1.48431067\n",
      "Iteration 368, loss = 1.48289300\n",
      "Iteration 369, loss = 1.48147685\n",
      "Iteration 370, loss = 1.48006223\n",
      "Iteration 371, loss = 1.47864917\n",
      "Iteration 372, loss = 1.47723769\n",
      "Iteration 373, loss = 1.47582781\n",
      "Iteration 374, loss = 1.47441956\n",
      "Iteration 375, loss = 1.47301296\n",
      "Iteration 376, loss = 1.47160803\n",
      "Iteration 377, loss = 1.47020480\n",
      "Iteration 378, loss = 1.46880328\n",
      "Iteration 379, loss = 1.46740351\n",
      "Iteration 380, loss = 1.46600550\n",
      "Iteration 381, loss = 1.46460928\n",
      "Iteration 382, loss = 1.46321487\n",
      "Iteration 383, loss = 1.46182229\n",
      "Iteration 384, loss = 1.46043158\n",
      "Iteration 385, loss = 1.45904274\n",
      "Iteration 386, loss = 1.45765581\n",
      "Iteration 387, loss = 1.45627081\n",
      "Iteration 388, loss = 1.45488776\n",
      "Iteration 389, loss = 1.45350669\n",
      "Iteration 390, loss = 1.45212762\n",
      "Iteration 391, loss = 1.45075057\n",
      "Iteration 392, loss = 1.44937557\n",
      "Iteration 393, loss = 1.44800264\n",
      "Iteration 394, loss = 1.44663181\n",
      "Iteration 395, loss = 1.44526309\n",
      "Iteration 396, loss = 1.44389652\n",
      "Iteration 397, loss = 1.44253211\n",
      "Iteration 398, loss = 1.44116989\n",
      "Iteration 399, loss = 1.43980989\n",
      "Iteration 400, loss = 1.43845213\n",
      "Iteration 401, loss = 1.43709662\n",
      "Iteration 402, loss = 1.43574340\n",
      "Iteration 403, loss = 1.43439250\n",
      "Iteration 404, loss = 1.43304392\n",
      "Iteration 405, loss = 1.43169770\n",
      "Iteration 406, loss = 1.43035386\n",
      "Iteration 407, loss = 1.42901243\n",
      "Iteration 408, loss = 1.42767342\n",
      "Iteration 409, loss = 1.42633687\n",
      "Iteration 410, loss = 1.42500279\n",
      "Iteration 411, loss = 1.42367121\n",
      "Iteration 412, loss = 1.42234215\n",
      "Iteration 413, loss = 1.42101563\n",
      "Iteration 414, loss = 1.41969169\n",
      "Iteration 415, loss = 1.41837034\n",
      "Iteration 416, loss = 1.41705160\n",
      "Iteration 417, loss = 1.41573550\n",
      "Iteration 418, loss = 1.41442207\n",
      "Iteration 419, loss = 1.41311132\n",
      "Iteration 420, loss = 1.41180328\n",
      "Iteration 421, loss = 1.41049797\n",
      "Iteration 422, loss = 1.40919541\n",
      "Iteration 423, loss = 1.40789563\n",
      "Iteration 424, loss = 1.40659865\n",
      "Iteration 425, loss = 1.40530450\n",
      "Iteration 426, loss = 1.40401319\n",
      "Iteration 427, loss = 1.40272474\n",
      "Iteration 428, loss = 1.40143919\n",
      "Iteration 429, loss = 1.40015655\n",
      "Iteration 430, loss = 1.39887685\n",
      "Iteration 431, loss = 1.39760010\n",
      "Iteration 432, loss = 1.39632633\n",
      "Iteration 433, loss = 1.39505557\n",
      "Iteration 434, loss = 1.39378782\n",
      "Iteration 435, loss = 1.39252313\n",
      "Iteration 436, loss = 1.39126149\n",
      "Iteration 437, loss = 1.39000295\n",
      "Iteration 438, loss = 1.38874752\n",
      "Iteration 439, loss = 1.38749521\n",
      "Iteration 440, loss = 1.38624606\n",
      "Iteration 441, loss = 1.38500009\n",
      "Iteration 442, loss = 1.38375731\n",
      "Iteration 443, loss = 1.38251774\n",
      "Iteration 444, loss = 1.38128141\n",
      "Iteration 445, loss = 1.38004833\n",
      "Iteration 446, loss = 1.37881853\n",
      "Iteration 447, loss = 1.37759203\n",
      "Iteration 448, loss = 1.37636885\n",
      "Iteration 449, loss = 1.37514900\n",
      "Iteration 450, loss = 1.37393251\n",
      "Iteration 451, loss = 1.37271940\n",
      "Iteration 452, loss = 1.37150969\n",
      "Iteration 453, loss = 1.37030339\n",
      "Iteration 454, loss = 1.36910052\n",
      "Iteration 455, loss = 1.36790111\n",
      "Iteration 456, loss = 1.36670518\n",
      "Iteration 457, loss = 1.36551273\n",
      "Iteration 458, loss = 1.36432380\n",
      "Iteration 459, loss = 1.36313840\n",
      "Iteration 460, loss = 1.36195654\n",
      "Iteration 461, loss = 1.36077825\n",
      "Iteration 462, loss = 1.35960354\n",
      "Iteration 463, loss = 1.35843244\n",
      "Iteration 464, loss = 1.35726495\n",
      "Iteration 465, loss = 1.35610110\n",
      "Iteration 466, loss = 1.35494091\n",
      "Iteration 467, loss = 1.35378439\n",
      "Iteration 468, loss = 1.35263155\n",
      "Iteration 469, loss = 1.35148242\n",
      "Iteration 470, loss = 1.35033701\n",
      "Iteration 471, loss = 1.34919534\n",
      "Iteration 472, loss = 1.34805742\n",
      "Iteration 473, loss = 1.34692327\n",
      "Iteration 474, loss = 1.34579290\n",
      "Iteration 475, loss = 1.34466634\n",
      "Iteration 476, loss = 1.34354359\n",
      "Iteration 477, loss = 1.34242468\n",
      "Iteration 478, loss = 1.34130961\n",
      "Iteration 479, loss = 1.34019840\n",
      "Iteration 480, loss = 1.33909107\n",
      "Iteration 481, loss = 1.33798762\n",
      "Iteration 482, loss = 1.33688809\n",
      "Iteration 483, loss = 1.33579247\n",
      "Iteration 484, loss = 1.33470078\n",
      "Iteration 485, loss = 1.33361303\n",
      "Iteration 486, loss = 1.33252925\n",
      "Iteration 487, loss = 1.33144943\n",
      "Iteration 488, loss = 1.33037360\n",
      "Iteration 489, loss = 1.32930177\n",
      "Iteration 490, loss = 1.32823395\n",
      "Iteration 491, loss = 1.32717015\n",
      "Iteration 492, loss = 1.32611038\n",
      "Iteration 493, loss = 1.32505465\n",
      "Iteration 494, loss = 1.32400299\n",
      "Iteration 495, loss = 1.32295539\n",
      "Iteration 496, loss = 1.32191187\n",
      "Iteration 497, loss = 1.32087244\n",
      "Iteration 498, loss = 1.31983711\n",
      "Iteration 499, loss = 1.31880589\n",
      "Iteration 500, loss = 1.31777879\n",
      "Iteration 501, loss = 1.31675582\n",
      "Iteration 502, loss = 1.31573699\n",
      "Iteration 503, loss = 1.31472231\n",
      "Iteration 504, loss = 1.31371179\n",
      "Iteration 505, loss = 1.31270543\n",
      "Iteration 506, loss = 1.31170325\n",
      "Iteration 507, loss = 1.31070526\n",
      "Iteration 508, loss = 1.30971146\n",
      "Iteration 509, loss = 1.30872186\n",
      "Iteration 510, loss = 1.30773647\n",
      "Iteration 511, loss = 1.30675529\n",
      "Iteration 512, loss = 1.30577834\n",
      "Iteration 513, loss = 1.30480561\n",
      "Iteration 514, loss = 1.30383713\n",
      "Iteration 515, loss = 1.30287288\n",
      "Iteration 516, loss = 1.30191289\n",
      "Iteration 517, loss = 1.30095715\n",
      "Iteration 518, loss = 1.30000567\n",
      "Iteration 519, loss = 1.29905845\n",
      "Iteration 520, loss = 1.29811551\n",
      "Iteration 521, loss = 1.29717685\n",
      "Iteration 522, loss = 1.29624246\n",
      "Iteration 523, loss = 1.29531237\n",
      "Iteration 524, loss = 1.29438656\n",
      "Iteration 525, loss = 1.29346504\n",
      "Iteration 526, loss = 1.29254783\n",
      "Iteration 527, loss = 1.29163491\n",
      "Iteration 528, loss = 1.29072630\n",
      "Iteration 529, loss = 1.28982199\n",
      "Iteration 530, loss = 1.28892200\n",
      "Iteration 531, loss = 1.28802632\n",
      "Iteration 532, loss = 1.28713495\n",
      "Iteration 533, loss = 1.28624790\n",
      "Iteration 534, loss = 1.28536517\n",
      "Iteration 535, loss = 1.28448675\n",
      "Iteration 536, loss = 1.28361266\n",
      "Iteration 537, loss = 1.28274289\n",
      "Iteration 538, loss = 1.28187744\n",
      "Iteration 539, loss = 1.28101632\n",
      "Iteration 540, loss = 1.28015952\n",
      "Iteration 541, loss = 1.27930704\n",
      "Iteration 542, loss = 1.27845889\n",
      "Iteration 543, loss = 1.27761505\n",
      "Iteration 544, loss = 1.27677554\n",
      "Iteration 545, loss = 1.27594035\n",
      "Iteration 546, loss = 1.27510948\n",
      "Iteration 547, loss = 1.27428293\n",
      "Iteration 548, loss = 1.27346070\n",
      "Iteration 549, loss = 1.27264278\n",
      "Iteration 550, loss = 1.27182917\n",
      "Iteration 551, loss = 1.27101987\n",
      "Iteration 552, loss = 1.27021488\n",
      "Iteration 553, loss = 1.26941419\n",
      "Iteration 554, loss = 1.26861780\n",
      "Iteration 555, loss = 1.26782571\n",
      "Iteration 556, loss = 1.26703792\n",
      "Iteration 557, loss = 1.26625441\n",
      "Iteration 558, loss = 1.26547519\n",
      "Iteration 559, loss = 1.26470026\n",
      "Iteration 560, loss = 1.26392960\n",
      "Iteration 561, loss = 1.26316321\n",
      "Iteration 562, loss = 1.26240109\n",
      "Iteration 563, loss = 1.26164323\n",
      "Iteration 564, loss = 1.26088963\n",
      "Iteration 565, loss = 1.26014028\n",
      "Iteration 566, loss = 1.25939518\n",
      "Iteration 567, loss = 1.25865431\n",
      "Iteration 568, loss = 1.25791768\n",
      "Iteration 569, loss = 1.25718527\n",
      "Iteration 570, loss = 1.25645709\n",
      "Iteration 571, loss = 1.25573311\n",
      "Iteration 572, loss = 1.25501334\n",
      "Iteration 573, loss = 1.25429777\n",
      "Iteration 574, loss = 1.25358639\n",
      "Iteration 575, loss = 1.25287920\n",
      "Iteration 576, loss = 1.25217617\n",
      "Iteration 577, loss = 1.25147732\n",
      "Iteration 578, loss = 1.25078262\n",
      "Iteration 579, loss = 1.25009207\n",
      "Iteration 580, loss = 1.24940566\n",
      "Iteration 581, loss = 1.24872338\n",
      "Iteration 582, loss = 1.24804523\n",
      "Iteration 583, loss = 1.24737119\n",
      "Iteration 584, loss = 1.24670125\n",
      "Iteration 585, loss = 1.24603540\n",
      "Iteration 586, loss = 1.24537364\n",
      "Iteration 587, loss = 1.24471595\n",
      "Iteration 588, loss = 1.24406233\n",
      "Iteration 589, loss = 1.24341276\n",
      "Iteration 590, loss = 1.24276723\n",
      "Iteration 591, loss = 1.24212573\n",
      "Iteration 592, loss = 1.24148825\n",
      "Iteration 593, loss = 1.24085479\n",
      "Iteration 594, loss = 1.24022531\n",
      "Iteration 595, loss = 1.23959983\n",
      "Iteration 596, loss = 1.23897832\n",
      "Iteration 597, loss = 1.23836077\n",
      "Iteration 598, loss = 1.23774718\n",
      "Iteration 599, loss = 1.23713752\n",
      "Iteration 600, loss = 1.23653179\n",
      "Iteration 601, loss = 1.23592997\n",
      "Iteration 602, loss = 1.23533205\n",
      "Iteration 603, loss = 1.23473802\n",
      "Iteration 604, loss = 1.23414787\n",
      "Iteration 605, loss = 1.23356158\n",
      "Iteration 606, loss = 1.23297914\n",
      "Iteration 607, loss = 1.23240054\n",
      "Iteration 608, loss = 1.23182575\n",
      "Iteration 609, loss = 1.23125478\n",
      "Iteration 610, loss = 1.23068760\n",
      "Iteration 611, loss = 1.23012421\n",
      "Iteration 612, loss = 1.22956458\n",
      "Iteration 613, loss = 1.22900870\n",
      "Iteration 614, loss = 1.22845656\n",
      "Iteration 615, loss = 1.22790815\n",
      "Iteration 616, loss = 1.22736345\n",
      "Iteration 617, loss = 1.22682244\n",
      "Iteration 618, loss = 1.22628512\n",
      "Iteration 619, loss = 1.22575146\n",
      "Iteration 620, loss = 1.22522145\n",
      "Iteration 621, loss = 1.22469508\n",
      "Iteration 622, loss = 1.22417233\n",
      "Iteration 623, loss = 1.22365318\n",
      "Iteration 624, loss = 1.22313763\n",
      "Iteration 625, loss = 1.22262565\n",
      "Iteration 626, loss = 1.22211723\n",
      "Iteration 627, loss = 1.22161235\n",
      "Iteration 628, loss = 1.22111101\n",
      "Iteration 629, loss = 1.22061317\n",
      "Iteration 630, loss = 1.22011884\n",
      "Iteration 631, loss = 1.21962798\n",
      "Iteration 632, loss = 1.21914059\n",
      "Iteration 633, loss = 1.21865665\n",
      "Iteration 634, loss = 1.21817614\n",
      "Iteration 635, loss = 1.21769905\n",
      "Iteration 636, loss = 1.21722536\n",
      "Iteration 637, loss = 1.21675505\n",
      "Iteration 638, loss = 1.21628811\n",
      "Iteration 639, loss = 1.21582453\n",
      "Iteration 640, loss = 1.21536427\n",
      "Iteration 641, loss = 1.21490734\n",
      "Iteration 642, loss = 1.21445370\n",
      "Iteration 643, loss = 1.21400335\n",
      "Iteration 644, loss = 1.21355627\n",
      "Iteration 645, loss = 1.21311244\n",
      "Iteration 646, loss = 1.21267185\n",
      "Iteration 647, loss = 1.21223447\n",
      "Iteration 648, loss = 1.21180030\n",
      "Iteration 649, loss = 1.21136930\n",
      "Iteration 650, loss = 1.21094148\n",
      "Iteration 651, loss = 1.21051680\n",
      "Iteration 652, loss = 1.21009526\n",
      "Iteration 653, loss = 1.20967683\n",
      "Iteration 654, loss = 1.20926150\n",
      "Iteration 655, loss = 1.20884926\n",
      "Iteration 656, loss = 1.20844007\n",
      "Iteration 657, loss = 1.20803394\n",
      "Iteration 658, loss = 1.20763083\n",
      "Iteration 659, loss = 1.20723074\n",
      "Iteration 660, loss = 1.20683364\n",
      "Iteration 661, loss = 1.20643953\n",
      "Iteration 662, loss = 1.20604837\n",
      "Iteration 663, loss = 1.20566016\n",
      "Iteration 664, loss = 1.20527487\n",
      "Iteration 665, loss = 1.20489249\n",
      "Iteration 666, loss = 1.20451301\n",
      "Iteration 667, loss = 1.20413640\n",
      "Iteration 668, loss = 1.20376265\n",
      "Iteration 669, loss = 1.20339174\n",
      "Iteration 670, loss = 1.20302365\n",
      "Iteration 671, loss = 1.20265837\n",
      "Iteration 672, loss = 1.20229588\n",
      "Iteration 673, loss = 1.20193616\n",
      "Iteration 674, loss = 1.20157919\n",
      "Iteration 675, loss = 1.20122497\n",
      "Iteration 676, loss = 1.20087346\n",
      "Iteration 677, loss = 1.20052465\n",
      "Iteration 678, loss = 1.20017853\n",
      "Iteration 679, loss = 1.19983508\n",
      "Iteration 680, loss = 1.19949429\n",
      "Iteration 681, loss = 1.19915612\n",
      "Iteration 682, loss = 1.19882058\n",
      "Iteration 683, loss = 1.19848763\n",
      "Iteration 684, loss = 1.19815727\n",
      "Iteration 685, loss = 1.19782948\n",
      "Iteration 686, loss = 1.19750423\n",
      "Iteration 687, loss = 1.19718152\n",
      "Iteration 688, loss = 1.19686132\n",
      "Iteration 689, loss = 1.19654363\n",
      "Iteration 690, loss = 1.19622841\n",
      "Iteration 691, loss = 1.19591567\n",
      "Iteration 692, loss = 1.19560537\n",
      "Iteration 693, loss = 1.19529750\n",
      "Iteration 694, loss = 1.19499205\n",
      "Iteration 695, loss = 1.19468900\n",
      "Iteration 696, loss = 1.19438833\n",
      "Iteration 697, loss = 1.19409003\n",
      "Iteration 698, loss = 1.19379408\n",
      "Iteration 699, loss = 1.19350046\n",
      "Iteration 700, loss = 1.19320916\n",
      "Iteration 701, loss = 1.19292017\n",
      "Iteration 702, loss = 1.19263345\n",
      "Iteration 703, loss = 1.19234901\n",
      "Iteration 704, loss = 1.19206681\n",
      "Iteration 705, loss = 1.19178686\n",
      "Iteration 706, loss = 1.19150912\n",
      "Iteration 707, loss = 1.19123359\n",
      "Iteration 708, loss = 1.19096025\n",
      "Iteration 709, loss = 1.19068908\n",
      "Iteration 710, loss = 1.19042007\n",
      "Iteration 711, loss = 1.19015320\n",
      "Iteration 712, loss = 1.18988845\n",
      "Iteration 713, loss = 1.18962582\n",
      "Iteration 714, loss = 1.18936528\n",
      "Iteration 715, loss = 1.18910682\n",
      "Iteration 716, loss = 1.18885042\n",
      "Iteration 717, loss = 1.18859607\n",
      "Iteration 718, loss = 1.18834376\n",
      "Iteration 719, loss = 1.18809346\n",
      "Iteration 720, loss = 1.18784517\n",
      "Iteration 721, loss = 1.18759887\n",
      "Iteration 722, loss = 1.18735453\n",
      "Iteration 723, loss = 1.18711216\n",
      "Iteration 724, loss = 1.18687173\n",
      "Iteration 725, loss = 1.18663323\n",
      "Iteration 726, loss = 1.18639664\n",
      "Iteration 727, loss = 1.18616195\n",
      "Iteration 728, loss = 1.18592915\n",
      "Iteration 729, loss = 1.18569822\n",
      "Iteration 730, loss = 1.18546914\n",
      "Iteration 731, loss = 1.18524191\n",
      "Iteration 732, loss = 1.18501650\n",
      "Iteration 733, loss = 1.18479290\n",
      "Iteration 734, loss = 1.18457111\n",
      "Iteration 735, loss = 1.18435110\n",
      "Iteration 736, loss = 1.18413286\n",
      "Iteration 737, loss = 1.18391638\n",
      "Iteration 738, loss = 1.18370164\n",
      "Iteration 739, loss = 1.18348863\n",
      "Iteration 740, loss = 1.18327734\n",
      "Iteration 741, loss = 1.18306775\n",
      "Iteration 742, loss = 1.18285985\n",
      "Iteration 743, loss = 1.18265362\n",
      "Iteration 744, loss = 1.18244906\n",
      "Iteration 745, loss = 1.18224614\n",
      "Iteration 746, loss = 1.18204487\n",
      "Iteration 747, loss = 1.18184521\n",
      "Iteration 748, loss = 1.18164717\n",
      "Iteration 749, loss = 1.18145072\n",
      "Iteration 750, loss = 1.18125586\n",
      "Iteration 751, loss = 1.18106256\n",
      "Iteration 752, loss = 1.18087083\n",
      "Iteration 753, loss = 1.18068064\n",
      "Iteration 754, loss = 1.18049199\n",
      "Iteration 755, loss = 1.18030486\n",
      "Iteration 756, loss = 1.18011923\n",
      "Iteration 757, loss = 1.17993510\n",
      "Iteration 758, loss = 1.17975246\n",
      "Iteration 759, loss = 1.17957129\n",
      "Iteration 760, loss = 1.17939158\n",
      "Iteration 761, loss = 1.17921332\n",
      "Iteration 762, loss = 1.17903649\n",
      "Iteration 763, loss = 1.17886109\n",
      "Iteration 764, loss = 1.17868710\n",
      "Iteration 765, loss = 1.17851451\n",
      "Iteration 766, loss = 1.17834331\n",
      "Iteration 767, loss = 1.17817348\n",
      "Iteration 768, loss = 1.17800503\n",
      "Iteration 769, loss = 1.17783793\n",
      "Iteration 770, loss = 1.17767217\n",
      "Iteration 771, loss = 1.17750775\n",
      "Iteration 772, loss = 1.17734465\n",
      "Iteration 773, loss = 1.17718286\n",
      "Iteration 774, loss = 1.17702237\n",
      "Iteration 775, loss = 1.17686316\n",
      "Iteration 776, loss = 1.17670524\n",
      "Iteration 777, loss = 1.17654858\n",
      "Iteration 778, loss = 1.17639318\n",
      "Iteration 779, loss = 1.17623903\n",
      "Iteration 780, loss = 1.17608611\n",
      "Iteration 781, loss = 1.17593442\n",
      "Iteration 782, loss = 1.17578394\n",
      "Iteration 783, loss = 1.17563467\n",
      "Iteration 784, loss = 1.17548659\n",
      "Iteration 785, loss = 1.17533970\n",
      "Iteration 786, loss = 1.17519398\n",
      "Iteration 787, loss = 1.17504943\n",
      "Iteration 788, loss = 1.17490603\n",
      "Iteration 789, loss = 1.17476377\n",
      "Iteration 790, loss = 1.17462265\n",
      "Iteration 791, loss = 1.17448266\n",
      "Iteration 792, loss = 1.17434378\n",
      "Iteration 793, loss = 1.17420601\n",
      "Iteration 794, loss = 1.17406934\n",
      "Iteration 795, loss = 1.17393375\n",
      "Iteration 796, loss = 1.17379925\n",
      "Iteration 797, loss = 1.17366581\n",
      "Iteration 798, loss = 1.17353343\n",
      "Iteration 799, loss = 1.17340211\n",
      "Iteration 800, loss = 1.17327182\n",
      "Iteration 801, loss = 1.17314257\n",
      "Iteration 802, loss = 1.17301435\n",
      "Iteration 803, loss = 1.17288714\n",
      "Iteration 804, loss = 1.17276094\n",
      "Iteration 805, loss = 1.17263573\n",
      "Iteration 806, loss = 1.17251152\n",
      "Iteration 807, loss = 1.17238829\n",
      "Iteration 808, loss = 1.17226603\n",
      "Iteration 809, loss = 1.17214474\n",
      "Iteration 810, loss = 1.17202440\n",
      "Iteration 811, loss = 1.17190501\n",
      "Iteration 812, loss = 1.17178656\n",
      "Iteration 813, loss = 1.17166905\n",
      "Iteration 814, loss = 1.17155246\n",
      "Iteration 815, loss = 1.17143678\n",
      "Iteration 816, loss = 1.17132201\n",
      "Iteration 817, loss = 1.17120815\n",
      "Iteration 818, loss = 1.17109517\n",
      "Iteration 819, loss = 1.17098308\n",
      "Iteration 820, loss = 1.17087187\n",
      "Iteration 821, loss = 1.17076153\n",
      "Iteration 822, loss = 1.17065206\n",
      "Iteration 823, loss = 1.17054343\n",
      "Iteration 824, loss = 1.17043566\n",
      "Iteration 825, loss = 1.17032872\n",
      "Iteration 826, loss = 1.17022262\n",
      "Iteration 827, loss = 1.17011735\n",
      "Iteration 828, loss = 1.17001289\n",
      "Iteration 829, loss = 1.16990925\n",
      "Iteration 830, loss = 1.16980641\n",
      "Iteration 831, loss = 1.16970437\n",
      "Iteration 832, loss = 1.16960312\n",
      "Iteration 833, loss = 1.16950266\n",
      "Iteration 834, loss = 1.16940297\n",
      "Iteration 835, loss = 1.16930406\n",
      "Iteration 836, loss = 1.16920591\n",
      "Iteration 837, loss = 1.16910852\n",
      "Iteration 838, loss = 1.16901188\n",
      "Iteration 839, loss = 1.16891598\n",
      "Iteration 840, loss = 1.16882082\n",
      "Iteration 841, loss = 1.16872640\n",
      "Iteration 842, loss = 1.16863270\n",
      "Iteration 843, loss = 1.16853972\n",
      "Iteration 844, loss = 1.16844746\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.45827620\n",
      "Iteration 2, loss = 1.45668695\n",
      "Iteration 3, loss = 1.45510110\n",
      "Iteration 4, loss = 1.45351874\n",
      "Iteration 5, loss = 1.45193992\n",
      "Iteration 6, loss = 1.45036473\n",
      "Iteration 7, loss = 1.44879322\n",
      "Iteration 8, loss = 1.44722547\n",
      "Iteration 9, loss = 1.44566154\n",
      "Iteration 10, loss = 1.44410150\n",
      "Iteration 11, loss = 1.44254541\n",
      "Iteration 12, loss = 1.44099334\n",
      "Iteration 13, loss = 1.43944534\n",
      "Iteration 14, loss = 1.43790149\n",
      "Iteration 15, loss = 1.43636182\n",
      "Iteration 16, loss = 1.43482641\n",
      "Iteration 17, loss = 1.43329531\n",
      "Iteration 18, loss = 1.43176857\n",
      "Iteration 19, loss = 1.43024624\n",
      "Iteration 20, loss = 1.42872837\n",
      "Iteration 21, loss = 1.42721501\n",
      "Iteration 22, loss = 1.42570620\n",
      "Iteration 23, loss = 1.42420200\n",
      "Iteration 24, loss = 1.42270243\n",
      "Iteration 25, loss = 1.42120755\n",
      "Iteration 26, loss = 1.41971739\n",
      "Iteration 27, loss = 1.41823198\n",
      "Iteration 28, loss = 1.41675137\n",
      "Iteration 29, loss = 1.41527559\n",
      "Iteration 30, loss = 1.41380466\n",
      "Iteration 31, loss = 1.41233863\n",
      "Iteration 32, loss = 1.41087751\n",
      "Iteration 33, loss = 1.40942134\n",
      "Iteration 34, loss = 1.40797014\n",
      "Iteration 35, loss = 1.40652394\n",
      "Iteration 36, loss = 1.40508275\n",
      "Iteration 37, loss = 1.40364661\n",
      "Iteration 38, loss = 1.40221553\n",
      "Iteration 39, loss = 1.40078953\n",
      "Iteration 40, loss = 1.39936863\n",
      "Iteration 41, loss = 1.39795285\n",
      "Iteration 42, loss = 1.39654220\n",
      "Iteration 43, loss = 1.39513669\n",
      "Iteration 44, loss = 1.39373635\n",
      "Iteration 45, loss = 1.39234119\n",
      "Iteration 46, loss = 1.39095121\n",
      "Iteration 47, loss = 1.38956643\n",
      "Iteration 48, loss = 1.38818686\n",
      "Iteration 49, loss = 1.38681252\n",
      "Iteration 50, loss = 1.38544340\n",
      "Iteration 51, loss = 1.38407952\n",
      "Iteration 52, loss = 1.38272089\n",
      "Iteration 53, loss = 1.38136751\n",
      "Iteration 54, loss = 1.38001939\n",
      "Iteration 55, loss = 1.37867654\n",
      "Iteration 56, loss = 1.37733896\n",
      "Iteration 57, loss = 1.37600667\n",
      "Iteration 58, loss = 1.37467965\n",
      "Iteration 59, loss = 1.37335792\n",
      "Iteration 60, loss = 1.37204149\n",
      "Iteration 61, loss = 1.37073035\n",
      "Iteration 62, loss = 1.36942452\n",
      "Iteration 63, loss = 1.36812398\n",
      "Iteration 64, loss = 1.36682875\n",
      "Iteration 65, loss = 1.36553883\n",
      "Iteration 66, loss = 1.36425421\n",
      "Iteration 67, loss = 1.36297491\n",
      "Iteration 68, loss = 1.36170092\n",
      "Iteration 69, loss = 1.36043224\n",
      "Iteration 70, loss = 1.35916888\n",
      "Iteration 71, loss = 1.35791083\n",
      "Iteration 72, loss = 1.35665810\n",
      "Iteration 73, loss = 1.35541068\n",
      "Iteration 74, loss = 1.35416858\n",
      "Iteration 75, loss = 1.35293180\n",
      "Iteration 76, loss = 1.35170033\n",
      "Iteration 77, loss = 1.35047417\n",
      "Iteration 78, loss = 1.34925333\n",
      "Iteration 79, loss = 1.34803780\n",
      "Iteration 80, loss = 1.34682757\n",
      "Iteration 81, loss = 1.34562266\n",
      "Iteration 82, loss = 1.34442306\n",
      "Iteration 83, loss = 1.34322876\n",
      "Iteration 84, loss = 1.34203976\n",
      "Iteration 85, loss = 1.34085607\n",
      "Iteration 86, loss = 1.33967767\n",
      "Iteration 87, loss = 1.33850457\n",
      "Iteration 88, loss = 1.33733676\n",
      "Iteration 89, loss = 1.33617424\n",
      "Iteration 90, loss = 1.33501701\n",
      "Iteration 91, loss = 1.33386507\n",
      "Iteration 92, loss = 1.33271840\n",
      "Iteration 93, loss = 1.33157701\n",
      "Iteration 94, loss = 1.33044089\n",
      "Iteration 95, loss = 1.32931005\n",
      "Iteration 96, loss = 1.32818446\n",
      "Iteration 97, loss = 1.32706414\n",
      "Iteration 98, loss = 1.32594907\n",
      "Iteration 99, loss = 1.32483926\n",
      "Iteration 100, loss = 1.32373469\n",
      "Iteration 101, loss = 1.32263536\n",
      "Iteration 102, loss = 1.32154127\n",
      "Iteration 103, loss = 1.32045242\n",
      "Iteration 104, loss = 1.31936879\n",
      "Iteration 105, loss = 1.31829038\n",
      "Iteration 106, loss = 1.31721718\n",
      "Iteration 107, loss = 1.31614920\n",
      "Iteration 108, loss = 1.31508642\n",
      "Iteration 109, loss = 1.31402883\n",
      "Iteration 110, loss = 1.31297644\n",
      "Iteration 111, loss = 1.31192923\n",
      "Iteration 112, loss = 1.31088720\n",
      "Iteration 113, loss = 1.30985035\n",
      "Iteration 114, loss = 1.30881865\n",
      "Iteration 115, loss = 1.30779212\n",
      "Iteration 116, loss = 1.30677074\n",
      "Iteration 117, loss = 1.30575450\n",
      "Iteration 118, loss = 1.30474340\n",
      "Iteration 119, loss = 1.30373742\n",
      "Iteration 120, loss = 1.30273657\n",
      "Iteration 121, loss = 1.30174083\n",
      "Iteration 122, loss = 1.30075020\n",
      "Iteration 123, loss = 1.29976467\n",
      "Iteration 124, loss = 1.29878422\n",
      "Iteration 125, loss = 1.29780885\n",
      "Iteration 126, loss = 1.29683856\n",
      "Iteration 127, loss = 1.29587333\n",
      "Iteration 128, loss = 1.29491316\n",
      "Iteration 129, loss = 1.29395803\n",
      "Iteration 130, loss = 1.29300794\n",
      "Iteration 131, loss = 1.29206288\n",
      "Iteration 132, loss = 1.29112284\n",
      "Iteration 133, loss = 1.29018781\n",
      "Iteration 134, loss = 1.28925777\n",
      "Iteration 135, loss = 1.28833273\n",
      "Iteration 136, loss = 1.28741267\n",
      "Iteration 137, loss = 1.28649758\n",
      "Iteration 138, loss = 1.28558745\n",
      "Iteration 139, loss = 1.28468226\n",
      "Iteration 140, loss = 1.28378202\n",
      "Iteration 141, loss = 1.28288671\n",
      "Iteration 142, loss = 1.28199632\n",
      "Iteration 143, loss = 1.28111084\n",
      "Iteration 144, loss = 1.28023025\n",
      "Iteration 145, loss = 1.27935455\n",
      "Iteration 146, loss = 1.27848373\n",
      "Iteration 147, loss = 1.27761777\n",
      "Iteration 148, loss = 1.27675667\n",
      "Iteration 149, loss = 1.27590040\n",
      "Iteration 150, loss = 1.27504897\n",
      "Iteration 151, loss = 1.27420236\n",
      "Iteration 152, loss = 1.27336055\n",
      "Iteration 153, loss = 1.27252354\n",
      "Iteration 154, loss = 1.27169131\n",
      "Iteration 155, loss = 1.27086386\n",
      "Iteration 156, loss = 1.27004116\n",
      "Iteration 157, loss = 1.26922321\n",
      "Iteration 158, loss = 1.26840999\n",
      "Iteration 159, loss = 1.26760150\n",
      "Iteration 160, loss = 1.26679772\n",
      "Iteration 161, loss = 1.26599863\n",
      "Iteration 162, loss = 1.26520423\n",
      "Iteration 163, loss = 1.26441450\n",
      "Iteration 164, loss = 1.26362942\n",
      "Iteration 165, loss = 1.26284899\n",
      "Iteration 166, loss = 1.26207320\n",
      "Iteration 167, loss = 1.26130202\n",
      "Iteration 168, loss = 1.26053545\n",
      "Iteration 169, loss = 1.25977346\n",
      "Iteration 170, loss = 1.25901606\n",
      "Iteration 171, loss = 1.25826322\n",
      "Iteration 172, loss = 1.25751493\n",
      "Iteration 173, loss = 1.25677118\n",
      "Iteration 174, loss = 1.25603196\n",
      "Iteration 175, loss = 1.25529724\n",
      "Iteration 176, loss = 1.25456701\n",
      "Iteration 177, loss = 1.25384127\n",
      "Iteration 178, loss = 1.25311999\n",
      "Iteration 179, loss = 1.25240317\n",
      "Iteration 180, loss = 1.25169078\n",
      "Iteration 181, loss = 1.25098282\n",
      "Iteration 182, loss = 1.25027926\n",
      "Iteration 183, loss = 1.24958010\n",
      "Iteration 184, loss = 1.24888532\n",
      "Iteration 185, loss = 1.24819490\n",
      "Iteration 186, loss = 1.24750883\n",
      "Iteration 187, loss = 1.24682709\n",
      "Iteration 188, loss = 1.24614968\n",
      "Iteration 189, loss = 1.24547656\n",
      "Iteration 190, loss = 1.24480774\n",
      "Iteration 191, loss = 1.24414319\n",
      "Iteration 192, loss = 1.24348290\n",
      "Iteration 193, loss = 1.24282686\n",
      "Iteration 194, loss = 1.24217504\n",
      "Iteration 195, loss = 1.24152743\n",
      "Iteration 196, loss = 1.24088402\n",
      "Iteration 197, loss = 1.24024479\n",
      "Iteration 198, loss = 1.23960972\n",
      "Iteration 199, loss = 1.23897881\n",
      "Iteration 200, loss = 1.23835203\n",
      "Iteration 201, loss = 1.23772936\n",
      "Iteration 202, loss = 1.23711080\n",
      "Iteration 203, loss = 1.23649633\n",
      "Iteration 204, loss = 1.23588592\n",
      "Iteration 205, loss = 1.23527957\n",
      "Iteration 206, loss = 1.23467725\n",
      "Iteration 207, loss = 1.23407896\n",
      "Iteration 208, loss = 1.23348467\n",
      "Iteration 209, loss = 1.23289437\n",
      "Iteration 210, loss = 1.23230805\n",
      "Iteration 211, loss = 1.23172568\n",
      "Iteration 212, loss = 1.23114725\n",
      "Iteration 213, loss = 1.23057274\n",
      "Iteration 214, loss = 1.23000214\n",
      "Iteration 215, loss = 1.22943544\n",
      "Iteration 216, loss = 1.22887260\n",
      "Iteration 217, loss = 1.22831363\n",
      "Iteration 218, loss = 1.22775849\n",
      "Iteration 219, loss = 1.22720718\n",
      "Iteration 220, loss = 1.22665968\n",
      "Iteration 221, loss = 1.22611597\n",
      "Iteration 222, loss = 1.22557603\n",
      "Iteration 223, loss = 1.22503985\n",
      "Iteration 224, loss = 1.22450741\n",
      "Iteration 225, loss = 1.22397869\n",
      "Iteration 226, loss = 1.22345368\n",
      "Iteration 227, loss = 1.22293236\n",
      "Iteration 228, loss = 1.22241472\n",
      "Iteration 229, loss = 1.22190073\n",
      "Iteration 230, loss = 1.22139038\n",
      "Iteration 231, loss = 1.22088365\n",
      "Iteration 232, loss = 1.22038053\n",
      "Iteration 233, loss = 1.21988099\n",
      "Iteration 234, loss = 1.21938503\n",
      "Iteration 235, loss = 1.21889262\n",
      "Iteration 236, loss = 1.21840375\n",
      "Iteration 237, loss = 1.21791840\n",
      "Iteration 238, loss = 1.21743656\n",
      "Iteration 239, loss = 1.21695820\n",
      "Iteration 240, loss = 1.21648331\n",
      "Iteration 241, loss = 1.21601187\n",
      "Iteration 242, loss = 1.21554387\n",
      "Iteration 243, loss = 1.21507928\n",
      "Iteration 244, loss = 1.21461810\n",
      "Iteration 245, loss = 1.21416030\n",
      "Iteration 246, loss = 1.21370587\n",
      "Iteration 247, loss = 1.21325478\n",
      "Iteration 248, loss = 1.21280703\n",
      "Iteration 249, loss = 1.21236260\n",
      "Iteration 250, loss = 1.21192146\n",
      "Iteration 251, loss = 1.21148361\n",
      "Iteration 252, loss = 1.21104902\n",
      "Iteration 253, loss = 1.21061768\n",
      "Iteration 254, loss = 1.21018957\n",
      "Iteration 255, loss = 1.20976467\n",
      "Iteration 256, loss = 1.20934297\n",
      "Iteration 257, loss = 1.20892444\n",
      "Iteration 258, loss = 1.20850908\n",
      "Iteration 259, loss = 1.20809687\n",
      "Iteration 260, loss = 1.20768778\n",
      "Iteration 261, loss = 1.20728180\n",
      "Iteration 262, loss = 1.20687892\n",
      "Iteration 263, loss = 1.20647912\n",
      "Iteration 264, loss = 1.20608237\n",
      "Iteration 265, loss = 1.20568867\n",
      "Iteration 266, loss = 1.20529800\n",
      "Iteration 267, loss = 1.20491033\n",
      "Iteration 268, loss = 1.20452566\n",
      "Iteration 269, loss = 1.20414397\n",
      "Iteration 270, loss = 1.20376523\n",
      "Iteration 271, loss = 1.20338944\n",
      "Iteration 272, loss = 1.20301657\n",
      "Iteration 273, loss = 1.20264661\n",
      "Iteration 274, loss = 1.20227954\n",
      "Iteration 275, loss = 1.20191535\n",
      "Iteration 276, loss = 1.20155402\n",
      "Iteration 277, loss = 1.20119553\n",
      "Iteration 278, loss = 1.20083986\n",
      "Iteration 279, loss = 1.20048701\n",
      "Iteration 280, loss = 1.20013695\n",
      "Iteration 281, loss = 1.19978966\n",
      "Iteration 282, loss = 1.19944513\n",
      "Iteration 283, loss = 1.19910335\n",
      "Iteration 284, loss = 1.19876429\n",
      "Iteration 285, loss = 1.19842794\n",
      "Iteration 286, loss = 1.19809429\n",
      "Iteration 287, loss = 1.19776332\n",
      "Iteration 288, loss = 1.19743500\n",
      "Iteration 289, loss = 1.19710933\n",
      "Iteration 290, loss = 1.19678629\n",
      "Iteration 291, loss = 1.19646587\n",
      "Iteration 292, loss = 1.19614804\n",
      "Iteration 293, loss = 1.19583279\n",
      "Iteration 294, loss = 1.19552010\n",
      "Iteration 295, loss = 1.19520997\n",
      "Iteration 296, loss = 1.19490236\n",
      "Iteration 297, loss = 1.19459728\n",
      "Iteration 298, loss = 1.19429469\n",
      "Iteration 299, loss = 1.19399459\n",
      "Iteration 300, loss = 1.19369696\n",
      "Iteration 301, loss = 1.19340178\n",
      "Iteration 302, loss = 1.19310904\n",
      "Iteration 303, loss = 1.19281873\n",
      "Iteration 304, loss = 1.19253082\n",
      "Iteration 305, loss = 1.19224530\n",
      "Iteration 306, loss = 1.19196216\n",
      "Iteration 307, loss = 1.19168137\n",
      "Iteration 308, loss = 1.19140293\n",
      "Iteration 309, loss = 1.19112683\n",
      "Iteration 310, loss = 1.19085303\n",
      "Iteration 311, loss = 1.19058154\n",
      "Iteration 312, loss = 1.19031233\n",
      "Iteration 313, loss = 1.19004538\n",
      "Iteration 314, loss = 1.18978069\n",
      "Iteration 315, loss = 1.18951824\n",
      "Iteration 316, loss = 1.18925802\n",
      "Iteration 317, loss = 1.18900000\n",
      "Iteration 318, loss = 1.18874417\n",
      "Iteration 319, loss = 1.18849053\n",
      "Iteration 320, loss = 1.18823905\n",
      "Iteration 321, loss = 1.18798971\n",
      "Iteration 322, loss = 1.18774252\n",
      "Iteration 323, loss = 1.18749744\n",
      "Iteration 324, loss = 1.18725447\n",
      "Iteration 325, loss = 1.18701359\n",
      "Iteration 326, loss = 1.18677479\n",
      "Iteration 327, loss = 1.18653804\n",
      "Iteration 328, loss = 1.18630335\n",
      "Iteration 329, loss = 1.18607069\n",
      "Iteration 330, loss = 1.18584005\n",
      "Iteration 331, loss = 1.18561142\n",
      "Iteration 332, loss = 1.18538478\n",
      "Iteration 333, loss = 1.18516011\n",
      "Iteration 334, loss = 1.18493741\n",
      "Iteration 335, loss = 1.18471666\n",
      "Iteration 336, loss = 1.18449784\n",
      "Iteration 337, loss = 1.18428094\n",
      "Iteration 338, loss = 1.18406596\n",
      "Iteration 339, loss = 1.18385287\n",
      "Iteration 340, loss = 1.18364165\n",
      "Iteration 341, loss = 1.18343231\n",
      "Iteration 342, loss = 1.18322482\n",
      "Iteration 343, loss = 1.18301917\n",
      "Iteration 344, loss = 1.18281535\n",
      "Iteration 345, loss = 1.18261334\n",
      "Iteration 346, loss = 1.18241313\n",
      "Iteration 347, loss = 1.18221471\n",
      "Iteration 348, loss = 1.18201806\n",
      "Iteration 349, loss = 1.18182318\n",
      "Iteration 350, loss = 1.18163004\n",
      "Iteration 351, loss = 1.18143864\n",
      "Iteration 352, loss = 1.18124896\n",
      "Iteration 353, loss = 1.18106099\n",
      "Iteration 354, loss = 1.18087472\n",
      "Iteration 355, loss = 1.18069013\n",
      "Iteration 356, loss = 1.18050721\n",
      "Iteration 357, loss = 1.18032595\n",
      "Iteration 358, loss = 1.18014634\n",
      "Iteration 359, loss = 1.17996836\n",
      "Iteration 360, loss = 1.17979201\n",
      "Iteration 361, loss = 1.17961726\n",
      "Iteration 362, loss = 1.17944411\n",
      "Iteration 363, loss = 1.17927255\n",
      "Iteration 364, loss = 1.17910256\n",
      "Iteration 365, loss = 1.17893413\n",
      "Iteration 366, loss = 1.17876725\n",
      "Iteration 367, loss = 1.17860191\n",
      "Iteration 368, loss = 1.17843809\n",
      "Iteration 369, loss = 1.17827579\n",
      "Iteration 370, loss = 1.17811499\n",
      "Iteration 371, loss = 1.17795567\n",
      "Iteration 372, loss = 1.17779784\n",
      "Iteration 373, loss = 1.17764147\n",
      "Iteration 374, loss = 1.17748656\n",
      "Iteration 375, loss = 1.17733310\n",
      "Iteration 376, loss = 1.17718106\n",
      "Iteration 377, loss = 1.17703045\n",
      "Iteration 378, loss = 1.17688125\n",
      "Iteration 379, loss = 1.17673345\n",
      "Iteration 380, loss = 1.17658704\n",
      "Iteration 381, loss = 1.17644200\n",
      "Iteration 382, loss = 1.17629833\n",
      "Iteration 383, loss = 1.17615601\n",
      "Iteration 384, loss = 1.17601504\n",
      "Iteration 385, loss = 1.17587541\n",
      "Iteration 386, loss = 1.17573709\n",
      "Iteration 387, loss = 1.17560009\n",
      "Iteration 388, loss = 1.17546439\n",
      "Iteration 389, loss = 1.17532999\n",
      "Iteration 390, loss = 1.17519686\n",
      "Iteration 391, loss = 1.17506500\n",
      "Iteration 392, loss = 1.17493441\n",
      "Iteration 393, loss = 1.17480506\n",
      "Iteration 394, loss = 1.17467696\n",
      "Iteration 395, loss = 1.17455008\n",
      "Iteration 396, loss = 1.17442442\n",
      "Iteration 397, loss = 1.17429998\n",
      "Iteration 398, loss = 1.17417673\n",
      "Iteration 399, loss = 1.17405467\n",
      "Iteration 400, loss = 1.17393380\n",
      "Iteration 401, loss = 1.17381409\n",
      "Iteration 402, loss = 1.17369554\n",
      "Iteration 403, loss = 1.17357815\n",
      "Iteration 404, loss = 1.17346189\n",
      "Iteration 405, loss = 1.17334677\n",
      "Iteration 406, loss = 1.17323277\n",
      "Iteration 407, loss = 1.17311988\n",
      "Iteration 408, loss = 1.17300810\n",
      "Iteration 409, loss = 1.17289741\n",
      "Iteration 410, loss = 1.17278781\n",
      "Iteration 411, loss = 1.17267928\n",
      "Iteration 412, loss = 1.17257182\n",
      "Iteration 413, loss = 1.17246541\n",
      "Iteration 414, loss = 1.17236006\n",
      "Iteration 415, loss = 1.17225574\n",
      "Iteration 416, loss = 1.17215246\n",
      "Iteration 417, loss = 1.17205020\n",
      "Iteration 418, loss = 1.17194895\n",
      "Iteration 419, loss = 1.17184871\n",
      "Iteration 420, loss = 1.17174947\n",
      "Iteration 421, loss = 1.17165121\n",
      "Iteration 422, loss = 1.17155393\n",
      "Iteration 423, loss = 1.17145762\n",
      "Iteration 424, loss = 1.17136228\n",
      "Iteration 425, loss = 1.17126789\n",
      "Iteration 426, loss = 1.17117444\n",
      "Iteration 427, loss = 1.17108193\n",
      "Iteration 428, loss = 1.17099036\n",
      "Iteration 429, loss = 1.17089970\n",
      "Iteration 430, loss = 1.17080996\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.57318057\n",
      "Iteration 2, loss = 1.57233040\n",
      "Iteration 3, loss = 1.57148101\n",
      "Iteration 4, loss = 1.57063241\n",
      "Iteration 5, loss = 1.56978463\n",
      "Iteration 6, loss = 1.56893767\n",
      "Iteration 7, loss = 1.56809155\n",
      "Iteration 8, loss = 1.56724628\n",
      "Iteration 9, loss = 1.56640188\n",
      "Iteration 10, loss = 1.56555835\n",
      "Iteration 11, loss = 1.56471571\n",
      "Iteration 12, loss = 1.56387398\n",
      "Iteration 13, loss = 1.56303315\n",
      "Iteration 14, loss = 1.56219326\n",
      "Iteration 15, loss = 1.56135430\n",
      "Iteration 16, loss = 1.56051628\n",
      "Iteration 17, loss = 1.55967922\n",
      "Iteration 18, loss = 1.55884313\n",
      "Iteration 19, loss = 1.55800801\n",
      "Iteration 20, loss = 1.55717388\n",
      "Iteration 21, loss = 1.55634074\n",
      "Iteration 22, loss = 1.55550860\n",
      "Iteration 23, loss = 1.55467746\n",
      "Iteration 24, loss = 1.55384734\n",
      "Iteration 25, loss = 1.55301825\n",
      "Iteration 26, loss = 1.55219018\n",
      "Iteration 27, loss = 1.55136314\n",
      "Iteration 28, loss = 1.55053715\n",
      "Iteration 29, loss = 1.54971219\n",
      "Iteration 30, loss = 1.54888829\n",
      "Iteration 31, loss = 1.54806544\n",
      "Iteration 32, loss = 1.54724364\n",
      "Iteration 33, loss = 1.54642291\n",
      "Iteration 34, loss = 1.54560323\n",
      "Iteration 35, loss = 1.54478463\n",
      "Iteration 36, loss = 1.54396709\n",
      "Iteration 37, loss = 1.54315062\n",
      "Iteration 38, loss = 1.54233522\n",
      "Iteration 39, loss = 1.54152090\n",
      "Iteration 40, loss = 1.54070765\n",
      "Iteration 41, loss = 1.53989548\n",
      "Iteration 42, loss = 1.53908438\n",
      "Iteration 43, loss = 1.53827436\n",
      "Iteration 44, loss = 1.53746542\n",
      "Iteration 45, loss = 1.53665755\n",
      "Iteration 46, loss = 1.53585077\n",
      "Iteration 47, loss = 1.53504506\n",
      "Iteration 48, loss = 1.53424042\n",
      "Iteration 49, loss = 1.53343686\n",
      "Iteration 50, loss = 1.53263438\n",
      "Iteration 51, loss = 1.53183297\n",
      "Iteration 52, loss = 1.53103263\n",
      "Iteration 53, loss = 1.53023337\n",
      "Iteration 54, loss = 1.52943517\n",
      "Iteration 55, loss = 1.52863805\n",
      "Iteration 56, loss = 1.52784199\n",
      "Iteration 57, loss = 1.52704699\n",
      "Iteration 58, loss = 1.52625306\n",
      "Iteration 59, loss = 1.52546019\n",
      "Iteration 60, loss = 1.52466838\n",
      "Iteration 61, loss = 1.52387763\n",
      "Iteration 62, loss = 1.52308793\n",
      "Iteration 63, loss = 1.52229928\n",
      "Iteration 64, loss = 1.52151169\n",
      "Iteration 65, loss = 1.52072515\n",
      "Iteration 66, loss = 1.51993965\n",
      "Iteration 67, loss = 1.51915519\n",
      "Iteration 68, loss = 1.51837178\n",
      "Iteration 69, loss = 1.51758940\n",
      "Iteration 70, loss = 1.51680807\n",
      "Iteration 71, loss = 1.51602776\n",
      "Iteration 72, loss = 1.51524849\n",
      "Iteration 73, loss = 1.51447024\n",
      "Iteration 74, loss = 1.51369302\n",
      "Iteration 75, loss = 1.51291683\n",
      "Iteration 76, loss = 1.51214165\n",
      "Iteration 77, loss = 1.51136750\n",
      "Iteration 78, loss = 1.51059436\n",
      "Iteration 79, loss = 1.50982223\n",
      "Iteration 80, loss = 1.50905111\n",
      "Iteration 81, loss = 1.50828100\n",
      "Iteration 82, loss = 1.50751189\n",
      "Iteration 83, loss = 1.50674378\n",
      "Iteration 84, loss = 1.50597668\n",
      "Iteration 85, loss = 1.50521057\n",
      "Iteration 86, loss = 1.50444545\n",
      "Iteration 87, loss = 1.50368133\n",
      "Iteration 88, loss = 1.50291819\n",
      "Iteration 89, loss = 1.50215604\n",
      "Iteration 90, loss = 1.50139487\n",
      "Iteration 91, loss = 1.50063469\n",
      "Iteration 92, loss = 1.49987548\n",
      "Iteration 93, loss = 1.49911725\n",
      "Iteration 94, loss = 1.49835999\n",
      "Iteration 95, loss = 1.49760371\n",
      "Iteration 96, loss = 1.49684839\n",
      "Iteration 97, loss = 1.49609403\n",
      "Iteration 98, loss = 1.49534065\n",
      "Iteration 99, loss = 1.49458822\n",
      "Iteration 100, loss = 1.49383675\n",
      "Iteration 101, loss = 1.49308624\n",
      "Iteration 102, loss = 1.49233668\n",
      "Iteration 103, loss = 1.49158808\n",
      "Iteration 104, loss = 1.49084042\n",
      "Iteration 105, loss = 1.49009371\n",
      "Iteration 106, loss = 1.48934795\n",
      "Iteration 107, loss = 1.48860314\n",
      "Iteration 108, loss = 1.48785926\n",
      "Iteration 109, loss = 1.48711633\n",
      "Iteration 110, loss = 1.48637433\n",
      "Iteration 111, loss = 1.48563327\n",
      "Iteration 112, loss = 1.48489314\n",
      "Iteration 113, loss = 1.48415395\n",
      "Iteration 114, loss = 1.48341568\n",
      "Iteration 115, loss = 1.48267835\n",
      "Iteration 116, loss = 1.48194194\n",
      "Iteration 117, loss = 1.48120646\n",
      "Iteration 118, loss = 1.48047190\n",
      "Iteration 119, loss = 1.47973827\n",
      "Iteration 120, loss = 1.47900555\n",
      "Iteration 121, loss = 1.47827376\n",
      "Iteration 122, loss = 1.47754289\n",
      "Iteration 123, loss = 1.47681293\n",
      "Iteration 124, loss = 1.47608388\n",
      "Iteration 125, loss = 1.47535576\n",
      "Iteration 126, loss = 1.47462854\n",
      "Iteration 127, loss = 1.47390224\n",
      "Iteration 128, loss = 1.47317685\n",
      "Iteration 129, loss = 1.47245236\n",
      "Iteration 130, loss = 1.47172879\n",
      "Iteration 131, loss = 1.47100612\n",
      "Iteration 132, loss = 1.47028437\n",
      "Iteration 133, loss = 1.46956351\n",
      "Iteration 134, loss = 1.46884356\n",
      "Iteration 135, loss = 1.46812452\n",
      "Iteration 136, loss = 1.46740638\n",
      "Iteration 137, loss = 1.46668914\n",
      "Iteration 138, loss = 1.46597281\n",
      "Iteration 139, loss = 1.46525737\n",
      "Iteration 140, loss = 1.46454284\n",
      "Iteration 141, loss = 1.46382920\n",
      "Iteration 142, loss = 1.46311647\n",
      "Iteration 143, loss = 1.46240463\n",
      "Iteration 144, loss = 1.46169369\n",
      "Iteration 145, loss = 1.46098365\n",
      "Iteration 146, loss = 1.46027451\n",
      "Iteration 147, loss = 1.45956626\n",
      "Iteration 148, loss = 1.45885891\n",
      "Iteration 149, loss = 1.45815246\n",
      "Iteration 150, loss = 1.45744690\n",
      "Iteration 151, loss = 1.45674224\n",
      "Iteration 152, loss = 1.45603847\n",
      "Iteration 153, loss = 1.45533560\n",
      "Iteration 154, loss = 1.45463362\n",
      "Iteration 155, loss = 1.45393253\n",
      "Iteration 156, loss = 1.45323234\n",
      "Iteration 157, loss = 1.45253305\n",
      "Iteration 158, loss = 1.45183464\n",
      "Iteration 159, loss = 1.45113713\n",
      "Iteration 160, loss = 1.45044052\n",
      "Iteration 161, loss = 1.44974480\n",
      "Iteration 162, loss = 1.44904997\n",
      "Iteration 163, loss = 1.44835603\n",
      "Iteration 164, loss = 1.44766299\n",
      "Iteration 165, loss = 1.44697084\n",
      "Iteration 166, loss = 1.44627958\n",
      "Iteration 167, loss = 1.44558922\n",
      "Iteration 168, loss = 1.44489975\n",
      "Iteration 169, loss = 1.44421117\n",
      "Iteration 170, loss = 1.44352349\n",
      "Iteration 171, loss = 1.44283670\n",
      "Iteration 172, loss = 1.44215080\n",
      "Iteration 173, loss = 1.44146580\n",
      "Iteration 174, loss = 1.44078169\n",
      "Iteration 175, loss = 1.44009847\n",
      "Iteration 176, loss = 1.43941615\n",
      "Iteration 177, loss = 1.43873472\n",
      "Iteration 178, loss = 1.43805418\n",
      "Iteration 179, loss = 1.43737454\n",
      "Iteration 180, loss = 1.43669579\n",
      "Iteration 181, loss = 1.43601794\n",
      "Iteration 182, loss = 1.43534098\n",
      "Iteration 183, loss = 1.43466491\n",
      "Iteration 184, loss = 1.43398974\n",
      "Iteration 185, loss = 1.43331547\n",
      "Iteration 186, loss = 1.43264208\n",
      "Iteration 187, loss = 1.43196960\n",
      "Iteration 188, loss = 1.43129801\n",
      "Iteration 189, loss = 1.43062731\n",
      "Iteration 190, loss = 1.42995751\n",
      "Iteration 191, loss = 1.42928860\n",
      "Iteration 192, loss = 1.42862059\n",
      "Iteration 193, loss = 1.42795348\n",
      "Iteration 194, loss = 1.42728726\n",
      "Iteration 195, loss = 1.42662194\n",
      "Iteration 196, loss = 1.42595752\n",
      "Iteration 197, loss = 1.42529399\n",
      "Iteration 198, loss = 1.42463136\n",
      "Iteration 199, loss = 1.42396962\n",
      "Iteration 200, loss = 1.42330878\n",
      "Iteration 201, loss = 1.42264884\n",
      "Iteration 202, loss = 1.42198980\n",
      "Iteration 203, loss = 1.42133165\n",
      "Iteration 204, loss = 1.42067440\n",
      "Iteration 205, loss = 1.42001805\n",
      "Iteration 206, loss = 1.41936260\n",
      "Iteration 207, loss = 1.41870804\n",
      "Iteration 208, loss = 1.41805438\n",
      "Iteration 209, loss = 1.41740163\n",
      "Iteration 210, loss = 1.41674977\n",
      "Iteration 211, loss = 1.41609880\n",
      "Iteration 212, loss = 1.41544874\n",
      "Iteration 213, loss = 1.41479958\n",
      "Iteration 214, loss = 1.41415131\n",
      "Iteration 215, loss = 1.41350395\n",
      "Iteration 216, loss = 1.41285748\n",
      "Iteration 217, loss = 1.41221191\n",
      "Iteration 218, loss = 1.41156725\n",
      "Iteration 219, loss = 1.41092348\n",
      "Iteration 220, loss = 1.41028061\n",
      "Iteration 221, loss = 1.40963864\n",
      "Iteration 222, loss = 1.40899757\n",
      "Iteration 223, loss = 1.40835740\n",
      "Iteration 224, loss = 1.40771813\n",
      "Iteration 225, loss = 1.40707977\n",
      "Iteration 226, loss = 1.40644230\n",
      "Iteration 227, loss = 1.40580573\n",
      "Iteration 228, loss = 1.40517006\n",
      "Iteration 229, loss = 1.40453530\n",
      "Iteration 230, loss = 1.40390143\n",
      "Iteration 231, loss = 1.40326847\n",
      "Iteration 232, loss = 1.40263640\n",
      "Iteration 233, loss = 1.40200524\n",
      "Iteration 234, loss = 1.40137498\n",
      "Iteration 235, loss = 1.40074561\n",
      "Iteration 236, loss = 1.40011715\n",
      "Iteration 237, loss = 1.39948959\n",
      "Iteration 238, loss = 1.39886293\n",
      "Iteration 239, loss = 1.39823718\n",
      "Iteration 240, loss = 1.39761232\n",
      "Iteration 241, loss = 1.39698836\n",
      "Iteration 242, loss = 1.39636531\n",
      "Iteration 243, loss = 1.39574316\n",
      "Iteration 244, loss = 1.39512190\n",
      "Iteration 245, loss = 1.39450155\n",
      "Iteration 246, loss = 1.39388210\n",
      "Iteration 247, loss = 1.39326356\n",
      "Iteration 248, loss = 1.39264591\n",
      "Iteration 249, loss = 1.39202916\n",
      "Iteration 250, loss = 1.39141332\n",
      "Iteration 251, loss = 1.39079838\n",
      "Iteration 252, loss = 1.39018433\n",
      "Iteration 253, loss = 1.38957119\n",
      "Iteration 254, loss = 1.38895895\n",
      "Iteration 255, loss = 1.38834761\n",
      "Iteration 256, loss = 1.38773718\n",
      "Iteration 257, loss = 1.38712764\n",
      "Iteration 258, loss = 1.38651900\n",
      "Iteration 259, loss = 1.38591127\n",
      "Iteration 260, loss = 1.38530443\n",
      "Iteration 261, loss = 1.38469850\n",
      "Iteration 262, loss = 1.38409347\n",
      "Iteration 263, loss = 1.38348934\n",
      "Iteration 264, loss = 1.38288611\n",
      "Iteration 265, loss = 1.38228377\n",
      "Iteration 266, loss = 1.38168234\n",
      "Iteration 267, loss = 1.38108181\n",
      "Iteration 268, loss = 1.38048219\n",
      "Iteration 269, loss = 1.37988346\n",
      "Iteration 270, loss = 1.37928563\n",
      "Iteration 271, loss = 1.37868870\n",
      "Iteration 272, loss = 1.37809267\n",
      "Iteration 273, loss = 1.37749754\n",
      "Iteration 274, loss = 1.37690331\n",
      "Iteration 275, loss = 1.37630998\n",
      "Iteration 276, loss = 1.37571755\n",
      "Iteration 277, loss = 1.37512602\n",
      "Iteration 278, loss = 1.37453538\n",
      "Iteration 279, loss = 1.37394565\n",
      "Iteration 280, loss = 1.37335682\n",
      "Iteration 281, loss = 1.37276888\n",
      "Iteration 282, loss = 1.37218184\n",
      "Iteration 283, loss = 1.37159570\n",
      "Iteration 284, loss = 1.37101046\n",
      "Iteration 285, loss = 1.37042612\n",
      "Iteration 286, loss = 1.36984268\n",
      "Iteration 287, loss = 1.36926013\n",
      "Iteration 288, loss = 1.36867848\n",
      "Iteration 289, loss = 1.36809773\n",
      "Iteration 290, loss = 1.36751788\n",
      "Iteration 291, loss = 1.36693892\n",
      "Iteration 292, loss = 1.36636087\n",
      "Iteration 293, loss = 1.36578370\n",
      "Iteration 294, loss = 1.36520744\n",
      "Iteration 295, loss = 1.36463207\n",
      "Iteration 296, loss = 1.36405759\n",
      "Iteration 297, loss = 1.36348402\n",
      "Iteration 298, loss = 1.36291134\n",
      "Iteration 299, loss = 1.36233955\n",
      "Iteration 300, loss = 1.36176866\n",
      "Iteration 301, loss = 1.36119867\n",
      "Iteration 302, loss = 1.36062957\n",
      "Iteration 303, loss = 1.36006136\n",
      "Iteration 304, loss = 1.35949405\n",
      "Iteration 305, loss = 1.35892763\n",
      "Iteration 306, loss = 1.35836211\n",
      "Iteration 307, loss = 1.35779748\n",
      "Iteration 308, loss = 1.35723375\n",
      "Iteration 309, loss = 1.35667091\n",
      "Iteration 310, loss = 1.35610896\n",
      "Iteration 311, loss = 1.35554790\n",
      "Iteration 312, loss = 1.35498774\n",
      "Iteration 313, loss = 1.35442847\n",
      "Iteration 314, loss = 1.35387009\n",
      "Iteration 315, loss = 1.35331261\n",
      "Iteration 316, loss = 1.35275601\n",
      "Iteration 317, loss = 1.35220031\n",
      "Iteration 318, loss = 1.35164549\n",
      "Iteration 319, loss = 1.35109157\n",
      "Iteration 320, loss = 1.35053854\n",
      "Iteration 321, loss = 1.34998640\n",
      "Iteration 322, loss = 1.34943515\n",
      "Iteration 323, loss = 1.34888478\n",
      "Iteration 324, loss = 1.34833531\n",
      "Iteration 325, loss = 1.34778673\n",
      "Iteration 326, loss = 1.34723903\n",
      "Iteration 327, loss = 1.34669222\n",
      "Iteration 328, loss = 1.34614630\n",
      "Iteration 329, loss = 1.34560127\n",
      "Iteration 330, loss = 1.34505713\n",
      "Iteration 331, loss = 1.34451387\n",
      "Iteration 332, loss = 1.34397150\n",
      "Iteration 333, loss = 1.34343002\n",
      "Iteration 334, loss = 1.34288942\n",
      "Iteration 335, loss = 1.34234971\n",
      "Iteration 336, loss = 1.34181088\n",
      "Iteration 337, loss = 1.34127294\n",
      "Iteration 338, loss = 1.34073588\n",
      "Iteration 339, loss = 1.34019971\n",
      "Iteration 340, loss = 1.33966442\n",
      "Iteration 341, loss = 1.33913002\n",
      "Iteration 342, loss = 1.33859650\n",
      "Iteration 343, loss = 1.33806386\n",
      "Iteration 344, loss = 1.33753210\n",
      "Iteration 345, loss = 1.33700123\n",
      "Iteration 346, loss = 1.33647124\n",
      "Iteration 347, loss = 1.33594213\n",
      "Iteration 348, loss = 1.33541390\n",
      "Iteration 349, loss = 1.33488655\n",
      "Iteration 350, loss = 1.33436008\n",
      "Iteration 351, loss = 1.33383449\n",
      "Iteration 352, loss = 1.33330978\n",
      "Iteration 353, loss = 1.33278595\n",
      "Iteration 354, loss = 1.33226300\n",
      "Iteration 355, loss = 1.33174093\n",
      "Iteration 356, loss = 1.33121973\n",
      "Iteration 357, loss = 1.33069942\n",
      "Iteration 358, loss = 1.33017997\n",
      "Iteration 359, loss = 1.32966141\n",
      "Iteration 360, loss = 1.32914372\n",
      "Iteration 361, loss = 1.32862691\n",
      "Iteration 362, loss = 1.32811098\n",
      "Iteration 363, loss = 1.32759592\n",
      "Iteration 364, loss = 1.32708173\n",
      "Iteration 365, loss = 1.32656842\n",
      "Iteration 366, loss = 1.32605598\n",
      "Iteration 367, loss = 1.32554441\n",
      "Iteration 368, loss = 1.32503372\n",
      "Iteration 369, loss = 1.32452390\n",
      "Iteration 370, loss = 1.32401495\n",
      "Iteration 371, loss = 1.32350687\n",
      "Iteration 372, loss = 1.32299967\n",
      "Iteration 373, loss = 1.32249333\n",
      "Iteration 374, loss = 1.32198787\n",
      "Iteration 375, loss = 1.32148327\n",
      "Iteration 376, loss = 1.32097955\n",
      "Iteration 377, loss = 1.32047669\n",
      "Iteration 378, loss = 1.31997470\n",
      "Iteration 379, loss = 1.31947358\n",
      "Iteration 380, loss = 1.31897332\n",
      "Iteration 381, loss = 1.31847394\n",
      "Iteration 382, loss = 1.31797541\n",
      "Iteration 383, loss = 1.31747776\n",
      "Iteration 384, loss = 1.31698097\n",
      "Iteration 385, loss = 1.31648504\n",
      "Iteration 386, loss = 1.31598998\n",
      "Iteration 387, loss = 1.31549579\n",
      "Iteration 388, loss = 1.31500245\n",
      "Iteration 389, loss = 1.31450998\n",
      "Iteration 390, loss = 1.31401837\n",
      "Iteration 391, loss = 1.31352763\n",
      "Iteration 392, loss = 1.31303774\n",
      "Iteration 393, loss = 1.31254872\n",
      "Iteration 394, loss = 1.31206055\n",
      "Iteration 395, loss = 1.31157325\n",
      "Iteration 396, loss = 1.31108680\n",
      "Iteration 397, loss = 1.31060122\n",
      "Iteration 398, loss = 1.31011649\n",
      "Iteration 399, loss = 1.30963262\n",
      "Iteration 400, loss = 1.30914960\n",
      "Iteration 401, loss = 1.30866744\n",
      "Iteration 402, loss = 1.30818614\n",
      "Iteration 403, loss = 1.30770570\n",
      "Iteration 404, loss = 1.30722611\n",
      "Iteration 405, loss = 1.30674737\n",
      "Iteration 406, loss = 1.30626949\n",
      "Iteration 407, loss = 1.30579246\n",
      "Iteration 408, loss = 1.30531628\n",
      "Iteration 409, loss = 1.30484096\n",
      "Iteration 410, loss = 1.30436648\n",
      "Iteration 411, loss = 1.30389286\n",
      "Iteration 412, loss = 1.30342009\n",
      "Iteration 413, loss = 1.30294817\n",
      "Iteration 414, loss = 1.30247709\n",
      "Iteration 415, loss = 1.30200687\n",
      "Iteration 416, loss = 1.30153750\n",
      "Iteration 417, loss = 1.30106897\n",
      "Iteration 418, loss = 1.30060129\n",
      "Iteration 419, loss = 1.30013445\n",
      "Iteration 420, loss = 1.29966846\n",
      "Iteration 421, loss = 1.29920332\n",
      "Iteration 422, loss = 1.29873902\n",
      "Iteration 423, loss = 1.29827556\n",
      "Iteration 424, loss = 1.29781295\n",
      "Iteration 425, loss = 1.29735118\n",
      "Iteration 426, loss = 1.29689026\n",
      "Iteration 427, loss = 1.29643017\n",
      "Iteration 428, loss = 1.29597093\n",
      "Iteration 429, loss = 1.29551252\n",
      "Iteration 430, loss = 1.29505496\n",
      "Iteration 431, loss = 1.29459823\n",
      "Iteration 432, loss = 1.29414235\n",
      "Iteration 433, loss = 1.29368730\n",
      "Iteration 434, loss = 1.29323308\n",
      "Iteration 435, loss = 1.29277971\n",
      "Iteration 436, loss = 1.29232717\n",
      "Iteration 437, loss = 1.29187547\n",
      "Iteration 438, loss = 1.29142460\n",
      "Iteration 439, loss = 1.29097456\n",
      "Iteration 440, loss = 1.29052536\n",
      "Iteration 441, loss = 1.29007699\n",
      "Iteration 442, loss = 1.28962945\n",
      "Iteration 443, loss = 1.28918275\n",
      "Iteration 444, loss = 1.28873687\n",
      "Iteration 445, loss = 1.28829183\n",
      "Iteration 446, loss = 1.28784762\n",
      "Iteration 447, loss = 1.28740423\n",
      "Iteration 448, loss = 1.28696167\n",
      "Iteration 449, loss = 1.28651994\n",
      "Iteration 450, loss = 1.28607904\n",
      "Iteration 451, loss = 1.28563896\n",
      "Iteration 452, loss = 1.28519971\n",
      "Iteration 453, loss = 1.28476128\n",
      "Iteration 454, loss = 1.28432368\n",
      "Iteration 455, loss = 1.28388690\n",
      "Iteration 456, loss = 1.28345094\n",
      "Iteration 457, loss = 1.28301581\n",
      "Iteration 458, loss = 1.28258150\n",
      "Iteration 459, loss = 1.28214800\n",
      "Iteration 460, loss = 1.28171533\n",
      "Iteration 461, loss = 1.28128348\n",
      "Iteration 462, loss = 1.28085244\n",
      "Iteration 463, loss = 1.28042223\n",
      "Iteration 464, loss = 1.27999283\n",
      "Iteration 465, loss = 1.27956425\n",
      "Iteration 466, loss = 1.27913648\n",
      "Iteration 467, loss = 1.27870953\n",
      "Iteration 468, loss = 1.27828339\n",
      "Iteration 469, loss = 1.27785807\n",
      "Iteration 470, loss = 1.27743356\n",
      "Iteration 471, loss = 1.27700986\n",
      "Iteration 472, loss = 1.27658697\n",
      "Iteration 473, loss = 1.27616490\n",
      "Iteration 474, loss = 1.27574363\n",
      "Iteration 475, loss = 1.27532318\n",
      "Iteration 476, loss = 1.27490353\n",
      "Iteration 477, loss = 1.27448469\n",
      "Iteration 478, loss = 1.27406666\n",
      "Iteration 479, loss = 1.27364943\n",
      "Iteration 480, loss = 1.27323301\n",
      "Iteration 481, loss = 1.27281740\n",
      "Iteration 482, loss = 1.27240259\n",
      "Iteration 483, loss = 1.27198858\n",
      "Iteration 484, loss = 1.27157538\n",
      "Iteration 485, loss = 1.27116298\n",
      "Iteration 486, loss = 1.27075138\n",
      "Iteration 487, loss = 1.27034058\n",
      "Iteration 488, loss = 1.26993058\n",
      "Iteration 489, loss = 1.26952137\n",
      "Iteration 490, loss = 1.26911297\n",
      "Iteration 491, loss = 1.26870537\n",
      "Iteration 492, loss = 1.26829856\n",
      "Iteration 493, loss = 1.26789255\n",
      "Iteration 494, loss = 1.26748733\n",
      "Iteration 495, loss = 1.26708291\n",
      "Iteration 496, loss = 1.26667928\n",
      "Iteration 497, loss = 1.26627644\n",
      "Iteration 498, loss = 1.26587440\n",
      "Iteration 499, loss = 1.26547315\n",
      "Iteration 500, loss = 1.26507268\n",
      "Iteration 501, loss = 1.26467301\n",
      "Iteration 502, loss = 1.26427413\n",
      "Iteration 503, loss = 1.26387604\n",
      "Iteration 504, loss = 1.26347873\n",
      "Iteration 505, loss = 1.26308221\n",
      "Iteration 506, loss = 1.26268647\n",
      "Iteration 507, loss = 1.26229153\n",
      "Iteration 508, loss = 1.26189736\n",
      "Iteration 509, loss = 1.26150398\n",
      "Iteration 510, loss = 1.26111138\n",
      "Iteration 511, loss = 1.26071957\n",
      "Iteration 512, loss = 1.26032853\n",
      "Iteration 513, loss = 1.25993828\n",
      "Iteration 514, loss = 1.25954880\n",
      "Iteration 515, loss = 1.25916011\n",
      "Iteration 516, loss = 1.25877219\n",
      "Iteration 517, loss = 1.25838505\n",
      "Iteration 518, loss = 1.25799868\n",
      "Iteration 519, loss = 1.25761309\n",
      "Iteration 520, loss = 1.25722828\n",
      "Iteration 521, loss = 1.25684424\n",
      "Iteration 522, loss = 1.25646097\n",
      "Iteration 523, loss = 1.25607848\n",
      "Iteration 524, loss = 1.25569675\n",
      "Iteration 525, loss = 1.25531580\n",
      "Iteration 526, loss = 1.25493562\n",
      "Iteration 527, loss = 1.25455620\n",
      "Iteration 528, loss = 1.25417756\n",
      "Iteration 529, loss = 1.25379968\n",
      "Iteration 530, loss = 1.25342257\n",
      "Iteration 531, loss = 1.25304622\n",
      "Iteration 532, loss = 1.25267064\n",
      "Iteration 533, loss = 1.25229582\n",
      "Iteration 534, loss = 1.25192177\n",
      "Iteration 535, loss = 1.25154848\n",
      "Iteration 536, loss = 1.25117595\n",
      "Iteration 537, loss = 1.25080418\n",
      "Iteration 538, loss = 1.25043317\n",
      "Iteration 539, loss = 1.25006292\n",
      "Iteration 540, loss = 1.24969343\n",
      "Iteration 541, loss = 1.24932469\n",
      "Iteration 542, loss = 1.24895671\n",
      "Iteration 543, loss = 1.24858949\n",
      "Iteration 544, loss = 1.24822302\n",
      "Iteration 545, loss = 1.24785731\n",
      "Iteration 546, loss = 1.24749235\n",
      "Iteration 547, loss = 1.24712814\n",
      "Iteration 548, loss = 1.24676468\n",
      "Iteration 549, loss = 1.24640198\n",
      "Iteration 550, loss = 1.24604002\n",
      "Iteration 551, loss = 1.24567881\n",
      "Iteration 552, loss = 1.24531835\n",
      "Iteration 553, loss = 1.24495863\n",
      "Iteration 554, loss = 1.24459967\n",
      "Iteration 555, loss = 1.24424144\n",
      "Iteration 556, loss = 1.24388397\n",
      "Iteration 557, loss = 1.24352723\n",
      "Iteration 558, loss = 1.24317124\n",
      "Iteration 559, loss = 1.24281599\n",
      "Iteration 560, loss = 1.24246148\n",
      "Iteration 561, loss = 1.24210771\n",
      "Iteration 562, loss = 1.24175468\n",
      "Iteration 563, loss = 1.24140239\n",
      "Iteration 564, loss = 1.24105084\n",
      "Iteration 565, loss = 1.24070002\n",
      "Iteration 566, loss = 1.24034994\n",
      "Iteration 567, loss = 1.24000059\n",
      "Iteration 568, loss = 1.23965198\n",
      "Iteration 569, loss = 1.23930410\n",
      "Iteration 570, loss = 1.23895695\n",
      "Iteration 571, loss = 1.23861053\n",
      "Iteration 572, loss = 1.23826485\n",
      "Iteration 573, loss = 1.23791989\n",
      "Iteration 574, loss = 1.23757566\n",
      "Iteration 575, loss = 1.23723216\n",
      "Iteration 576, loss = 1.23688939\n",
      "Iteration 577, loss = 1.23654734\n",
      "Iteration 578, loss = 1.23620602\n",
      "Iteration 579, loss = 1.23586542\n",
      "Iteration 580, loss = 1.23552554\n",
      "Iteration 581, loss = 1.23518639\n",
      "Iteration 582, loss = 1.23484796\n",
      "Iteration 583, loss = 1.23451024\n",
      "Iteration 584, loss = 1.23417325\n",
      "Iteration 585, loss = 1.23383698\n",
      "Iteration 586, loss = 1.23350142\n",
      "Iteration 587, loss = 1.23316658\n",
      "Iteration 588, loss = 1.23283246\n",
      "Iteration 589, loss = 1.23249905\n",
      "Iteration 590, loss = 1.23216636\n",
      "Iteration 591, loss = 1.23183438\n",
      "Iteration 592, loss = 1.23150311\n",
      "Iteration 593, loss = 1.23117255\n",
      "Iteration 594, loss = 1.23084270\n",
      "Iteration 595, loss = 1.23051356\n",
      "Iteration 596, loss = 1.23018514\n",
      "Iteration 597, loss = 1.22985741\n",
      "Iteration 598, loss = 1.22953040\n",
      "Iteration 599, loss = 1.22920409\n",
      "Iteration 600, loss = 1.22887849\n",
      "Iteration 601, loss = 1.22855359\n",
      "Iteration 602, loss = 1.22822939\n",
      "Iteration 603, loss = 1.22790590\n",
      "Iteration 604, loss = 1.22758310\n",
      "Iteration 605, loss = 1.22726101\n",
      "Iteration 606, loss = 1.22693962\n",
      "Iteration 607, loss = 1.22661892\n",
      "Iteration 608, loss = 1.22629892\n",
      "Iteration 609, loss = 1.22597962\n",
      "Iteration 610, loss = 1.22566101\n",
      "Iteration 611, loss = 1.22534310\n",
      "Iteration 612, loss = 1.22502589\n",
      "Iteration 613, loss = 1.22470936\n",
      "Iteration 614, loss = 1.22439353\n",
      "Iteration 615, loss = 1.22407839\n",
      "Iteration 616, loss = 1.22376394\n",
      "Iteration 617, loss = 1.22345017\n",
      "Iteration 618, loss = 1.22313710\n",
      "Iteration 619, loss = 1.22282471\n",
      "Iteration 620, loss = 1.22251301\n",
      "Iteration 621, loss = 1.22220199\n",
      "Iteration 622, loss = 1.22189166\n",
      "Iteration 623, loss = 1.22158201\n",
      "Iteration 624, loss = 1.22127305\n",
      "Iteration 625, loss = 1.22096476\n",
      "Iteration 626, loss = 1.22065716\n",
      "Iteration 627, loss = 1.22035024\n",
      "Iteration 628, loss = 1.22004399\n",
      "Iteration 629, loss = 1.21973842\n",
      "Iteration 630, loss = 1.21943353\n",
      "Iteration 631, loss = 1.21912932\n",
      "Iteration 632, loss = 1.21882578\n",
      "Iteration 633, loss = 1.21852291\n",
      "Iteration 634, loss = 1.21822072\n",
      "Iteration 635, loss = 1.21791920\n",
      "Iteration 636, loss = 1.21761835\n",
      "Iteration 637, loss = 1.21731817\n",
      "Iteration 638, loss = 1.21701865\n",
      "Iteration 639, loss = 1.21671981\n",
      "Iteration 640, loss = 1.21642164\n",
      "Iteration 641, loss = 1.21612413\n",
      "Iteration 642, loss = 1.21582728\n",
      "Iteration 643, loss = 1.21553110\n",
      "Iteration 644, loss = 1.21523559\n",
      "Iteration 645, loss = 1.21494073\n",
      "Iteration 646, loss = 1.21464654\n",
      "Iteration 647, loss = 1.21435301\n",
      "Iteration 648, loss = 1.21406014\n",
      "Iteration 649, loss = 1.21376792\n",
      "Iteration 650, loss = 1.21347637\n",
      "Iteration 651, loss = 1.21318547\n",
      "Iteration 652, loss = 1.21289522\n",
      "Iteration 653, loss = 1.21260563\n",
      "Iteration 654, loss = 1.21231670\n",
      "Iteration 655, loss = 1.21202842\n",
      "Iteration 656, loss = 1.21174079\n",
      "Iteration 657, loss = 1.21145381\n",
      "Iteration 658, loss = 1.21116748\n",
      "Iteration 659, loss = 1.21088179\n",
      "Iteration 660, loss = 1.21059676\n",
      "Iteration 661, loss = 1.21031238\n",
      "Iteration 662, loss = 1.21002864\n",
      "Iteration 663, loss = 1.20974554\n",
      "Iteration 664, loss = 1.20946309\n",
      "Iteration 665, loss = 1.20918128\n",
      "Iteration 666, loss = 1.20890012\n",
      "Iteration 667, loss = 1.20861959\n",
      "Iteration 668, loss = 1.20833971\n",
      "Iteration 669, loss = 1.20806046\n",
      "Iteration 670, loss = 1.20778185\n",
      "Iteration 671, loss = 1.20750388\n",
      "Iteration 672, loss = 1.20722655\n",
      "Iteration 673, loss = 1.20694985\n",
      "Iteration 674, loss = 1.20667379\n",
      "Iteration 675, loss = 1.20639836\n",
      "Iteration 676, loss = 1.20612356\n",
      "Iteration 677, loss = 1.20584939\n",
      "Iteration 678, loss = 1.20557586\n",
      "Iteration 679, loss = 1.20530295\n",
      "Iteration 680, loss = 1.20503067\n",
      "Iteration 681, loss = 1.20475902\n",
      "Iteration 682, loss = 1.20448800\n",
      "Iteration 683, loss = 1.20421760\n",
      "Iteration 684, loss = 1.20394783\n",
      "Iteration 685, loss = 1.20367868\n",
      "Iteration 686, loss = 1.20341015\n",
      "Iteration 687, loss = 1.20314224\n",
      "Iteration 688, loss = 1.20287496\n",
      "Iteration 689, loss = 1.20260829\n",
      "Iteration 690, loss = 1.20234224\n",
      "Iteration 691, loss = 1.20207681\n",
      "Iteration 692, loss = 1.20181200\n",
      "Iteration 693, loss = 1.20154780\n",
      "Iteration 694, loss = 1.20128422\n",
      "Iteration 695, loss = 1.20102125\n",
      "Iteration 696, loss = 1.20075890\n",
      "Iteration 697, loss = 1.20049715\n",
      "Iteration 698, loss = 1.20023602\n",
      "Iteration 699, loss = 1.19997550\n",
      "Iteration 700, loss = 1.19971558\n",
      "Iteration 701, loss = 1.19945627\n",
      "Iteration 702, loss = 1.19919758\n",
      "Iteration 703, loss = 1.19893948\n",
      "Iteration 704, loss = 1.19868199\n",
      "Iteration 705, loss = 1.19842511\n",
      "Iteration 706, loss = 1.19816883\n",
      "Iteration 707, loss = 1.19791315\n",
      "Iteration 708, loss = 1.19765807\n",
      "Iteration 709, loss = 1.19740359\n",
      "Iteration 710, loss = 1.19714971\n",
      "Iteration 711, loss = 1.19689643\n",
      "Iteration 712, loss = 1.19664374\n",
      "Iteration 713, loss = 1.19639165\n",
      "Iteration 714, loss = 1.19614016\n",
      "Iteration 715, loss = 1.19588926\n",
      "Iteration 716, loss = 1.19563895\n",
      "Iteration 717, loss = 1.19538924\n",
      "Iteration 718, loss = 1.19514012\n",
      "Iteration 719, loss = 1.19489158\n",
      "Iteration 720, loss = 1.19464364\n",
      "Iteration 721, loss = 1.19439629\n",
      "Iteration 722, loss = 1.19414952\n",
      "Iteration 723, loss = 1.19390333\n",
      "Iteration 724, loss = 1.19365774\n",
      "Iteration 725, loss = 1.19341273\n",
      "Iteration 726, loss = 1.19316830\n",
      "Iteration 727, loss = 1.19292445\n",
      "Iteration 728, loss = 1.19268118\n",
      "Iteration 729, loss = 1.19243850\n",
      "Iteration 730, loss = 1.19219639\n",
      "Iteration 731, loss = 1.19195486\n",
      "Iteration 732, loss = 1.19171391\n",
      "Iteration 733, loss = 1.19147354\n",
      "Iteration 734, loss = 1.19123374\n",
      "Iteration 735, loss = 1.19099452\n",
      "Iteration 736, loss = 1.19075587\n",
      "Iteration 737, loss = 1.19051779\n",
      "Iteration 738, loss = 1.19028028\n",
      "Iteration 739, loss = 1.19004335\n",
      "Iteration 740, loss = 1.18980698\n",
      "Iteration 741, loss = 1.18957118\n",
      "Iteration 742, loss = 1.18933595\n",
      "Iteration 743, loss = 1.18910129\n",
      "Iteration 744, loss = 1.18886719\n",
      "Iteration 745, loss = 1.18863365\n",
      "Iteration 746, loss = 1.18840068\n",
      "Iteration 747, loss = 1.18816827\n",
      "Iteration 748, loss = 1.18793643\n",
      "Iteration 749, loss = 1.18770514\n",
      "Iteration 750, loss = 1.18747441\n",
      "Iteration 751, loss = 1.18724425\n",
      "Iteration 752, loss = 1.18701464\n",
      "Iteration 753, loss = 1.18678558\n",
      "Iteration 754, loss = 1.18655709\n",
      "Iteration 755, loss = 1.18632914\n",
      "Iteration 756, loss = 1.18610176\n",
      "Iteration 757, loss = 1.18587492\n",
      "Iteration 758, loss = 1.18564864\n",
      "Iteration 759, loss = 1.18542290\n",
      "Iteration 760, loss = 1.18519772\n",
      "Iteration 761, loss = 1.18497308\n",
      "Iteration 762, loss = 1.18474900\n",
      "Iteration 763, loss = 1.18452546\n",
      "Iteration 764, loss = 1.18430246\n",
      "Iteration 765, loss = 1.18408001\n",
      "Iteration 766, loss = 1.18385811\n",
      "Iteration 767, loss = 1.18363674\n",
      "Iteration 768, loss = 1.18341592\n",
      "Iteration 769, loss = 1.18319564\n",
      "Iteration 770, loss = 1.18297591\n",
      "Iteration 771, loss = 1.18275671\n",
      "Iteration 772, loss = 1.18253804\n",
      "Iteration 773, loss = 1.18231992\n",
      "Iteration 774, loss = 1.18210233\n",
      "Iteration 775, loss = 1.18188527\n",
      "Iteration 776, loss = 1.18166876\n",
      "Iteration 777, loss = 1.18145277\n",
      "Iteration 778, loss = 1.18123731\n",
      "Iteration 779, loss = 1.18102239\n",
      "Iteration 780, loss = 1.18080800\n",
      "Iteration 781, loss = 1.18059414\n",
      "Iteration 782, loss = 1.18038080\n",
      "Iteration 783, loss = 1.18016799\n",
      "Iteration 784, loss = 1.17995571\n",
      "Iteration 785, loss = 1.17974395\n",
      "Iteration 786, loss = 1.17953272\n",
      "Iteration 787, loss = 1.17932201\n",
      "Iteration 788, loss = 1.17911183\n",
      "Iteration 789, loss = 1.17890216\n",
      "Iteration 790, loss = 1.17869302\n",
      "Iteration 791, loss = 1.17848440\n",
      "Iteration 792, loss = 1.17827629\n",
      "Iteration 793, loss = 1.17806870\n",
      "Iteration 794, loss = 1.17786163\n",
      "Iteration 795, loss = 1.17765508\n",
      "Iteration 796, loss = 1.17744904\n",
      "Iteration 797, loss = 1.17724351\n",
      "Iteration 798, loss = 1.17703849\n",
      "Iteration 799, loss = 1.17683399\n",
      "Iteration 800, loss = 1.17663000\n",
      "Iteration 801, loss = 1.17642652\n",
      "Iteration 802, loss = 1.17622355\n",
      "Iteration 803, loss = 1.17602108\n",
      "Iteration 804, loss = 1.17581912\n",
      "Iteration 805, loss = 1.17561767\n",
      "Iteration 806, loss = 1.17541672\n",
      "Iteration 807, loss = 1.17521628\n",
      "Iteration 808, loss = 1.17501634\n",
      "Iteration 809, loss = 1.17481690\n",
      "Iteration 810, loss = 1.17461797\n",
      "Iteration 811, loss = 1.17441953\n",
      "Iteration 812, loss = 1.17422160\n",
      "Iteration 813, loss = 1.17402416\n",
      "Iteration 814, loss = 1.17382722\n",
      "Iteration 815, loss = 1.17363077\n",
      "Iteration 816, loss = 1.17343482\n",
      "Iteration 817, loss = 1.17323937\n",
      "Iteration 818, loss = 1.17304441\n",
      "Iteration 819, loss = 1.17284994\n",
      "Iteration 820, loss = 1.17265596\n",
      "Iteration 821, loss = 1.17246247\n",
      "Iteration 822, loss = 1.17226948\n",
      "Iteration 823, loss = 1.17207697\n",
      "Iteration 824, loss = 1.17188495\n",
      "Iteration 825, loss = 1.17169342\n",
      "Iteration 826, loss = 1.17150237\n",
      "Iteration 827, loss = 1.17131181\n",
      "Iteration 828, loss = 1.17112173\n",
      "Iteration 829, loss = 1.17093213\n",
      "Iteration 830, loss = 1.17074302\n",
      "Iteration 831, loss = 1.17055439\n",
      "Iteration 832, loss = 1.17036624\n",
      "Iteration 833, loss = 1.17017856\n",
      "Iteration 834, loss = 1.16999137\n",
      "Iteration 835, loss = 1.16980465\n",
      "Iteration 836, loss = 1.16961841\n",
      "Iteration 837, loss = 1.16943265\n",
      "Iteration 838, loss = 1.16924736\n",
      "Iteration 839, loss = 1.16906254\n",
      "Iteration 840, loss = 1.16887820\n",
      "Iteration 841, loss = 1.16869432\n",
      "Iteration 842, loss = 1.16851092\n",
      "Iteration 843, loss = 1.16832799\n",
      "Iteration 844, loss = 1.16814553\n",
      "Iteration 845, loss = 1.16796353\n",
      "Iteration 846, loss = 1.16778200\n",
      "Iteration 847, loss = 1.16760094\n",
      "Iteration 848, loss = 1.16742034\n",
      "Iteration 849, loss = 1.16724021\n",
      "Iteration 850, loss = 1.16706054\n",
      "Iteration 851, loss = 1.16688133\n",
      "Iteration 852, loss = 1.16670259\n",
      "Iteration 853, loss = 1.16652430\n",
      "Iteration 854, loss = 1.16634648\n",
      "Iteration 855, loss = 1.16616911\n",
      "Iteration 856, loss = 1.16599220\n",
      "Iteration 857, loss = 1.16581575\n",
      "Iteration 858, loss = 1.16563975\n",
      "Iteration 859, loss = 1.16546421\n",
      "Iteration 860, loss = 1.16528912\n",
      "Iteration 861, loss = 1.16511448\n",
      "Iteration 862, loss = 1.16494030\n",
      "Iteration 863, loss = 1.16476657\n",
      "Iteration 864, loss = 1.16459328\n",
      "Iteration 865, loss = 1.16442045\n",
      "Iteration 866, loss = 1.16424807\n",
      "Iteration 867, loss = 1.16407613\n",
      "Iteration 868, loss = 1.16390464\n",
      "Iteration 869, loss = 1.16373359\n",
      "Iteration 870, loss = 1.16356299\n",
      "Iteration 871, loss = 1.16339284\n",
      "Iteration 872, loss = 1.16322312\n",
      "Iteration 873, loss = 1.16305385\n",
      "Iteration 874, loss = 1.16288502\n",
      "Iteration 875, loss = 1.16271663\n",
      "Iteration 876, loss = 1.16254868\n",
      "Iteration 877, loss = 1.16238116\n",
      "Iteration 878, loss = 1.16221409\n",
      "Iteration 879, loss = 1.16204745\n",
      "Iteration 880, loss = 1.16188124\n",
      "Iteration 881, loss = 1.16171547\n",
      "Iteration 882, loss = 1.16155014\n",
      "Iteration 883, loss = 1.16138524\n",
      "Iteration 884, loss = 1.16122076\n",
      "Iteration 885, loss = 1.16105672\n",
      "Iteration 886, loss = 1.16089311\n",
      "Iteration 887, loss = 1.16072993\n",
      "Iteration 888, loss = 1.16056718\n",
      "Iteration 889, loss = 1.16040485\n",
      "Iteration 890, loss = 1.16024296\n",
      "Iteration 891, loss = 1.16008148\n",
      "Iteration 892, loss = 1.15992043\n",
      "Iteration 893, loss = 1.15975981\n",
      "Iteration 894, loss = 1.15959961\n",
      "Iteration 895, loss = 1.15943982\n",
      "Iteration 896, loss = 1.15928047\n",
      "Iteration 897, loss = 1.15912153\n",
      "Iteration 898, loss = 1.15896301\n",
      "Iteration 899, loss = 1.15880490\n",
      "Iteration 900, loss = 1.15864722\n",
      "Iteration 901, loss = 1.15848995\n",
      "Iteration 902, loss = 1.15833310\n",
      "Iteration 903, loss = 1.15817666\n",
      "Iteration 904, loss = 1.15802064\n",
      "Iteration 905, loss = 1.15786503\n",
      "Iteration 906, loss = 1.15770983\n",
      "Iteration 907, loss = 1.15755504\n",
      "Iteration 908, loss = 1.15740067\n",
      "Iteration 909, loss = 1.15724670\n",
      "Iteration 910, loss = 1.15709314\n",
      "Iteration 911, loss = 1.15693999\n",
      "Iteration 912, loss = 1.15678725\n",
      "Iteration 913, loss = 1.15663491\n",
      "Iteration 914, loss = 1.15648297\n",
      "Iteration 915, loss = 1.15633145\n",
      "Iteration 916, loss = 1.15618032\n",
      "Iteration 917, loss = 1.15602960\n",
      "Iteration 918, loss = 1.15587927\n",
      "Iteration 919, loss = 1.15572935\n",
      "Iteration 920, loss = 1.15557983\n",
      "Iteration 921, loss = 1.15543071\n",
      "Iteration 922, loss = 1.15528198\n",
      "Iteration 923, loss = 1.15513365\n",
      "Iteration 924, loss = 1.15498572\n",
      "Iteration 925, loss = 1.15483819\n",
      "Iteration 926, loss = 1.15469105\n",
      "Iteration 927, loss = 1.15454430\n",
      "Iteration 928, loss = 1.15439794\n",
      "Iteration 929, loss = 1.15425198\n",
      "Iteration 930, loss = 1.15410641\n",
      "Iteration 931, loss = 1.15396123\n",
      "Iteration 932, loss = 1.15381643\n",
      "Iteration 933, loss = 1.15367203\n",
      "Iteration 934, loss = 1.15352801\n",
      "Iteration 935, loss = 1.15338438\n",
      "Iteration 936, loss = 1.15324114\n",
      "Iteration 937, loss = 1.15309828\n",
      "Iteration 938, loss = 1.15295580\n",
      "Iteration 939, loss = 1.15281371\n",
      "Iteration 940, loss = 1.15267200\n",
      "Iteration 941, loss = 1.15253068\n",
      "Iteration 942, loss = 1.15238973\n",
      "Iteration 943, loss = 1.15224916\n",
      "Iteration 944, loss = 1.15210898\n",
      "Iteration 945, loss = 1.15196917\n",
      "Iteration 946, loss = 1.15182973\n",
      "Iteration 947, loss = 1.15169068\n",
      "Iteration 948, loss = 1.15155200\n",
      "Iteration 949, loss = 1.15141369\n",
      "Iteration 950, loss = 1.15127576\n",
      "Iteration 951, loss = 1.15113820\n",
      "Iteration 952, loss = 1.15100102\n",
      "Iteration 953, loss = 1.15086420\n",
      "Iteration 954, loss = 1.15072776\n",
      "Iteration 955, loss = 1.15059169\n",
      "Iteration 956, loss = 1.15045598\n",
      "Iteration 957, loss = 1.15032064\n",
      "Iteration 958, loss = 1.15018568\n",
      "Iteration 959, loss = 1.15005107\n",
      "Iteration 960, loss = 1.14991683\n",
      "Iteration 961, loss = 1.14978296\n",
      "Iteration 962, loss = 1.14964945\n",
      "Iteration 963, loss = 1.14951631\n",
      "Iteration 964, loss = 1.14938352\n",
      "Iteration 965, loss = 1.14925110\n",
      "Iteration 966, loss = 1.14911904\n",
      "Iteration 967, loss = 1.14898734\n",
      "Iteration 968, loss = 1.14885599\n",
      "Iteration 969, loss = 1.14872501\n",
      "Iteration 970, loss = 1.14859438\n",
      "Iteration 971, loss = 1.14846411\n",
      "Iteration 972, loss = 1.14833419\n",
      "Iteration 973, loss = 1.14820463\n",
      "Iteration 974, loss = 1.14807543\n",
      "Iteration 975, loss = 1.14794657\n",
      "Iteration 976, loss = 1.14781807\n",
      "Iteration 977, loss = 1.14768992\n",
      "Iteration 978, loss = 1.14756213\n",
      "Iteration 979, loss = 1.14743468\n",
      "Iteration 980, loss = 1.14730758\n",
      "Iteration 981, loss = 1.14718083\n",
      "Iteration 982, loss = 1.14705442\n",
      "Iteration 983, loss = 1.14692837\n",
      "Iteration 984, loss = 1.14680266\n",
      "Iteration 985, loss = 1.14667729\n",
      "Iteration 986, loss = 1.14655227\n",
      "Iteration 987, loss = 1.14642759\n",
      "Iteration 988, loss = 1.14630325\n",
      "Iteration 989, loss = 1.14617926\n",
      "Iteration 990, loss = 1.14605561\n",
      "Iteration 991, loss = 1.14593230\n",
      "Iteration 992, loss = 1.14580932\n",
      "Iteration 993, loss = 1.14568669\n",
      "Iteration 994, loss = 1.14556439\n",
      "Iteration 995, loss = 1.14544244\n",
      "Iteration 996, loss = 1.14532081\n",
      "Iteration 997, loss = 1.14519953\n",
      "Iteration 998, loss = 1.14507857\n",
      "Iteration 999, loss = 1.14495795\n",
      "Iteration 1000, loss = 1.14483767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\projeto1\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 32.76696225\n",
      "Iteration 2, loss = 32.76696225\n",
      "Iteration 3, loss = 32.76696224\n",
      "Iteration 4, loss = 32.76696224\n",
      "Iteration 5, loss = 32.76696223\n",
      "Iteration 6, loss = 32.76696222\n",
      "Iteration 7, loss = 32.76696222\n",
      "Iteration 8, loss = 32.76696221\n",
      "Iteration 9, loss = 32.76696220\n",
      "Iteration 10, loss = 32.76696220\n",
      "Iteration 11, loss = 32.76696219\n",
      "Iteration 12, loss = 32.76696219\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.53451062\n",
      "Iteration 2, loss = 1.53374612\n",
      "Iteration 3, loss = 1.53298221\n",
      "Iteration 4, loss = 1.53221892\n",
      "Iteration 5, loss = 1.53145625\n",
      "Iteration 6, loss = 1.53069422\n",
      "Iteration 7, loss = 1.52993283\n",
      "Iteration 8, loss = 1.52917210\n",
      "Iteration 9, loss = 1.52841203\n",
      "Iteration 10, loss = 1.52765264\n",
      "Iteration 11, loss = 1.52689394\n",
      "Iteration 12, loss = 1.52613593\n",
      "Iteration 13, loss = 1.52537863\n",
      "Iteration 14, loss = 1.52462204\n",
      "Iteration 15, loss = 1.52386618\n",
      "Iteration 16, loss = 1.52311105\n",
      "Iteration 17, loss = 1.52235667\n",
      "Iteration 18, loss = 1.52160303\n",
      "Iteration 19, loss = 1.52085015\n",
      "Iteration 20, loss = 1.52009804\n",
      "Iteration 21, loss = 1.51934671\n",
      "Iteration 22, loss = 1.51859615\n",
      "Iteration 23, loss = 1.51784639\n",
      "Iteration 24, loss = 1.51709742\n",
      "Iteration 25, loss = 1.51634925\n",
      "Iteration 26, loss = 1.51560189\n",
      "Iteration 27, loss = 1.51485534\n",
      "Iteration 28, loss = 1.51410962\n",
      "Iteration 29, loss = 1.51336471\n",
      "Iteration 30, loss = 1.51262064\n",
      "Iteration 31, loss = 1.51187741\n",
      "Iteration 32, loss = 1.51113501\n",
      "Iteration 33, loss = 1.51039346\n",
      "Iteration 34, loss = 1.50965275\n",
      "Iteration 35, loss = 1.50891290\n",
      "Iteration 36, loss = 1.50817390\n",
      "Iteration 37, loss = 1.50743577\n",
      "Iteration 38, loss = 1.50669849\n",
      "Iteration 39, loss = 1.50596209\n",
      "Iteration 40, loss = 1.50522655\n",
      "Iteration 41, loss = 1.50449188\n",
      "Iteration 42, loss = 1.50375809\n",
      "Iteration 43, loss = 1.50302517\n",
      "Iteration 44, loss = 1.50229314\n",
      "Iteration 45, loss = 1.50156198\n",
      "Iteration 46, loss = 1.50083171\n",
      "Iteration 47, loss = 1.50010233\n",
      "Iteration 48, loss = 1.49937383\n",
      "Iteration 49, loss = 1.49864622\n",
      "Iteration 50, loss = 1.49791950\n",
      "Iteration 51, loss = 1.49719368\n",
      "Iteration 52, loss = 1.49646875\n",
      "Iteration 53, loss = 1.49574472\n",
      "Iteration 54, loss = 1.49502158\n",
      "Iteration 55, loss = 1.49429934\n",
      "Iteration 56, loss = 1.49357800\n",
      "Iteration 57, loss = 1.49285756\n",
      "Iteration 58, loss = 1.49213802\n",
      "Iteration 59, loss = 1.49141938\n",
      "Iteration 60, loss = 1.49070165\n",
      "Iteration 61, loss = 1.48998482\n",
      "Iteration 62, loss = 1.48926889\n",
      "Iteration 63, loss = 1.48855388\n",
      "Iteration 64, loss = 1.48783976\n",
      "Iteration 65, loss = 1.48712656\n",
      "Iteration 66, loss = 1.48641426\n",
      "Iteration 67, loss = 1.48570287\n",
      "Iteration 68, loss = 1.48499240\n",
      "Iteration 69, loss = 1.48428283\n",
      "Iteration 70, loss = 1.48357417\n",
      "Iteration 71, loss = 1.48286642\n",
      "Iteration 72, loss = 1.48215959\n",
      "Iteration 73, loss = 1.48145366\n",
      "Iteration 74, loss = 1.48074865\n",
      "Iteration 75, loss = 1.48004456\n",
      "Iteration 76, loss = 1.47934137\n",
      "Iteration 77, loss = 1.47863910\n",
      "Iteration 78, loss = 1.47793774\n",
      "Iteration 79, loss = 1.47723730\n",
      "Iteration 80, loss = 1.47653778\n",
      "Iteration 81, loss = 1.47583917\n",
      "Iteration 82, loss = 1.47514147\n",
      "Iteration 83, loss = 1.47444469\n",
      "Iteration 84, loss = 1.47374883\n",
      "Iteration 85, loss = 1.47305389\n",
      "Iteration 86, loss = 1.47235986\n",
      "Iteration 87, loss = 1.47166675\n",
      "Iteration 88, loss = 1.47097456\n",
      "Iteration 89, loss = 1.47028328\n",
      "Iteration 90, loss = 1.46959293\n",
      "Iteration 91, loss = 1.46890349\n",
      "Iteration 92, loss = 1.46821498\n",
      "Iteration 93, loss = 1.46752738\n",
      "Iteration 94, loss = 1.46684070\n",
      "Iteration 95, loss = 1.46615494\n",
      "Iteration 96, loss = 1.46547010\n",
      "Iteration 97, loss = 1.46478618\n",
      "Iteration 98, loss = 1.46410319\n",
      "Iteration 99, loss = 1.46342111\n",
      "Iteration 100, loss = 1.46273996\n",
      "Iteration 101, loss = 1.46205972\n",
      "Iteration 102, loss = 1.46138041\n",
      "Iteration 103, loss = 1.46070202\n",
      "Iteration 104, loss = 1.46002455\n",
      "Iteration 105, loss = 1.45934801\n",
      "Iteration 106, loss = 1.45867238\n",
      "Iteration 107, loss = 1.45799768\n",
      "Iteration 108, loss = 1.45732391\n",
      "Iteration 109, loss = 1.45665105\n",
      "Iteration 110, loss = 1.45597912\n",
      "Iteration 111, loss = 1.45530811\n",
      "Iteration 112, loss = 1.45463803\n",
      "Iteration 113, loss = 1.45396887\n",
      "Iteration 114, loss = 1.45330063\n",
      "Iteration 115, loss = 1.45263332\n",
      "Iteration 116, loss = 1.45196693\n",
      "Iteration 117, loss = 1.45130147\n",
      "Iteration 118, loss = 1.45063693\n",
      "Iteration 119, loss = 1.44997332\n",
      "Iteration 120, loss = 1.44931063\n",
      "Iteration 121, loss = 1.44864886\n",
      "Iteration 122, loss = 1.44798803\n",
      "Iteration 123, loss = 1.44732811\n",
      "Iteration 124, loss = 1.44666913\n",
      "Iteration 125, loss = 1.44601106\n",
      "Iteration 126, loss = 1.44535393\n",
      "Iteration 127, loss = 1.44469772\n",
      "Iteration 128, loss = 1.44404243\n",
      "Iteration 129, loss = 1.44338808\n",
      "Iteration 130, loss = 1.44273464\n",
      "Iteration 131, loss = 1.44208214\n",
      "Iteration 132, loss = 1.44143056\n",
      "Iteration 133, loss = 1.44077991\n",
      "Iteration 134, loss = 1.44013018\n",
      "Iteration 135, loss = 1.43948139\n",
      "Iteration 136, loss = 1.43883351\n",
      "Iteration 137, loss = 1.43818657\n",
      "Iteration 138, loss = 1.43754055\n",
      "Iteration 139, loss = 1.43689546\n",
      "Iteration 140, loss = 1.43625130\n",
      "Iteration 141, loss = 1.43560806\n",
      "Iteration 142, loss = 1.43496575\n",
      "Iteration 143, loss = 1.43432437\n",
      "Iteration 144, loss = 1.43368392\n",
      "Iteration 145, loss = 1.43304439\n",
      "Iteration 146, loss = 1.43240579\n",
      "Iteration 147, loss = 1.43176812\n",
      "Iteration 148, loss = 1.43113138\n",
      "Iteration 149, loss = 1.43049557\n",
      "Iteration 150, loss = 1.42986068\n",
      "Iteration 151, loss = 1.42922672\n",
      "Iteration 152, loss = 1.42859369\n",
      "Iteration 153, loss = 1.42796158\n",
      "Iteration 154, loss = 1.42733041\n",
      "Iteration 155, loss = 1.42670016\n",
      "Iteration 156, loss = 1.42607084\n",
      "Iteration 157, loss = 1.42544245\n",
      "Iteration 158, loss = 1.42481498\n",
      "Iteration 159, loss = 1.42418845\n",
      "Iteration 160, loss = 1.42356284\n",
      "Iteration 161, loss = 1.42293816\n",
      "Iteration 162, loss = 1.42231441\n",
      "Iteration 163, loss = 1.42169158\n",
      "Iteration 164, loss = 1.42106969\n",
      "Iteration 165, loss = 1.42044872\n",
      "Iteration 166, loss = 1.41982868\n",
      "Iteration 167, loss = 1.41920957\n",
      "Iteration 168, loss = 1.41859138\n",
      "Iteration 169, loss = 1.41797412\n",
      "Iteration 170, loss = 1.41735780\n",
      "Iteration 171, loss = 1.41674239\n",
      "Iteration 172, loss = 1.41612792\n",
      "Iteration 173, loss = 1.41551438\n",
      "Iteration 174, loss = 1.41490176\n",
      "Iteration 175, loss = 1.41429007\n",
      "Iteration 176, loss = 1.41367930\n",
      "Iteration 177, loss = 1.41306947\n",
      "Iteration 178, loss = 1.41246056\n",
      "Iteration 179, loss = 1.41185258\n",
      "Iteration 180, loss = 1.41124553\n",
      "Iteration 181, loss = 1.41063940\n",
      "Iteration 182, loss = 1.41003420\n",
      "Iteration 183, loss = 1.40942993\n",
      "Iteration 184, loss = 1.40882659\n",
      "Iteration 185, loss = 1.40822417\n",
      "Iteration 186, loss = 1.40762268\n",
      "Iteration 187, loss = 1.40702211\n",
      "Iteration 188, loss = 1.40642247\n",
      "Iteration 189, loss = 1.40582376\n",
      "Iteration 190, loss = 1.40522598\n",
      "Iteration 191, loss = 1.40462912\n",
      "Iteration 192, loss = 1.40403319\n",
      "Iteration 193, loss = 1.40343818\n",
      "Iteration 194, loss = 1.40284410\n",
      "Iteration 195, loss = 1.40225094\n",
      "Iteration 196, loss = 1.40165871\n",
      "Iteration 197, loss = 1.40106741\n",
      "Iteration 198, loss = 1.40047703\n",
      "Iteration 199, loss = 1.39988758\n",
      "Iteration 200, loss = 1.39929905\n",
      "Iteration 201, loss = 1.39871145\n",
      "Iteration 202, loss = 1.39812477\n",
      "Iteration 203, loss = 1.39753902\n",
      "Iteration 204, loss = 1.39695419\n",
      "Iteration 205, loss = 1.39637028\n",
      "Iteration 206, loss = 1.39578730\n",
      "Iteration 207, loss = 1.39520524\n",
      "Iteration 208, loss = 1.39462411\n",
      "Iteration 209, loss = 1.39404390\n",
      "Iteration 210, loss = 1.39346461\n",
      "Iteration 211, loss = 1.39288625\n",
      "Iteration 212, loss = 1.39230881\n",
      "Iteration 213, loss = 1.39173230\n",
      "Iteration 214, loss = 1.39115670\n",
      "Iteration 215, loss = 1.39058203\n",
      "Iteration 216, loss = 1.39000828\n",
      "Iteration 217, loss = 1.38943545\n",
      "Iteration 218, loss = 1.38886355\n",
      "Iteration 219, loss = 1.38829256\n",
      "Iteration 220, loss = 1.38772250\n",
      "Iteration 221, loss = 1.38715336\n",
      "Iteration 222, loss = 1.38658514\n",
      "Iteration 223, loss = 1.38601784\n",
      "Iteration 224, loss = 1.38545146\n",
      "Iteration 225, loss = 1.38488600\n",
      "Iteration 226, loss = 1.38432146\n",
      "Iteration 227, loss = 1.38375784\n",
      "Iteration 228, loss = 1.38319514\n",
      "Iteration 229, loss = 1.38263336\n",
      "Iteration 230, loss = 1.38207250\n",
      "Iteration 231, loss = 1.38151256\n",
      "Iteration 232, loss = 1.38095354\n",
      "Iteration 233, loss = 1.38039543\n",
      "Iteration 234, loss = 1.37983824\n",
      "Iteration 235, loss = 1.37928197\n",
      "Iteration 236, loss = 1.37872662\n",
      "Iteration 237, loss = 1.37817219\n",
      "Iteration 238, loss = 1.37761867\n",
      "Iteration 239, loss = 1.37706607\n",
      "Iteration 240, loss = 1.37651438\n",
      "Iteration 241, loss = 1.37596361\n",
      "Iteration 242, loss = 1.37541376\n",
      "Iteration 243, loss = 1.37486482\n",
      "Iteration 244, loss = 1.37431680\n",
      "Iteration 245, loss = 1.37376969\n",
      "Iteration 246, loss = 1.37322349\n",
      "Iteration 247, loss = 1.37267821\n",
      "Iteration 248, loss = 1.37213385\n",
      "Iteration 249, loss = 1.37159039\n",
      "Iteration 250, loss = 1.37104785\n",
      "Iteration 251, loss = 1.37050623\n",
      "Iteration 252, loss = 1.36996551\n",
      "Iteration 253, loss = 1.36942571\n",
      "Iteration 254, loss = 1.36888682\n",
      "Iteration 255, loss = 1.36834884\n",
      "Iteration 256, loss = 1.36781178\n",
      "Iteration 257, loss = 1.36727562\n",
      "Iteration 258, loss = 1.36674037\n",
      "Iteration 259, loss = 1.36620604\n",
      "Iteration 260, loss = 1.36567261\n",
      "Iteration 261, loss = 1.36514009\n",
      "Iteration 262, loss = 1.36460849\n",
      "Iteration 263, loss = 1.36407779\n",
      "Iteration 264, loss = 1.36354800\n",
      "Iteration 265, loss = 1.36301911\n",
      "Iteration 266, loss = 1.36249114\n",
      "Iteration 267, loss = 1.36196407\n",
      "Iteration 268, loss = 1.36143791\n",
      "Iteration 269, loss = 1.36091265\n",
      "Iteration 270, loss = 1.36038830\n",
      "Iteration 271, loss = 1.35986486\n",
      "Iteration 272, loss = 1.35934232\n",
      "Iteration 273, loss = 1.35882068\n",
      "Iteration 274, loss = 1.35829995\n",
      "Iteration 275, loss = 1.35778013\n",
      "Iteration 276, loss = 1.35726120\n",
      "Iteration 277, loss = 1.35674318\n",
      "Iteration 278, loss = 1.35622607\n",
      "Iteration 279, loss = 1.35570985\n",
      "Iteration 280, loss = 1.35519454\n",
      "Iteration 281, loss = 1.35468013\n",
      "Iteration 282, loss = 1.35416662\n",
      "Iteration 283, loss = 1.35365401\n",
      "Iteration 284, loss = 1.35314230\n",
      "Iteration 285, loss = 1.35263149\n",
      "Iteration 286, loss = 1.35212157\n",
      "Iteration 287, loss = 1.35161256\n",
      "Iteration 288, loss = 1.35110445\n",
      "Iteration 289, loss = 1.35059723\n",
      "Iteration 290, loss = 1.35009091\n",
      "Iteration 291, loss = 1.34958549\n",
      "Iteration 292, loss = 1.34908096\n",
      "Iteration 293, loss = 1.34857733\n",
      "Iteration 294, loss = 1.34807459\n",
      "Iteration 295, loss = 1.34757275\n",
      "Iteration 296, loss = 1.34707181\n",
      "Iteration 297, loss = 1.34657175\n",
      "Iteration 298, loss = 1.34607260\n",
      "Iteration 299, loss = 1.34557433\n",
      "Iteration 300, loss = 1.34507696\n",
      "Iteration 301, loss = 1.34458048\n",
      "Iteration 302, loss = 1.34408489\n",
      "Iteration 303, loss = 1.34359019\n",
      "Iteration 304, loss = 1.34309638\n",
      "Iteration 305, loss = 1.34260346\n",
      "Iteration 306, loss = 1.34211143\n",
      "Iteration 307, loss = 1.34162029\n",
      "Iteration 308, loss = 1.34113004\n",
      "Iteration 309, loss = 1.34064068\n",
      "Iteration 310, loss = 1.34015220\n",
      "Iteration 311, loss = 1.33966461\n",
      "Iteration 312, loss = 1.33917791\n",
      "Iteration 313, loss = 1.33869209\n",
      "Iteration 314, loss = 1.33820715\n",
      "Iteration 315, loss = 1.33772311\n",
      "Iteration 316, loss = 1.33723994\n",
      "Iteration 317, loss = 1.33675766\n",
      "Iteration 318, loss = 1.33627626\n",
      "Iteration 319, loss = 1.33579575\n",
      "Iteration 320, loss = 1.33531612\n",
      "Iteration 321, loss = 1.33483736\n",
      "Iteration 322, loss = 1.33435949\n",
      "Iteration 323, loss = 1.33388250\n",
      "Iteration 324, loss = 1.33340639\n",
      "Iteration 325, loss = 1.33293116\n",
      "Iteration 326, loss = 1.33245680\n",
      "Iteration 327, loss = 1.33198333\n",
      "Iteration 328, loss = 1.33151073\n",
      "Iteration 329, loss = 1.33103901\n",
      "Iteration 330, loss = 1.33056816\n",
      "Iteration 331, loss = 1.33009819\n",
      "Iteration 332, loss = 1.32962909\n",
      "Iteration 333, loss = 1.32916087\n",
      "Iteration 334, loss = 1.32869352\n",
      "Iteration 335, loss = 1.32822705\n",
      "Iteration 336, loss = 1.32776145\n",
      "Iteration 337, loss = 1.32729672\n",
      "Iteration 338, loss = 1.32683286\n",
      "Iteration 339, loss = 1.32636987\n",
      "Iteration 340, loss = 1.32590775\n",
      "Iteration 341, loss = 1.32544650\n",
      "Iteration 342, loss = 1.32498612\n",
      "Iteration 343, loss = 1.32452661\n",
      "Iteration 344, loss = 1.32406796\n",
      "Iteration 345, loss = 1.32361018\n",
      "Iteration 346, loss = 1.32315327\n",
      "Iteration 347, loss = 1.32269723\n",
      "Iteration 348, loss = 1.32224204\n",
      "Iteration 349, loss = 1.32178773\n",
      "Iteration 350, loss = 1.32133427\n",
      "Iteration 351, loss = 1.32088168\n",
      "Iteration 352, loss = 1.32042996\n",
      "Iteration 353, loss = 1.31997909\n",
      "Iteration 354, loss = 1.31952908\n",
      "Iteration 355, loss = 1.31907994\n",
      "Iteration 356, loss = 1.31863165\n",
      "Iteration 357, loss = 1.31818423\n",
      "Iteration 358, loss = 1.31773766\n",
      "Iteration 359, loss = 1.31729195\n",
      "Iteration 360, loss = 1.31684710\n",
      "Iteration 361, loss = 1.31640310\n",
      "Iteration 362, loss = 1.31595996\n",
      "Iteration 363, loss = 1.31551767\n",
      "Iteration 364, loss = 1.31507624\n",
      "Iteration 365, loss = 1.31463566\n",
      "Iteration 366, loss = 1.31419593\n",
      "Iteration 367, loss = 1.31375706\n",
      "Iteration 368, loss = 1.31331904\n",
      "Iteration 369, loss = 1.31288187\n",
      "Iteration 370, loss = 1.31244554\n",
      "Iteration 371, loss = 1.31201007\n",
      "Iteration 372, loss = 1.31157545\n",
      "Iteration 373, loss = 1.31114167\n",
      "Iteration 374, loss = 1.31070875\n",
      "Iteration 375, loss = 1.31027666\n",
      "Iteration 376, loss = 1.30984543\n",
      "Iteration 377, loss = 1.30941504\n",
      "Iteration 378, loss = 1.30898549\n",
      "Iteration 379, loss = 1.30855679\n",
      "Iteration 380, loss = 1.30812893\n",
      "Iteration 381, loss = 1.30770191\n",
      "Iteration 382, loss = 1.30727573\n",
      "Iteration 383, loss = 1.30685040\n",
      "Iteration 384, loss = 1.30642590\n",
      "Iteration 385, loss = 1.30600224\n",
      "Iteration 386, loss = 1.30557942\n",
      "Iteration 387, loss = 1.30515744\n",
      "Iteration 388, loss = 1.30473630\n",
      "Iteration 389, loss = 1.30431599\n",
      "Iteration 390, loss = 1.30389652\n",
      "Iteration 391, loss = 1.30347788\n",
      "Iteration 392, loss = 1.30306008\n",
      "Iteration 393, loss = 1.30264311\n",
      "Iteration 394, loss = 1.30222697\n",
      "Iteration 395, loss = 1.30181166\n",
      "Iteration 396, loss = 1.30139719\n",
      "Iteration 397, loss = 1.30098354\n",
      "Iteration 398, loss = 1.30057073\n",
      "Iteration 399, loss = 1.30015874\n",
      "Iteration 400, loss = 1.29974758\n",
      "Iteration 401, loss = 1.29933724\n",
      "Iteration 402, loss = 1.29892774\n",
      "Iteration 403, loss = 1.29851906\n",
      "Iteration 404, loss = 1.29811120\n",
      "Iteration 405, loss = 1.29770416\n",
      "Iteration 406, loss = 1.29729795\n",
      "Iteration 407, loss = 1.29689257\n",
      "Iteration 408, loss = 1.29648800\n",
      "Iteration 409, loss = 1.29608425\n",
      "Iteration 410, loss = 1.29568133\n",
      "Iteration 411, loss = 1.29527922\n",
      "Iteration 412, loss = 1.29487793\n",
      "Iteration 413, loss = 1.29447746\n",
      "Iteration 414, loss = 1.29407780\n",
      "Iteration 415, loss = 1.29367896\n",
      "Iteration 416, loss = 1.29328094\n",
      "Iteration 417, loss = 1.29288373\n",
      "Iteration 418, loss = 1.29248733\n",
      "Iteration 419, loss = 1.29209175\n",
      "Iteration 420, loss = 1.29169697\n",
      "Iteration 421, loss = 1.29130301\n",
      "Iteration 422, loss = 1.29090986\n",
      "Iteration 423, loss = 1.29051752\n",
      "Iteration 424, loss = 1.29012598\n",
      "Iteration 425, loss = 1.28973525\n",
      "Iteration 426, loss = 1.28934533\n",
      "Iteration 427, loss = 1.28895622\n",
      "Iteration 428, loss = 1.28856791\n",
      "Iteration 429, loss = 1.28818040\n",
      "Iteration 430, loss = 1.28779370\n",
      "Iteration 431, loss = 1.28740780\n",
      "Iteration 432, loss = 1.28702270\n",
      "Iteration 433, loss = 1.28663840\n",
      "Iteration 434, loss = 1.28625491\n",
      "Iteration 435, loss = 1.28587221\n",
      "Iteration 436, loss = 1.28549031\n",
      "Iteration 437, loss = 1.28510920\n",
      "Iteration 438, loss = 1.28472890\n",
      "Iteration 439, loss = 1.28434938\n",
      "Iteration 440, loss = 1.28397067\n",
      "Iteration 441, loss = 1.28359274\n",
      "Iteration 442, loss = 1.28321561\n",
      "Iteration 443, loss = 1.28283928\n",
      "Iteration 444, loss = 1.28246373\n",
      "Iteration 445, loss = 1.28208897\n",
      "Iteration 446, loss = 1.28171501\n",
      "Iteration 447, loss = 1.28134183\n",
      "Iteration 448, loss = 1.28096944\n",
      "Iteration 449, loss = 1.28059783\n",
      "Iteration 450, loss = 1.28022702\n",
      "Iteration 451, loss = 1.27985698\n",
      "Iteration 452, loss = 1.27948773\n",
      "Iteration 453, loss = 1.27911927\n",
      "Iteration 454, loss = 1.27875159\n",
      "Iteration 455, loss = 1.27838468\n",
      "Iteration 456, loss = 1.27801856\n",
      "Iteration 457, loss = 1.27765322\n",
      "Iteration 458, loss = 1.27728866\n",
      "Iteration 459, loss = 1.27692487\n",
      "Iteration 460, loss = 1.27656187\n",
      "Iteration 461, loss = 1.27619964\n",
      "Iteration 462, loss = 1.27583818\n",
      "Iteration 463, loss = 1.27547750\n",
      "Iteration 464, loss = 1.27511759\n",
      "Iteration 465, loss = 1.27475845\n",
      "Iteration 466, loss = 1.27440009\n",
      "Iteration 467, loss = 1.27404249\n",
      "Iteration 468, loss = 1.27368567\n",
      "Iteration 469, loss = 1.27332961\n",
      "Iteration 470, loss = 1.27297432\n",
      "Iteration 471, loss = 1.27261980\n",
      "Iteration 472, loss = 1.27226605\n",
      "Iteration 473, loss = 1.27191306\n",
      "Iteration 474, loss = 1.27156083\n",
      "Iteration 475, loss = 1.27120937\n",
      "Iteration 476, loss = 1.27085867\n",
      "Iteration 477, loss = 1.27050873\n",
      "Iteration 478, loss = 1.27015955\n",
      "Iteration 479, loss = 1.26981114\n",
      "Iteration 480, loss = 1.26946348\n",
      "Iteration 481, loss = 1.26911657\n",
      "Iteration 482, loss = 1.26877043\n",
      "Iteration 483, loss = 1.26842504\n",
      "Iteration 484, loss = 1.26808040\n",
      "Iteration 485, loss = 1.26773652\n",
      "Iteration 486, loss = 1.26739339\n",
      "Iteration 487, loss = 1.26705102\n",
      "Iteration 488, loss = 1.26670939\n",
      "Iteration 489, loss = 1.26636852\n",
      "Iteration 490, loss = 1.26602839\n",
      "Iteration 491, loss = 1.26568901\n",
      "Iteration 492, loss = 1.26535038\n",
      "Iteration 493, loss = 1.26501250\n",
      "Iteration 494, loss = 1.26467536\n",
      "Iteration 495, loss = 1.26433896\n",
      "Iteration 496, loss = 1.26400331\n",
      "Iteration 497, loss = 1.26366840\n",
      "Iteration 498, loss = 1.26333424\n",
      "Iteration 499, loss = 1.26300081\n",
      "Iteration 500, loss = 1.26266812\n",
      "Iteration 501, loss = 1.26233617\n",
      "Iteration 502, loss = 1.26200496\n",
      "Iteration 503, loss = 1.26167449\n",
      "Iteration 504, loss = 1.26134475\n",
      "Iteration 505, loss = 1.26101575\n",
      "Iteration 506, loss = 1.26068748\n",
      "Iteration 507, loss = 1.26035994\n",
      "Iteration 508, loss = 1.26003313\n",
      "Iteration 509, loss = 1.25970706\n",
      "Iteration 510, loss = 1.25938171\n",
      "Iteration 511, loss = 1.25905710\n",
      "Iteration 512, loss = 1.25873321\n",
      "Iteration 513, loss = 1.25841005\n",
      "Iteration 514, loss = 1.25808762\n",
      "Iteration 515, loss = 1.25776591\n",
      "Iteration 516, loss = 1.25744492\n",
      "Iteration 517, loss = 1.25712466\n",
      "Iteration 518, loss = 1.25680512\n",
      "Iteration 519, loss = 1.25648630\n",
      "Iteration 520, loss = 1.25616820\n",
      "Iteration 521, loss = 1.25585082\n",
      "Iteration 522, loss = 1.25553415\n",
      "Iteration 523, loss = 1.25521821\n",
      "Iteration 524, loss = 1.25490298\n",
      "Iteration 525, loss = 1.25458847\n",
      "Iteration 526, loss = 1.25427467\n",
      "Iteration 527, loss = 1.25396158\n",
      "Iteration 528, loss = 1.25364920\n",
      "Iteration 529, loss = 1.25333754\n",
      "Iteration 530, loss = 1.25302659\n",
      "Iteration 531, loss = 1.25271634\n",
      "Iteration 532, loss = 1.25240681\n",
      "Iteration 533, loss = 1.25209798\n",
      "Iteration 534, loss = 1.25178985\n",
      "Iteration 535, loss = 1.25148244\n",
      "Iteration 536, loss = 1.25117572\n",
      "Iteration 537, loss = 1.25086971\n",
      "Iteration 538, loss = 1.25056440\n",
      "Iteration 539, loss = 1.25025980\n",
      "Iteration 540, loss = 1.24995589\n",
      "Iteration 541, loss = 1.24965268\n",
      "Iteration 542, loss = 1.24935017\n",
      "Iteration 543, loss = 1.24904836\n",
      "Iteration 544, loss = 1.24874724\n",
      "Iteration 545, loss = 1.24844682\n",
      "Iteration 546, loss = 1.24814709\n",
      "Iteration 547, loss = 1.24784806\n",
      "Iteration 548, loss = 1.24754972\n",
      "Iteration 549, loss = 1.24725206\n",
      "Iteration 550, loss = 1.24695510\n",
      "Iteration 551, loss = 1.24665883\n",
      "Iteration 552, loss = 1.24636325\n",
      "Iteration 553, loss = 1.24606835\n",
      "Iteration 554, loss = 1.24577414\n",
      "Iteration 555, loss = 1.24548061\n",
      "Iteration 556, loss = 1.24518777\n",
      "Iteration 557, loss = 1.24489561\n",
      "Iteration 558, loss = 1.24460413\n",
      "Iteration 559, loss = 1.24431333\n",
      "Iteration 560, loss = 1.24402321\n",
      "Iteration 561, loss = 1.24373378\n",
      "Iteration 562, loss = 1.24344501\n",
      "Iteration 563, loss = 1.24315693\n",
      "Iteration 564, loss = 1.24286952\n",
      "Iteration 565, loss = 1.24258279\n",
      "Iteration 566, loss = 1.24229673\n",
      "Iteration 567, loss = 1.24201134\n",
      "Iteration 568, loss = 1.24172663\n",
      "Iteration 569, loss = 1.24144258\n",
      "Iteration 570, loss = 1.24115921\n",
      "Iteration 571, loss = 1.24087650\n",
      "Iteration 572, loss = 1.24059446\n",
      "Iteration 573, loss = 1.24031309\n",
      "Iteration 574, loss = 1.24003238\n",
      "Iteration 575, loss = 1.23975234\n",
      "Iteration 576, loss = 1.23947296\n",
      "Iteration 577, loss = 1.23919424\n",
      "Iteration 578, loss = 1.23891618\n",
      "Iteration 579, loss = 1.23863879\n",
      "Iteration 580, loss = 1.23836205\n",
      "Iteration 581, loss = 1.23808597\n",
      "Iteration 582, loss = 1.23781055\n",
      "Iteration 583, loss = 1.23753579\n",
      "Iteration 584, loss = 1.23726168\n",
      "Iteration 585, loss = 1.23698822\n",
      "Iteration 586, loss = 1.23671542\n",
      "Iteration 587, loss = 1.23644327\n",
      "Iteration 588, loss = 1.23617177\n",
      "Iteration 589, loss = 1.23590092\n",
      "Iteration 590, loss = 1.23563072\n",
      "Iteration 591, loss = 1.23536117\n",
      "Iteration 592, loss = 1.23509226\n",
      "Iteration 593, loss = 1.23482400\n",
      "Iteration 594, loss = 1.23455639\n",
      "Iteration 595, loss = 1.23428942\n",
      "Iteration 596, loss = 1.23402309\n",
      "Iteration 597, loss = 1.23375740\n",
      "Iteration 598, loss = 1.23349236\n",
      "Iteration 599, loss = 1.23322795\n",
      "Iteration 600, loss = 1.23296418\n",
      "Iteration 601, loss = 1.23270105\n",
      "Iteration 602, loss = 1.23243856\n",
      "Iteration 603, loss = 1.23217670\n",
      "Iteration 604, loss = 1.23191547\n",
      "Iteration 605, loss = 1.23165488\n",
      "Iteration 606, loss = 1.23139492\n",
      "Iteration 607, loss = 1.23113559\n",
      "Iteration 608, loss = 1.23087690\n",
      "Iteration 609, loss = 1.23061883\n",
      "Iteration 610, loss = 1.23036139\n",
      "Iteration 611, loss = 1.23010457\n",
      "Iteration 612, loss = 1.22984839\n",
      "Iteration 613, loss = 1.22959282\n",
      "Iteration 614, loss = 1.22933789\n",
      "Iteration 615, loss = 1.22908357\n",
      "Iteration 616, loss = 1.22882988\n",
      "Iteration 617, loss = 1.22857680\n",
      "Iteration 618, loss = 1.22832435\n",
      "Iteration 619, loss = 1.22807252\n",
      "Iteration 620, loss = 1.22782130\n",
      "Iteration 621, loss = 1.22757070\n",
      "Iteration 622, loss = 1.22732072\n",
      "Iteration 623, loss = 1.22707135\n",
      "Iteration 624, loss = 1.22682259\n",
      "Iteration 625, loss = 1.22657445\n",
      "Iteration 626, loss = 1.22632691\n",
      "Iteration 627, loss = 1.22607999\n",
      "Iteration 628, loss = 1.22583368\n",
      "Iteration 629, loss = 1.22558797\n",
      "Iteration 630, loss = 1.22534288\n",
      "Iteration 631, loss = 1.22509838\n",
      "Iteration 632, loss = 1.22485450\n",
      "Iteration 633, loss = 1.22461122\n",
      "Iteration 634, loss = 1.22436854\n",
      "Iteration 635, loss = 1.22412646\n",
      "Iteration 636, loss = 1.22388498\n",
      "Iteration 637, loss = 1.22364411\n",
      "Iteration 638, loss = 1.22340383\n",
      "Iteration 639, loss = 1.22316415\n",
      "Iteration 640, loss = 1.22292507\n",
      "Iteration 641, loss = 1.22268658\n",
      "Iteration 642, loss = 1.22244869\n",
      "Iteration 643, loss = 1.22221139\n",
      "Iteration 644, loss = 1.22197469\n",
      "Iteration 645, loss = 1.22173857\n",
      "Iteration 646, loss = 1.22150305\n",
      "Iteration 647, loss = 1.22126811\n",
      "Iteration 648, loss = 1.22103377\n",
      "Iteration 649, loss = 1.22080001\n",
      "Iteration 650, loss = 1.22056684\n",
      "Iteration 651, loss = 1.22033425\n",
      "Iteration 652, loss = 1.22010225\n",
      "Iteration 653, loss = 1.21987083\n",
      "Iteration 654, loss = 1.21963999\n",
      "Iteration 655, loss = 1.21940973\n",
      "Iteration 656, loss = 1.21918006\n",
      "Iteration 657, loss = 1.21895096\n",
      "Iteration 658, loss = 1.21872244\n",
      "Iteration 659, loss = 1.21849450\n",
      "Iteration 660, loss = 1.21826713\n",
      "Iteration 661, loss = 1.21804034\n",
      "Iteration 662, loss = 1.21781412\n",
      "Iteration 663, loss = 1.21758848\n",
      "Iteration 664, loss = 1.21736340\n",
      "Iteration 665, loss = 1.21713890\n",
      "Iteration 666, loss = 1.21691497\n",
      "Iteration 667, loss = 1.21669160\n",
      "Iteration 668, loss = 1.21646881\n",
      "Iteration 669, loss = 1.21624658\n",
      "Iteration 670, loss = 1.21602491\n",
      "Iteration 671, loss = 1.21580381\n",
      "Iteration 672, loss = 1.21558328\n",
      "Iteration 673, loss = 1.21536330\n",
      "Iteration 674, loss = 1.21514389\n",
      "Iteration 675, loss = 1.21492503\n",
      "Iteration 676, loss = 1.21470674\n",
      "Iteration 677, loss = 1.21448901\n",
      "Iteration 678, loss = 1.21427183\n",
      "Iteration 679, loss = 1.21405521\n",
      "Iteration 680, loss = 1.21383914\n",
      "Iteration 681, loss = 1.21362363\n",
      "Iteration 682, loss = 1.21340867\n",
      "Iteration 683, loss = 1.21319426\n",
      "Iteration 684, loss = 1.21298040\n",
      "Iteration 685, loss = 1.21276710\n",
      "Iteration 686, loss = 1.21255434\n",
      "Iteration 687, loss = 1.21234213\n",
      "Iteration 688, loss = 1.21213046\n",
      "Iteration 689, loss = 1.21191935\n",
      "Iteration 690, loss = 1.21170877\n",
      "Iteration 691, loss = 1.21149875\n",
      "Iteration 692, loss = 1.21128926\n",
      "Iteration 693, loss = 1.21108032\n",
      "Iteration 694, loss = 1.21087191\n",
      "Iteration 695, loss = 1.21066405\n",
      "Iteration 696, loss = 1.21045672\n",
      "Iteration 697, loss = 1.21024993\n",
      "Iteration 698, loss = 1.21004368\n",
      "Iteration 699, loss = 1.20983797\n",
      "Iteration 700, loss = 1.20963279\n",
      "Iteration 701, loss = 1.20942814\n",
      "Iteration 702, loss = 1.20922402\n",
      "Iteration 703, loss = 1.20902044\n",
      "Iteration 704, loss = 1.20881738\n",
      "Iteration 705, loss = 1.20861486\n",
      "Iteration 706, loss = 1.20841286\n",
      "Iteration 707, loss = 1.20821139\n",
      "Iteration 708, loss = 1.20801045\n",
      "Iteration 709, loss = 1.20781003\n",
      "Iteration 710, loss = 1.20761014\n",
      "Iteration 711, loss = 1.20741077\n",
      "Iteration 712, loss = 1.20721192\n",
      "Iteration 713, loss = 1.20701360\n",
      "Iteration 714, loss = 1.20681579\n",
      "Iteration 715, loss = 1.20661850\n",
      "Iteration 716, loss = 1.20642174\n",
      "Iteration 717, loss = 1.20622548\n",
      "Iteration 718, loss = 1.20602975\n",
      "Iteration 719, loss = 1.20583453\n",
      "Iteration 720, loss = 1.20563982\n",
      "Iteration 721, loss = 1.20544563\n",
      "Iteration 722, loss = 1.20525194\n",
      "Iteration 723, loss = 1.20505877\n",
      "Iteration 724, loss = 1.20486611\n",
      "Iteration 725, loss = 1.20467396\n",
      "Iteration 726, loss = 1.20448232\n",
      "Iteration 727, loss = 1.20429118\n",
      "Iteration 728, loss = 1.20410055\n",
      "Iteration 729, loss = 1.20391042\n",
      "Iteration 730, loss = 1.20372080\n",
      "Iteration 731, loss = 1.20353168\n",
      "Iteration 732, loss = 1.20334307\n",
      "Iteration 733, loss = 1.20315495\n",
      "Iteration 734, loss = 1.20296733\n",
      "Iteration 735, loss = 1.20278022\n",
      "Iteration 736, loss = 1.20259360\n",
      "Iteration 737, loss = 1.20240747\n",
      "Iteration 738, loss = 1.20222185\n",
      "Iteration 739, loss = 1.20203671\n",
      "Iteration 740, loss = 1.20185208\n",
      "Iteration 741, loss = 1.20166793\n",
      "Iteration 742, loss = 1.20148428\n",
      "Iteration 743, loss = 1.20130111\n",
      "Iteration 744, loss = 1.20111844\n",
      "Iteration 745, loss = 1.20093626\n",
      "Iteration 746, loss = 1.20075456\n",
      "Iteration 747, loss = 1.20057335\n",
      "Iteration 748, loss = 1.20039263\n",
      "Iteration 749, loss = 1.20021239\n",
      "Iteration 750, loss = 1.20003264\n",
      "Iteration 751, loss = 1.19985337\n",
      "Iteration 752, loss = 1.19967458\n",
      "Iteration 753, loss = 1.19949627\n",
      "Iteration 754, loss = 1.19931844\n",
      "Iteration 755, loss = 1.19914109\n",
      "Iteration 756, loss = 1.19896422\n",
      "Iteration 757, loss = 1.19878782\n",
      "Iteration 758, loss = 1.19861190\n",
      "Iteration 759, loss = 1.19843646\n",
      "Iteration 760, loss = 1.19826149\n",
      "Iteration 761, loss = 1.19808699\n",
      "Iteration 762, loss = 1.19791296\n",
      "Iteration 763, loss = 1.19773941\n",
      "Iteration 764, loss = 1.19756633\n",
      "Iteration 765, loss = 1.19739371\n",
      "Iteration 766, loss = 1.19722156\n",
      "Iteration 767, loss = 1.19704988\n",
      "Iteration 768, loss = 1.19687867\n",
      "Iteration 769, loss = 1.19670792\n",
      "Iteration 770, loss = 1.19653764\n",
      "Iteration 771, loss = 1.19636782\n",
      "Iteration 772, loss = 1.19619846\n",
      "Iteration 773, loss = 1.19602956\n",
      "Iteration 774, loss = 1.19586112\n",
      "Iteration 775, loss = 1.19569314\n",
      "Iteration 776, loss = 1.19552562\n",
      "Iteration 777, loss = 1.19535856\n",
      "Iteration 778, loss = 1.19519196\n",
      "Iteration 779, loss = 1.19502581\n",
      "Iteration 780, loss = 1.19486011\n",
      "Iteration 781, loss = 1.19469487\n",
      "Iteration 782, loss = 1.19453008\n",
      "Iteration 783, loss = 1.19436574\n",
      "Iteration 784, loss = 1.19420185\n",
      "Iteration 785, loss = 1.19403841\n",
      "Iteration 786, loss = 1.19387542\n",
      "Iteration 787, loss = 1.19371288\n",
      "Iteration 788, loss = 1.19355078\n",
      "Iteration 789, loss = 1.19338913\n",
      "Iteration 790, loss = 1.19322793\n",
      "Iteration 791, loss = 1.19306717\n",
      "Iteration 792, loss = 1.19290685\n",
      "Iteration 793, loss = 1.19274697\n",
      "Iteration 794, loss = 1.19258754\n",
      "Iteration 795, loss = 1.19242854\n",
      "Iteration 796, loss = 1.19226998\n",
      "Iteration 797, loss = 1.19211187\n",
      "Iteration 798, loss = 1.19195419\n",
      "Iteration 799, loss = 1.19179694\n",
      "Iteration 800, loss = 1.19164013\n",
      "Iteration 801, loss = 1.19148376\n",
      "Iteration 802, loss = 1.19132781\n",
      "Iteration 803, loss = 1.19117230\n",
      "Iteration 804, loss = 1.19101723\n",
      "Iteration 805, loss = 1.19086258\n",
      "Iteration 806, loss = 1.19070836\n",
      "Iteration 807, loss = 1.19055457\n",
      "Iteration 808, loss = 1.19040121\n",
      "Iteration 809, loss = 1.19024828\n",
      "Iteration 810, loss = 1.19009577\n",
      "Iteration 811, loss = 1.18994368\n",
      "Iteration 812, loss = 1.18979202\n",
      "Iteration 813, loss = 1.18964079\n",
      "Iteration 814, loss = 1.18948997\n",
      "Iteration 815, loss = 1.18933958\n",
      "Iteration 816, loss = 1.18918961\n",
      "Iteration 817, loss = 1.18904005\n",
      "Iteration 818, loss = 1.18889092\n",
      "Iteration 819, loss = 1.18874220\n",
      "Iteration 820, loss = 1.18859390\n",
      "Iteration 821, loss = 1.18844601\n",
      "Iteration 822, loss = 1.18829854\n",
      "Iteration 823, loss = 1.18815148\n",
      "Iteration 824, loss = 1.18800484\n",
      "Iteration 825, loss = 1.18785860\n",
      "Iteration 826, loss = 1.18771278\n",
      "Iteration 827, loss = 1.18756737\n",
      "Iteration 828, loss = 1.18742237\n",
      "Iteration 829, loss = 1.18727777\n",
      "Iteration 830, loss = 1.18713358\n",
      "Iteration 831, loss = 1.18698980\n",
      "Iteration 832, loss = 1.18684643\n",
      "Iteration 833, loss = 1.18670346\n",
      "Iteration 834, loss = 1.18656089\n",
      "Iteration 835, loss = 1.18641872\n",
      "Iteration 836, loss = 1.18627696\n",
      "Iteration 837, loss = 1.18613560\n",
      "Iteration 838, loss = 1.18599464\n",
      "Iteration 839, loss = 1.18585408\n",
      "Iteration 840, loss = 1.18571391\n",
      "Iteration 841, loss = 1.18557414\n",
      "Iteration 842, loss = 1.18543477\n",
      "Iteration 843, loss = 1.18529580\n",
      "Iteration 844, loss = 1.18515722\n",
      "Iteration 845, loss = 1.18501903\n",
      "Iteration 846, loss = 1.18488124\n",
      "Iteration 847, loss = 1.18474383\n",
      "Iteration 848, loss = 1.18460682\n",
      "Iteration 849, loss = 1.18447020\n",
      "Iteration 850, loss = 1.18433397\n",
      "Iteration 851, loss = 1.18419812\n",
      "Iteration 852, loss = 1.18406267\n",
      "Iteration 853, loss = 1.18392760\n",
      "Iteration 854, loss = 1.18379291\n",
      "Iteration 855, loss = 1.18365861\n",
      "Iteration 856, loss = 1.18352470\n",
      "Iteration 857, loss = 1.18339117\n",
      "Iteration 858, loss = 1.18325801\n",
      "Iteration 859, loss = 1.18312525\n",
      "Iteration 860, loss = 1.18299286\n",
      "Iteration 861, loss = 1.18286085\n",
      "Iteration 862, loss = 1.18272922\n",
      "Iteration 863, loss = 1.18259796\n",
      "Iteration 864, loss = 1.18246709\n",
      "Iteration 865, loss = 1.18233659\n",
      "Iteration 866, loss = 1.18220646\n",
      "Iteration 867, loss = 1.18207671\n",
      "Iteration 868, loss = 1.18194733\n",
      "Iteration 869, loss = 1.18181832\n",
      "Iteration 870, loss = 1.18168969\n",
      "Iteration 871, loss = 1.18156143\n",
      "Iteration 872, loss = 1.18143353\n",
      "Iteration 873, loss = 1.18130601\n",
      "Iteration 874, loss = 1.18117885\n",
      "Iteration 875, loss = 1.18105206\n",
      "Iteration 876, loss = 1.18092564\n",
      "Iteration 877, loss = 1.18079958\n",
      "Iteration 878, loss = 1.18067389\n",
      "Iteration 879, loss = 1.18054856\n",
      "Iteration 880, loss = 1.18042360\n",
      "Iteration 881, loss = 1.18029899\n",
      "Iteration 882, loss = 1.18017475\n",
      "Iteration 883, loss = 1.18005087\n",
      "Iteration 884, loss = 1.17992735\n",
      "Iteration 885, loss = 1.17980418\n",
      "Iteration 886, loss = 1.17968138\n",
      "Iteration 887, loss = 1.17955893\n",
      "Iteration 888, loss = 1.17943684\n",
      "Iteration 889, loss = 1.17931510\n",
      "Iteration 890, loss = 1.17919372\n",
      "Iteration 891, loss = 1.17907269\n",
      "Iteration 892, loss = 1.17895201\n",
      "Iteration 893, loss = 1.17883169\n",
      "Iteration 894, loss = 1.17871171\n",
      "Iteration 895, loss = 1.17859209\n",
      "Iteration 896, loss = 1.17847281\n",
      "Iteration 897, loss = 1.17835389\n",
      "Iteration 898, loss = 1.17823531\n",
      "Iteration 899, loss = 1.17811708\n",
      "Iteration 900, loss = 1.17799920\n",
      "Iteration 901, loss = 1.17788166\n",
      "Iteration 902, loss = 1.17776446\n",
      "Iteration 903, loss = 1.17764761\n",
      "Iteration 904, loss = 1.17753110\n",
      "Iteration 905, loss = 1.17741494\n",
      "Iteration 906, loss = 1.17729911\n",
      "Iteration 907, loss = 1.17718362\n",
      "Iteration 908, loss = 1.17706848\n",
      "Iteration 909, loss = 1.17695367\n",
      "Iteration 910, loss = 1.17683920\n",
      "Iteration 911, loss = 1.17672507\n",
      "Iteration 912, loss = 1.17661127\n",
      "Iteration 913, loss = 1.17649781\n",
      "Iteration 914, loss = 1.17638469\n",
      "Iteration 915, loss = 1.17627189\n",
      "Iteration 916, loss = 1.17615944\n",
      "Iteration 917, loss = 1.17604731\n",
      "Iteration 918, loss = 1.17593551\n",
      "Iteration 919, loss = 1.17582405\n",
      "Iteration 920, loss = 1.17571291\n",
      "Iteration 921, loss = 1.17560210\n",
      "Iteration 922, loss = 1.17549163\n",
      "Iteration 923, loss = 1.17538147\n",
      "Iteration 924, loss = 1.17527165\n",
      "Iteration 925, loss = 1.17516215\n",
      "Iteration 926, loss = 1.17505298\n",
      "Iteration 927, loss = 1.17494412\n",
      "Iteration 928, loss = 1.17483560\n",
      "Iteration 929, loss = 1.17472739\n",
      "Iteration 930, loss = 1.17461951\n",
      "Iteration 931, loss = 1.17451195\n",
      "Iteration 932, loss = 1.17440471\n",
      "Iteration 933, loss = 1.17429778\n",
      "Iteration 934, loss = 1.17419118\n",
      "Iteration 935, loss = 1.17408490\n",
      "Iteration 936, loss = 1.17397893\n",
      "Iteration 937, loss = 1.17387327\n",
      "Iteration 938, loss = 1.17376794\n",
      "Iteration 939, loss = 1.17366291\n",
      "Iteration 940, loss = 1.17355820\n",
      "Iteration 941, loss = 1.17345381\n",
      "Iteration 942, loss = 1.17334973\n",
      "Iteration 943, loss = 1.17324595\n",
      "Iteration 944, loss = 1.17314249\n",
      "Iteration 945, loss = 1.17303934\n",
      "Iteration 946, loss = 1.17293650\n",
      "Iteration 947, loss = 1.17283397\n",
      "Iteration 948, loss = 1.17273174\n",
      "Iteration 949, loss = 1.17262982\n",
      "Iteration 950, loss = 1.17252821\n",
      "Iteration 951, loss = 1.17242690\n",
      "Iteration 952, loss = 1.17232590\n",
      "Iteration 953, loss = 1.17222520\n",
      "Iteration 954, loss = 1.17212480\n",
      "Iteration 955, loss = 1.17202471\n",
      "Iteration 956, loss = 1.17192492\n",
      "Iteration 957, loss = 1.17182543\n",
      "Iteration 958, loss = 1.17172624\n",
      "Iteration 959, loss = 1.17162734\n",
      "Iteration 960, loss = 1.17152875\n",
      "Iteration 961, loss = 1.17143045\n",
      "Iteration 962, loss = 1.17133246\n",
      "Iteration 963, loss = 1.17123475\n",
      "Iteration 964, loss = 1.17113735\n",
      "Iteration 965, loss = 1.17104024\n",
      "Iteration 966, loss = 1.17094342\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 31.12861462\n",
      "Iteration 2, loss = 31.12861461\n",
      "Iteration 3, loss = 31.12861460\n",
      "Iteration 4, loss = 31.12861459\n",
      "Iteration 5, loss = 31.12861457\n",
      "Iteration 6, loss = 31.12861456\n",
      "Iteration 7, loss = 31.12861455\n",
      "Iteration 8, loss = 31.12861453\n",
      "Iteration 9, loss = 31.12861452\n",
      "Iteration 10, loss = 31.12861451\n",
      "Iteration 11, loss = 31.12861450\n",
      "Iteration 12, loss = 31.12861448\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.48955834\n",
      "Iteration 2, loss = 1.48844268\n",
      "Iteration 3, loss = 1.48732757\n",
      "Iteration 4, loss = 1.48621305\n",
      "Iteration 5, loss = 1.48509913\n",
      "Iteration 6, loss = 1.48398583\n",
      "Iteration 7, loss = 1.48287316\n",
      "Iteration 8, loss = 1.48176114\n",
      "Iteration 9, loss = 1.48064979\n",
      "Iteration 10, loss = 1.47953911\n",
      "Iteration 11, loss = 1.47842913\n",
      "Iteration 12, loss = 1.47731985\n",
      "Iteration 13, loss = 1.47621128\n",
      "Iteration 14, loss = 1.47510344\n",
      "Iteration 15, loss = 1.47399633\n",
      "Iteration 16, loss = 1.47288997\n",
      "Iteration 17, loss = 1.47178436\n",
      "Iteration 18, loss = 1.47067952\n",
      "Iteration 19, loss = 1.46957545\n",
      "Iteration 20, loss = 1.46847216\n",
      "Iteration 21, loss = 1.46736967\n",
      "Iteration 22, loss = 1.46626797\n",
      "Iteration 23, loss = 1.46516709\n",
      "Iteration 24, loss = 1.46406702\n",
      "Iteration 25, loss = 1.46296779\n",
      "Iteration 26, loss = 1.46186938\n",
      "Iteration 27, loss = 1.46077183\n",
      "Iteration 28, loss = 1.45967512\n",
      "Iteration 29, loss = 1.45857928\n",
      "Iteration 30, loss = 1.45748432\n",
      "Iteration 31, loss = 1.45639023\n",
      "Iteration 32, loss = 1.45529703\n",
      "Iteration 33, loss = 1.45420473\n",
      "Iteration 34, loss = 1.45311335\n",
      "Iteration 35, loss = 1.45202288\n",
      "Iteration 36, loss = 1.45093334\n",
      "Iteration 37, loss = 1.44984474\n",
      "Iteration 38, loss = 1.44875709\n",
      "Iteration 39, loss = 1.44767039\n",
      "Iteration 40, loss = 1.44658467\n",
      "Iteration 41, loss = 1.44549993\n",
      "Iteration 42, loss = 1.44441619\n",
      "Iteration 43, loss = 1.44333344\n",
      "Iteration 44, loss = 1.44225171\n",
      "Iteration 45, loss = 1.44117101\n",
      "Iteration 46, loss = 1.44009135\n",
      "Iteration 47, loss = 1.43901273\n",
      "Iteration 48, loss = 1.43793518\n",
      "Iteration 49, loss = 1.43685871\n",
      "Iteration 50, loss = 1.43578332\n",
      "Iteration 51, loss = 1.43470903\n",
      "Iteration 52, loss = 1.43363586\n",
      "Iteration 53, loss = 1.43256381\n",
      "Iteration 54, loss = 1.43149290\n",
      "Iteration 55, loss = 1.43042315\n",
      "Iteration 56, loss = 1.42935457\n",
      "Iteration 57, loss = 1.42828716\n",
      "Iteration 58, loss = 1.42722096\n",
      "Iteration 59, loss = 1.42615596\n",
      "Iteration 60, loss = 1.42509219\n",
      "Iteration 61, loss = 1.42402965\n",
      "Iteration 62, loss = 1.42296837\n",
      "Iteration 63, loss = 1.42190836\n",
      "Iteration 64, loss = 1.42084964\n",
      "Iteration 65, loss = 1.41979221\n",
      "Iteration 66, loss = 1.41873610\n",
      "Iteration 67, loss = 1.41768131\n",
      "Iteration 68, loss = 1.41662788\n",
      "Iteration 69, loss = 1.41557580\n",
      "Iteration 70, loss = 1.41452511\n",
      "Iteration 71, loss = 1.41347580\n",
      "Iteration 72, loss = 1.41242791\n",
      "Iteration 73, loss = 1.41138144\n",
      "Iteration 74, loss = 1.41033642\n",
      "Iteration 75, loss = 1.40929285\n",
      "Iteration 76, loss = 1.40825076\n",
      "Iteration 77, loss = 1.40721016\n",
      "Iteration 78, loss = 1.40617107\n",
      "Iteration 79, loss = 1.40513350\n",
      "Iteration 80, loss = 1.40409748\n",
      "Iteration 81, loss = 1.40306301\n",
      "Iteration 82, loss = 1.40203013\n",
      "Iteration 83, loss = 1.40099883\n",
      "Iteration 84, loss = 1.39996915\n",
      "Iteration 85, loss = 1.39894109\n",
      "Iteration 86, loss = 1.39791468\n",
      "Iteration 87, loss = 1.39688993\n",
      "Iteration 88, loss = 1.39586686\n",
      "Iteration 89, loss = 1.39484549\n",
      "Iteration 90, loss = 1.39382583\n",
      "Iteration 91, loss = 1.39280790\n",
      "Iteration 92, loss = 1.39179173\n",
      "Iteration 93, loss = 1.39077732\n",
      "Iteration 94, loss = 1.38976469\n",
      "Iteration 95, loss = 1.38875387\n",
      "Iteration 96, loss = 1.38774486\n",
      "Iteration 97, loss = 1.38673769\n",
      "Iteration 98, loss = 1.38573238\n",
      "Iteration 99, loss = 1.38472894\n",
      "Iteration 100, loss = 1.38372738\n",
      "Iteration 101, loss = 1.38272774\n",
      "Iteration 102, loss = 1.38173002\n",
      "Iteration 103, loss = 1.38073424\n",
      "Iteration 104, loss = 1.37974042\n",
      "Iteration 105, loss = 1.37874858\n",
      "Iteration 106, loss = 1.37775873\n",
      "Iteration 107, loss = 1.37677090\n",
      "Iteration 108, loss = 1.37578509\n",
      "Iteration 109, loss = 1.37480134\n",
      "Iteration 110, loss = 1.37381964\n",
      "Iteration 111, loss = 1.37284003\n",
      "Iteration 112, loss = 1.37186252\n",
      "Iteration 113, loss = 1.37088712\n",
      "Iteration 114, loss = 1.36991386\n",
      "Iteration 115, loss = 1.36894275\n",
      "Iteration 116, loss = 1.36797380\n",
      "Iteration 117, loss = 1.36700705\n",
      "Iteration 118, loss = 1.36604249\n",
      "Iteration 119, loss = 1.36508015\n",
      "Iteration 120, loss = 1.36412005\n",
      "Iteration 121, loss = 1.36316220\n",
      "Iteration 122, loss = 1.36220662\n",
      "Iteration 123, loss = 1.36125333\n",
      "Iteration 124, loss = 1.36030234\n",
      "Iteration 125, loss = 1.35935366\n",
      "Iteration 126, loss = 1.35840733\n",
      "Iteration 127, loss = 1.35746335\n",
      "Iteration 128, loss = 1.35652173\n",
      "Iteration 129, loss = 1.35558250\n",
      "Iteration 130, loss = 1.35464567\n",
      "Iteration 131, loss = 1.35371126\n",
      "Iteration 132, loss = 1.35277928\n",
      "Iteration 133, loss = 1.35184975\n",
      "Iteration 134, loss = 1.35092268\n",
      "Iteration 135, loss = 1.34999810\n",
      "Iteration 136, loss = 1.34907601\n",
      "Iteration 137, loss = 1.34815643\n",
      "Iteration 138, loss = 1.34723938\n",
      "Iteration 139, loss = 1.34632487\n",
      "Iteration 140, loss = 1.34541292\n",
      "Iteration 141, loss = 1.34450354\n",
      "Iteration 142, loss = 1.34359675\n",
      "Iteration 143, loss = 1.34269256\n",
      "Iteration 144, loss = 1.34179099\n",
      "Iteration 145, loss = 1.34089205\n",
      "Iteration 146, loss = 1.33999576\n",
      "Iteration 147, loss = 1.33910212\n",
      "Iteration 148, loss = 1.33821116\n",
      "Iteration 149, loss = 1.33732289\n",
      "Iteration 150, loss = 1.33643732\n",
      "Iteration 151, loss = 1.33555447\n",
      "Iteration 152, loss = 1.33467435\n",
      "Iteration 153, loss = 1.33379698\n",
      "Iteration 154, loss = 1.33292236\n",
      "Iteration 155, loss = 1.33205051\n",
      "Iteration 156, loss = 1.33118144\n",
      "Iteration 157, loss = 1.33031518\n",
      "Iteration 158, loss = 1.32945172\n",
      "Iteration 159, loss = 1.32859108\n",
      "Iteration 160, loss = 1.32773328\n",
      "Iteration 161, loss = 1.32687833\n",
      "Iteration 162, loss = 1.32602624\n",
      "Iteration 163, loss = 1.32517702\n",
      "Iteration 164, loss = 1.32433069\n",
      "Iteration 165, loss = 1.32348725\n",
      "Iteration 166, loss = 1.32264672\n",
      "Iteration 167, loss = 1.32180911\n",
      "Iteration 168, loss = 1.32097443\n",
      "Iteration 169, loss = 1.32014269\n",
      "Iteration 170, loss = 1.31931391\n",
      "Iteration 171, loss = 1.31848809\n",
      "Iteration 172, loss = 1.31766525\n",
      "Iteration 173, loss = 1.31684539\n",
      "Iteration 174, loss = 1.31602853\n",
      "Iteration 175, loss = 1.31521467\n",
      "Iteration 176, loss = 1.31440383\n",
      "Iteration 177, loss = 1.31359602\n",
      "Iteration 178, loss = 1.31279124\n",
      "Iteration 179, loss = 1.31198951\n",
      "Iteration 180, loss = 1.31119084\n",
      "Iteration 181, loss = 1.31039523\n",
      "Iteration 182, loss = 1.30960269\n",
      "Iteration 183, loss = 1.30881324\n",
      "Iteration 184, loss = 1.30802688\n",
      "Iteration 185, loss = 1.30724362\n",
      "Iteration 186, loss = 1.30646347\n",
      "Iteration 187, loss = 1.30568643\n",
      "Iteration 188, loss = 1.30491252\n",
      "Iteration 189, loss = 1.30414174\n",
      "Iteration 190, loss = 1.30337411\n",
      "Iteration 191, loss = 1.30260962\n",
      "Iteration 192, loss = 1.30184828\n",
      "Iteration 193, loss = 1.30109011\n",
      "Iteration 194, loss = 1.30033511\n",
      "Iteration 195, loss = 1.29958329\n",
      "Iteration 196, loss = 1.29883464\n",
      "Iteration 197, loss = 1.29808919\n",
      "Iteration 198, loss = 1.29734693\n",
      "Iteration 199, loss = 1.29660788\n",
      "Iteration 200, loss = 1.29587203\n",
      "Iteration 201, loss = 1.29513940\n",
      "Iteration 202, loss = 1.29440998\n",
      "Iteration 203, loss = 1.29368379\n",
      "Iteration 204, loss = 1.29296083\n",
      "Iteration 205, loss = 1.29224110\n",
      "Iteration 206, loss = 1.29152462\n",
      "Iteration 207, loss = 1.29081137\n",
      "Iteration 208, loss = 1.29010138\n",
      "Iteration 209, loss = 1.28939463\n",
      "Iteration 210, loss = 1.28869115\n",
      "Iteration 211, loss = 1.28799092\n",
      "Iteration 212, loss = 1.28729396\n",
      "Iteration 213, loss = 1.28660026\n",
      "Iteration 214, loss = 1.28590984\n",
      "Iteration 215, loss = 1.28522268\n",
      "Iteration 216, loss = 1.28453881\n",
      "Iteration 217, loss = 1.28385821\n",
      "Iteration 218, loss = 1.28318090\n",
      "Iteration 219, loss = 1.28250687\n",
      "Iteration 220, loss = 1.28183613\n",
      "Iteration 221, loss = 1.28116868\n",
      "Iteration 222, loss = 1.28050451\n",
      "Iteration 223, loss = 1.27984364\n",
      "Iteration 224, loss = 1.27918606\n",
      "Iteration 225, loss = 1.27853178\n",
      "Iteration 226, loss = 1.27788079\n",
      "Iteration 227, loss = 1.27723309\n",
      "Iteration 228, loss = 1.27658870\n",
      "Iteration 229, loss = 1.27594760\n",
      "Iteration 230, loss = 1.27530980\n",
      "Iteration 231, loss = 1.27467530\n",
      "Iteration 232, loss = 1.27404409\n",
      "Iteration 233, loss = 1.27341619\n",
      "Iteration 234, loss = 1.27279158\n",
      "Iteration 235, loss = 1.27217026\n",
      "Iteration 236, loss = 1.27155225\n",
      "Iteration 237, loss = 1.27093752\n",
      "Iteration 238, loss = 1.27032610\n",
      "Iteration 239, loss = 1.26971796\n",
      "Iteration 240, loss = 1.26911312\n",
      "Iteration 241, loss = 1.26851156\n",
      "Iteration 242, loss = 1.26791329\n",
      "Iteration 243, loss = 1.26731831\n",
      "Iteration 244, loss = 1.26672661\n",
      "Iteration 245, loss = 1.26613819\n",
      "Iteration 246, loss = 1.26555305\n",
      "Iteration 247, loss = 1.26497119\n",
      "Iteration 248, loss = 1.26439260\n",
      "Iteration 249, loss = 1.26381728\n",
      "Iteration 250, loss = 1.26324522\n",
      "Iteration 251, loss = 1.26267643\n",
      "Iteration 252, loss = 1.26211090\n",
      "Iteration 253, loss = 1.26154862\n",
      "Iteration 254, loss = 1.26098960\n",
      "Iteration 255, loss = 1.26043382\n",
      "Iteration 256, loss = 1.25988129\n",
      "Iteration 257, loss = 1.25933199\n",
      "Iteration 258, loss = 1.25878593\n",
      "Iteration 259, loss = 1.25824311\n",
      "Iteration 260, loss = 1.25770350\n",
      "Iteration 261, loss = 1.25716712\n",
      "Iteration 262, loss = 1.25663395\n",
      "Iteration 263, loss = 1.25610399\n",
      "Iteration 264, loss = 1.25557724\n",
      "Iteration 265, loss = 1.25505369\n",
      "Iteration 266, loss = 1.25453333\n",
      "Iteration 267, loss = 1.25401615\n",
      "Iteration 268, loss = 1.25350216\n",
      "Iteration 269, loss = 1.25299134\n",
      "Iteration 270, loss = 1.25248369\n",
      "Iteration 271, loss = 1.25197920\n",
      "Iteration 272, loss = 1.25147787\n",
      "Iteration 273, loss = 1.25097968\n",
      "Iteration 274, loss = 1.25048464\n",
      "Iteration 275, loss = 1.24999273\n",
      "Iteration 276, loss = 1.24950395\n",
      "Iteration 277, loss = 1.24901829\n",
      "Iteration 278, loss = 1.24853574\n",
      "Iteration 279, loss = 1.24805630\n",
      "Iteration 280, loss = 1.24757995\n",
      "Iteration 281, loss = 1.24710670\n",
      "Iteration 282, loss = 1.24663652\n",
      "Iteration 283, loss = 1.24616942\n",
      "Iteration 284, loss = 1.24570539\n",
      "Iteration 285, loss = 1.24524441\n",
      "Iteration 286, loss = 1.24478647\n",
      "Iteration 287, loss = 1.24433158\n",
      "Iteration 288, loss = 1.24387972\n",
      "Iteration 289, loss = 1.24343088\n",
      "Iteration 290, loss = 1.24298506\n",
      "Iteration 291, loss = 1.24254224\n",
      "Iteration 292, loss = 1.24210241\n",
      "Iteration 293, loss = 1.24166557\n",
      "Iteration 294, loss = 1.24123171\n",
      "Iteration 295, loss = 1.24080081\n",
      "Iteration 296, loss = 1.24037287\n",
      "Iteration 297, loss = 1.23994788\n",
      "Iteration 298, loss = 1.23952582\n",
      "Iteration 299, loss = 1.23910669\n",
      "Iteration 300, loss = 1.23869049\n",
      "Iteration 301, loss = 1.23827718\n",
      "Iteration 302, loss = 1.23786678\n",
      "Iteration 303, loss = 1.23745926\n",
      "Iteration 304, loss = 1.23705463\n",
      "Iteration 305, loss = 1.23665285\n",
      "Iteration 306, loss = 1.23625394\n",
      "Iteration 307, loss = 1.23585786\n",
      "Iteration 308, loss = 1.23546463\n",
      "Iteration 309, loss = 1.23507421\n",
      "Iteration 310, loss = 1.23468661\n",
      "Iteration 311, loss = 1.23430182\n",
      "Iteration 312, loss = 1.23391981\n",
      "Iteration 313, loss = 1.23354058\n",
      "Iteration 314, loss = 1.23316412\n",
      "Iteration 315, loss = 1.23279042\n",
      "Iteration 316, loss = 1.23241946\n",
      "Iteration 317, loss = 1.23205124\n",
      "Iteration 318, loss = 1.23168574\n",
      "Iteration 319, loss = 1.23132296\n",
      "Iteration 320, loss = 1.23096287\n",
      "Iteration 321, loss = 1.23060547\n",
      "Iteration 322, loss = 1.23025075\n",
      "Iteration 323, loss = 1.22989870\n",
      "Iteration 324, loss = 1.22954929\n",
      "Iteration 325, loss = 1.22920253\n",
      "Iteration 326, loss = 1.22885840\n",
      "Iteration 327, loss = 1.22851688\n",
      "Iteration 328, loss = 1.22817797\n",
      "Iteration 329, loss = 1.22784165\n",
      "Iteration 330, loss = 1.22750792\n",
      "Iteration 331, loss = 1.22717675\n",
      "Iteration 332, loss = 1.22684814\n",
      "Iteration 333, loss = 1.22652207\n",
      "Iteration 334, loss = 1.22619853\n",
      "Iteration 335, loss = 1.22587751\n",
      "Iteration 336, loss = 1.22555901\n",
      "Iteration 337, loss = 1.22524299\n",
      "Iteration 338, loss = 1.22492946\n",
      "Iteration 339, loss = 1.22461840\n",
      "Iteration 340, loss = 1.22430979\n",
      "Iteration 341, loss = 1.22400363\n",
      "Iteration 342, loss = 1.22369990\n",
      "Iteration 343, loss = 1.22339859\n",
      "Iteration 344, loss = 1.22309969\n",
      "Iteration 345, loss = 1.22280318\n",
      "Iteration 346, loss = 1.22250905\n",
      "Iteration 347, loss = 1.22221729\n",
      "Iteration 348, loss = 1.22192789\n",
      "Iteration 349, loss = 1.22164083\n",
      "Iteration 350, loss = 1.22135610\n",
      "Iteration 351, loss = 1.22107369\n",
      "Iteration 352, loss = 1.22079358\n",
      "Iteration 353, loss = 1.22051577\n",
      "Iteration 354, loss = 1.22024023\n",
      "Iteration 355, loss = 1.21996697\n",
      "Iteration 356, loss = 1.21969595\n",
      "Iteration 357, loss = 1.21942718\n",
      "Iteration 358, loss = 1.21916063\n",
      "Iteration 359, loss = 1.21889630\n",
      "Iteration 360, loss = 1.21863417\n",
      "Iteration 361, loss = 1.21837423\n",
      "Iteration 362, loss = 1.21811647\n",
      "Iteration 363, loss = 1.21786087\n",
      "Iteration 364, loss = 1.21760742\n",
      "Iteration 365, loss = 1.21735611\n",
      "Iteration 366, loss = 1.21710692\n",
      "Iteration 367, loss = 1.21685985\n",
      "Iteration 368, loss = 1.21661488\n",
      "Iteration 369, loss = 1.21637199\n",
      "Iteration 370, loss = 1.21613118\n",
      "Iteration 371, loss = 1.21589243\n",
      "Iteration 372, loss = 1.21565572\n",
      "Iteration 373, loss = 1.21542105\n",
      "Iteration 374, loss = 1.21518841\n",
      "Iteration 375, loss = 1.21495777\n",
      "Iteration 376, loss = 1.21472913\n",
      "Iteration 377, loss = 1.21450248\n",
      "Iteration 378, loss = 1.21427780\n",
      "Iteration 379, loss = 1.21405508\n",
      "Iteration 380, loss = 1.21383430\n",
      "Iteration 381, loss = 1.21361546\n",
      "Iteration 382, loss = 1.21339854\n",
      "Iteration 383, loss = 1.21318353\n",
      "Iteration 384, loss = 1.21297041\n",
      "Iteration 385, loss = 1.21275918\n",
      "Iteration 386, loss = 1.21254982\n",
      "Iteration 387, loss = 1.21234232\n",
      "Iteration 388, loss = 1.21213666\n",
      "Iteration 389, loss = 1.21193284\n",
      "Iteration 390, loss = 1.21173084\n",
      "Iteration 391, loss = 1.21153065\n",
      "Iteration 392, loss = 1.21133225\n",
      "Iteration 393, loss = 1.21113564\n",
      "Iteration 394, loss = 1.21094080\n",
      "Iteration 395, loss = 1.21074772\n",
      "Iteration 396, loss = 1.21055639\n",
      "Iteration 397, loss = 1.21036680\n",
      "Iteration 398, loss = 1.21017893\n",
      "Iteration 399, loss = 1.20999276\n",
      "Iteration 400, loss = 1.20980830\n",
      "Iteration 401, loss = 1.20962553\n",
      "Iteration 402, loss = 1.20944443\n",
      "Iteration 403, loss = 1.20926499\n",
      "Iteration 404, loss = 1.20908721\n",
      "Iteration 405, loss = 1.20891107\n",
      "Iteration 406, loss = 1.20873655\n",
      "Iteration 407, loss = 1.20856365\n",
      "Iteration 408, loss = 1.20839235\n",
      "Iteration 409, loss = 1.20822265\n",
      "Iteration 410, loss = 1.20805452\n",
      "Iteration 411, loss = 1.20788797\n",
      "Iteration 412, loss = 1.20772297\n",
      "Iteration 413, loss = 1.20755952\n",
      "Iteration 414, loss = 1.20739761\n",
      "Iteration 415, loss = 1.20723722\n",
      "Iteration 416, loss = 1.20707834\n",
      "Iteration 417, loss = 1.20692096\n",
      "Iteration 418, loss = 1.20676507\n",
      "Iteration 419, loss = 1.20661066\n",
      "Iteration 420, loss = 1.20645771\n",
      "Iteration 421, loss = 1.20630622\n",
      "Iteration 422, loss = 1.20615618\n",
      "Iteration 423, loss = 1.20600757\n",
      "Iteration 424, loss = 1.20586038\n",
      "Iteration 425, loss = 1.20571461\n",
      "Iteration 426, loss = 1.20557023\n",
      "Iteration 427, loss = 1.20542725\n",
      "Iteration 428, loss = 1.20528564\n",
      "Iteration 429, loss = 1.20514540\n",
      "Iteration 430, loss = 1.20500652\n",
      "Iteration 431, loss = 1.20486899\n",
      "Iteration 432, loss = 1.20473280\n",
      "Iteration 433, loss = 1.20459793\n",
      "Iteration 434, loss = 1.20446437\n",
      "Iteration 435, loss = 1.20433213\n",
      "Iteration 436, loss = 1.20420117\n",
      "Iteration 437, loss = 1.20407150\n",
      "Iteration 438, loss = 1.20394311\n",
      "Iteration 439, loss = 1.20381598\n",
      "Iteration 440, loss = 1.20369010\n",
      "Iteration 441, loss = 1.20356547\n",
      "Iteration 442, loss = 1.20344207\n",
      "Iteration 443, loss = 1.20331990\n",
      "Iteration 444, loss = 1.20319893\n",
      "Iteration 445, loss = 1.20307918\n",
      "Iteration 446, loss = 1.20296061\n",
      "Iteration 447, loss = 1.20284324\n",
      "Iteration 448, loss = 1.20272703\n",
      "Iteration 449, loss = 1.20261199\n",
      "Iteration 450, loss = 1.20249811\n",
      "Iteration 451, loss = 1.20238537\n",
      "Iteration 452, loss = 1.20227377\n",
      "Iteration 453, loss = 1.20216329\n",
      "Iteration 454, loss = 1.20205394\n",
      "Iteration 455, loss = 1.20194569\n",
      "Iteration 456, loss = 1.20183854\n",
      "Iteration 457, loss = 1.20173248\n",
      "Iteration 458, loss = 1.20162750\n",
      "Iteration 459, loss = 1.20152359\n",
      "Iteration 460, loss = 1.20142074\n",
      "Iteration 461, loss = 1.20131895\n",
      "Iteration 462, loss = 1.20121820\n",
      "Iteration 463, loss = 1.20111849\n",
      "Iteration 464, loss = 1.20101980\n",
      "Iteration 465, loss = 1.20092213\n",
      "Iteration 466, loss = 1.20082547\n",
      "Iteration 467, loss = 1.20072981\n",
      "Iteration 468, loss = 1.20063515\n",
      "Iteration 469, loss = 1.20054146\n",
      "Iteration 470, loss = 1.20044875\n",
      "Iteration 471, loss = 1.20035701\n",
      "Iteration 472, loss = 1.20026622\n",
      "Iteration 473, loss = 1.20017639\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 32.22084630\n",
      "Iteration 2, loss = 32.22084629\n",
      "Iteration 3, loss = 32.22084629\n",
      "Iteration 4, loss = 32.22084628\n",
      "Iteration 5, loss = 32.22084627\n",
      "Iteration 6, loss = 32.22084627\n",
      "Iteration 7, loss = 32.22084626\n",
      "Iteration 8, loss = 32.22084626\n",
      "Iteration 9, loss = 32.22084625\n",
      "Iteration 10, loss = 32.22084625\n",
      "Iteration 11, loss = 32.22084624\n",
      "Iteration 12, loss = 32.22084624\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.44196776\n",
      "Iteration 2, loss = 1.44132615\n",
      "Iteration 3, loss = 1.44068526\n",
      "Iteration 4, loss = 1.44004510\n",
      "Iteration 5, loss = 1.43940568\n",
      "Iteration 6, loss = 1.43876701\n",
      "Iteration 7, loss = 1.43812911\n",
      "Iteration 8, loss = 1.43749198\n",
      "Iteration 9, loss = 1.43685565\n",
      "Iteration 10, loss = 1.43622012\n",
      "Iteration 11, loss = 1.43558540\n",
      "Iteration 12, loss = 1.43495151\n",
      "Iteration 13, loss = 1.43431845\n",
      "Iteration 14, loss = 1.43368625\n",
      "Iteration 15, loss = 1.43305489\n",
      "Iteration 16, loss = 1.43242441\n",
      "Iteration 17, loss = 1.43179481\n",
      "Iteration 18, loss = 1.43116609\n",
      "Iteration 19, loss = 1.43053827\n",
      "Iteration 20, loss = 1.42991135\n",
      "Iteration 21, loss = 1.42928535\n",
      "Iteration 22, loss = 1.42866027\n",
      "Iteration 23, loss = 1.42803612\n",
      "Iteration 24, loss = 1.42741291\n",
      "Iteration 25, loss = 1.42679064\n",
      "Iteration 26, loss = 1.42616932\n",
      "Iteration 27, loss = 1.42554896\n",
      "Iteration 28, loss = 1.42492956\n",
      "Iteration 29, loss = 1.42431113\n",
      "Iteration 30, loss = 1.42369368\n",
      "Iteration 31, loss = 1.42307720\n",
      "Iteration 32, loss = 1.42246171\n",
      "Iteration 33, loss = 1.42184721\n",
      "Iteration 34, loss = 1.42123369\n",
      "Iteration 35, loss = 1.42062118\n",
      "Iteration 36, loss = 1.42000966\n",
      "Iteration 37, loss = 1.41939915\n",
      "Iteration 38, loss = 1.41878965\n",
      "Iteration 39, loss = 1.41818115\n",
      "Iteration 40, loss = 1.41757367\n",
      "Iteration 41, loss = 1.41696720\n",
      "Iteration 42, loss = 1.41636175\n",
      "Iteration 43, loss = 1.41575731\n",
      "Iteration 44, loss = 1.41515390\n",
      "Iteration 45, loss = 1.41455151\n",
      "Iteration 46, loss = 1.41395015\n",
      "Iteration 47, loss = 1.41334981\n",
      "Iteration 48, loss = 1.41275050\n",
      "Iteration 49, loss = 1.41215221\n",
      "Iteration 50, loss = 1.41155496\n",
      "Iteration 51, loss = 1.41095874\n",
      "Iteration 52, loss = 1.41036354\n",
      "Iteration 53, loss = 1.40976938\n",
      "Iteration 54, loss = 1.40917625\n",
      "Iteration 55, loss = 1.40858416\n",
      "Iteration 56, loss = 1.40799309\n",
      "Iteration 57, loss = 1.40740306\n",
      "Iteration 58, loss = 1.40681407\n",
      "Iteration 59, loss = 1.40622611\n",
      "Iteration 60, loss = 1.40563918\n",
      "Iteration 61, loss = 1.40505329\n",
      "Iteration 62, loss = 1.40446843\n",
      "Iteration 63, loss = 1.40388461\n",
      "Iteration 64, loss = 1.40330182\n",
      "Iteration 65, loss = 1.40272007\n",
      "Iteration 66, loss = 1.40213935\n",
      "Iteration 67, loss = 1.40155966\n",
      "Iteration 68, loss = 1.40098101\n",
      "Iteration 69, loss = 1.40040339\n",
      "Iteration 70, loss = 1.39982681\n",
      "Iteration 71, loss = 1.39925126\n",
      "Iteration 72, loss = 1.39867674\n",
      "Iteration 73, loss = 1.39810325\n",
      "Iteration 74, loss = 1.39753080\n",
      "Iteration 75, loss = 1.39695937\n",
      "Iteration 76, loss = 1.39638898\n",
      "Iteration 77, loss = 1.39581962\n",
      "Iteration 78, loss = 1.39525129\n",
      "Iteration 79, loss = 1.39468399\n",
      "Iteration 80, loss = 1.39411772\n",
      "Iteration 81, loss = 1.39355247\n",
      "Iteration 82, loss = 1.39298826\n",
      "Iteration 83, loss = 1.39242507\n",
      "Iteration 84, loss = 1.39186291\n",
      "Iteration 85, loss = 1.39130177\n",
      "Iteration 86, loss = 1.39074166\n",
      "Iteration 87, loss = 1.39018258\n",
      "Iteration 88, loss = 1.38962452\n",
      "Iteration 89, loss = 1.38906749\n",
      "Iteration 90, loss = 1.38851148\n",
      "Iteration 91, loss = 1.38795649\n",
      "Iteration 92, loss = 1.38740252\n",
      "Iteration 93, loss = 1.38684958\n",
      "Iteration 94, loss = 1.38629765\n",
      "Iteration 95, loss = 1.38574675\n",
      "Iteration 96, loss = 1.38519686\n",
      "Iteration 97, loss = 1.38464800\n",
      "Iteration 98, loss = 1.38410015\n",
      "Iteration 99, loss = 1.38355332\n",
      "Iteration 100, loss = 1.38300751\n",
      "Iteration 101, loss = 1.38246271\n",
      "Iteration 102, loss = 1.38191893\n",
      "Iteration 103, loss = 1.38137617\n",
      "Iteration 104, loss = 1.38083441\n",
      "Iteration 105, loss = 1.38029368\n",
      "Iteration 106, loss = 1.37975395\n",
      "Iteration 107, loss = 1.37921524\n",
      "Iteration 108, loss = 1.37867753\n",
      "Iteration 109, loss = 1.37814084\n",
      "Iteration 110, loss = 1.37760516\n",
      "Iteration 111, loss = 1.37707048\n",
      "Iteration 112, loss = 1.37653682\n",
      "Iteration 113, loss = 1.37600416\n",
      "Iteration 114, loss = 1.37547251\n",
      "Iteration 115, loss = 1.37494186\n",
      "Iteration 116, loss = 1.37441222\n",
      "Iteration 117, loss = 1.37388359\n",
      "Iteration 118, loss = 1.37335596\n",
      "Iteration 119, loss = 1.37282933\n",
      "Iteration 120, loss = 1.37230370\n",
      "Iteration 121, loss = 1.37177908\n",
      "Iteration 122, loss = 1.37125545\n",
      "Iteration 123, loss = 1.37073283\n",
      "Iteration 124, loss = 1.37021120\n",
      "Iteration 125, loss = 1.36969058\n",
      "Iteration 126, loss = 1.36917095\n",
      "Iteration 127, loss = 1.36865231\n",
      "Iteration 128, loss = 1.36813468\n",
      "Iteration 129, loss = 1.36761803\n",
      "Iteration 130, loss = 1.36710239\n",
      "Iteration 131, loss = 1.36658773\n",
      "Iteration 132, loss = 1.36607407\n",
      "Iteration 133, loss = 1.36556140\n",
      "Iteration 134, loss = 1.36504972\n",
      "Iteration 135, loss = 1.36453904\n",
      "Iteration 136, loss = 1.36402934\n",
      "Iteration 137, loss = 1.36352063\n",
      "Iteration 138, loss = 1.36301290\n",
      "Iteration 139, loss = 1.36250617\n",
      "Iteration 140, loss = 1.36200042\n",
      "Iteration 141, loss = 1.36149566\n",
      "Iteration 142, loss = 1.36099188\n",
      "Iteration 143, loss = 1.36048908\n",
      "Iteration 144, loss = 1.35998727\n",
      "Iteration 145, loss = 1.35948644\n",
      "Iteration 146, loss = 1.35898659\n",
      "Iteration 147, loss = 1.35848772\n",
      "Iteration 148, loss = 1.35798983\n",
      "Iteration 149, loss = 1.35749292\n",
      "Iteration 150, loss = 1.35699699\n",
      "Iteration 151, loss = 1.35650203\n",
      "Iteration 152, loss = 1.35600805\n",
      "Iteration 153, loss = 1.35551505\n",
      "Iteration 154, loss = 1.35502301\n",
      "Iteration 155, loss = 1.35453196\n",
      "Iteration 156, loss = 1.35404187\n",
      "Iteration 157, loss = 1.35355276\n",
      "Iteration 158, loss = 1.35306462\n",
      "Iteration 159, loss = 1.35257744\n",
      "Iteration 160, loss = 1.35209124\n",
      "Iteration 161, loss = 1.35160601\n",
      "Iteration 162, loss = 1.35112174\n",
      "Iteration 163, loss = 1.35063844\n",
      "Iteration 164, loss = 1.35015610\n",
      "Iteration 165, loss = 1.34967473\n",
      "Iteration 166, loss = 1.34919432\n",
      "Iteration 167, loss = 1.34871488\n",
      "Iteration 168, loss = 1.34823640\n",
      "Iteration 169, loss = 1.34775888\n",
      "Iteration 170, loss = 1.34728232\n",
      "Iteration 171, loss = 1.34680671\n",
      "Iteration 172, loss = 1.34633207\n",
      "Iteration 173, loss = 1.34585839\n",
      "Iteration 174, loss = 1.34538566\n",
      "Iteration 175, loss = 1.34491388\n",
      "Iteration 176, loss = 1.34444306\n",
      "Iteration 177, loss = 1.34397320\n",
      "Iteration 178, loss = 1.34350429\n",
      "Iteration 179, loss = 1.34303633\n",
      "Iteration 180, loss = 1.34256932\n",
      "Iteration 181, loss = 1.34210326\n",
      "Iteration 182, loss = 1.34163815\n",
      "Iteration 183, loss = 1.34117399\n",
      "Iteration 184, loss = 1.34071078\n",
      "Iteration 185, loss = 1.34024851\n",
      "Iteration 186, loss = 1.33978719\n",
      "Iteration 187, loss = 1.33932682\n",
      "Iteration 188, loss = 1.33886738\n",
      "Iteration 189, loss = 1.33840889\n",
      "Iteration 190, loss = 1.33795135\n",
      "Iteration 191, loss = 1.33749474\n",
      "Iteration 192, loss = 1.33703907\n",
      "Iteration 193, loss = 1.33658434\n",
      "Iteration 194, loss = 1.33613055\n",
      "Iteration 195, loss = 1.33567770\n",
      "Iteration 196, loss = 1.33522579\n",
      "Iteration 197, loss = 1.33477480\n",
      "Iteration 198, loss = 1.33432476\n",
      "Iteration 199, loss = 1.33387564\n",
      "Iteration 200, loss = 1.33342746\n",
      "Iteration 201, loss = 1.33298021\n",
      "Iteration 202, loss = 1.33253389\n",
      "Iteration 203, loss = 1.33208850\n",
      "Iteration 204, loss = 1.33164404\n",
      "Iteration 205, loss = 1.33120051\n",
      "Iteration 206, loss = 1.33075790\n",
      "Iteration 207, loss = 1.33031622\n",
      "Iteration 208, loss = 1.32987547\n",
      "Iteration 209, loss = 1.32943564\n",
      "Iteration 210, loss = 1.32899673\n",
      "Iteration 211, loss = 1.32855874\n",
      "Iteration 212, loss = 1.32812167\n",
      "Iteration 213, loss = 1.32768552\n",
      "Iteration 214, loss = 1.32725030\n",
      "Iteration 215, loss = 1.32681599\n",
      "Iteration 216, loss = 1.32638259\n",
      "Iteration 217, loss = 1.32595012\n",
      "Iteration 218, loss = 1.32551855\n",
      "Iteration 219, loss = 1.32508791\n",
      "Iteration 220, loss = 1.32465817\n",
      "Iteration 221, loss = 1.32422935\n",
      "Iteration 222, loss = 1.32380144\n",
      "Iteration 223, loss = 1.32337443\n",
      "Iteration 224, loss = 1.32294834\n",
      "Iteration 225, loss = 1.32252316\n",
      "Iteration 226, loss = 1.32209888\n",
      "Iteration 227, loss = 1.32167551\n",
      "Iteration 228, loss = 1.32125304\n",
      "Iteration 229, loss = 1.32083148\n",
      "Iteration 230, loss = 1.32041082\n",
      "Iteration 231, loss = 1.31999106\n",
      "Iteration 232, loss = 1.31957221\n",
      "Iteration 233, loss = 1.31915425\n",
      "Iteration 234, loss = 1.31873719\n",
      "Iteration 235, loss = 1.31832104\n",
      "Iteration 236, loss = 1.31790577\n",
      "Iteration 237, loss = 1.31749141\n",
      "Iteration 238, loss = 1.31707794\n",
      "Iteration 239, loss = 1.31666536\n",
      "Iteration 240, loss = 1.31625368\n",
      "Iteration 241, loss = 1.31584289\n",
      "Iteration 242, loss = 1.31543299\n",
      "Iteration 243, loss = 1.31502398\n",
      "Iteration 244, loss = 1.31461586\n",
      "Iteration 245, loss = 1.31420862\n",
      "Iteration 246, loss = 1.31380227\n",
      "Iteration 247, loss = 1.31339681\n",
      "Iteration 248, loss = 1.31299224\n",
      "Iteration 249, loss = 1.31258855\n",
      "Iteration 250, loss = 1.31218574\n",
      "Iteration 251, loss = 1.31178381\n",
      "Iteration 252, loss = 1.31138276\n",
      "Iteration 253, loss = 1.31098260\n",
      "Iteration 254, loss = 1.31058331\n",
      "Iteration 255, loss = 1.31018490\n",
      "Iteration 256, loss = 1.30978736\n",
      "Iteration 257, loss = 1.30939071\n",
      "Iteration 258, loss = 1.30899492\n",
      "Iteration 259, loss = 1.30860001\n",
      "Iteration 260, loss = 1.30820597\n",
      "Iteration 261, loss = 1.30781281\n",
      "Iteration 262, loss = 1.30742051\n",
      "Iteration 263, loss = 1.30702908\n",
      "Iteration 264, loss = 1.30663852\n",
      "Iteration 265, loss = 1.30624883\n",
      "Iteration 266, loss = 1.30586001\n",
      "Iteration 267, loss = 1.30547205\n",
      "Iteration 268, loss = 1.30508495\n",
      "Iteration 269, loss = 1.30469872\n",
      "Iteration 270, loss = 1.30431335\n",
      "Iteration 271, loss = 1.30392884\n",
      "Iteration 272, loss = 1.30354519\n",
      "Iteration 273, loss = 1.30316240\n",
      "Iteration 274, loss = 1.30278046\n",
      "Iteration 275, loss = 1.30239939\n",
      "Iteration 276, loss = 1.30201916\n",
      "Iteration 277, loss = 1.30163980\n",
      "Iteration 278, loss = 1.30126128\n",
      "Iteration 279, loss = 1.30088362\n",
      "Iteration 280, loss = 1.30050681\n",
      "Iteration 281, loss = 1.30013085\n",
      "Iteration 282, loss = 1.29975574\n",
      "Iteration 283, loss = 1.29938148\n",
      "Iteration 284, loss = 1.29900806\n",
      "Iteration 285, loss = 1.29863549\n",
      "Iteration 286, loss = 1.29826377\n",
      "Iteration 287, loss = 1.29789289\n",
      "Iteration 288, loss = 1.29752285\n",
      "Iteration 289, loss = 1.29715366\n",
      "Iteration 290, loss = 1.29678530\n",
      "Iteration 291, loss = 1.29641778\n",
      "Iteration 292, loss = 1.29605111\n",
      "Iteration 293, loss = 1.29568527\n",
      "Iteration 294, loss = 1.29532026\n",
      "Iteration 295, loss = 1.29495609\n",
      "Iteration 296, loss = 1.29459276\n",
      "Iteration 297, loss = 1.29423025\n",
      "Iteration 298, loss = 1.29386858\n",
      "Iteration 299, loss = 1.29350774\n",
      "Iteration 300, loss = 1.29314773\n",
      "Iteration 301, loss = 1.29278855\n",
      "Iteration 302, loss = 1.29243020\n",
      "Iteration 303, loss = 1.29207267\n",
      "Iteration 304, loss = 1.29171597\n",
      "Iteration 305, loss = 1.29136009\n",
      "Iteration 306, loss = 1.29100504\n",
      "Iteration 307, loss = 1.29065081\n",
      "Iteration 308, loss = 1.29029739\n",
      "Iteration 309, loss = 1.28994480\n",
      "Iteration 310, loss = 1.28959303\n",
      "Iteration 311, loss = 1.28924207\n",
      "Iteration 312, loss = 1.28889193\n",
      "Iteration 313, loss = 1.28854261\n",
      "Iteration 314, loss = 1.28819410\n",
      "Iteration 315, loss = 1.28784640\n",
      "Iteration 316, loss = 1.28749952\n",
      "Iteration 317, loss = 1.28715345\n",
      "Iteration 318, loss = 1.28680818\n",
      "Iteration 319, loss = 1.28646373\n",
      "Iteration 320, loss = 1.28612008\n",
      "Iteration 321, loss = 1.28577724\n",
      "Iteration 322, loss = 1.28543520\n",
      "Iteration 323, loss = 1.28509397\n",
      "Iteration 324, loss = 1.28475354\n",
      "Iteration 325, loss = 1.28441392\n",
      "Iteration 326, loss = 1.28407509\n",
      "Iteration 327, loss = 1.28373707\n",
      "Iteration 328, loss = 1.28339984\n",
      "Iteration 329, loss = 1.28306341\n",
      "Iteration 330, loss = 1.28272778\n",
      "Iteration 331, loss = 1.28239294\n",
      "Iteration 332, loss = 1.28205889\n",
      "Iteration 333, loss = 1.28172564\n",
      "Iteration 334, loss = 1.28139318\n",
      "Iteration 335, loss = 1.28106151\n",
      "Iteration 336, loss = 1.28073064\n",
      "Iteration 337, loss = 1.28040054\n",
      "Iteration 338, loss = 1.28007124\n",
      "Iteration 339, loss = 1.27974272\n",
      "Iteration 340, loss = 1.27941499\n",
      "Iteration 341, loss = 1.27908804\n",
      "Iteration 342, loss = 1.27876188\n",
      "Iteration 343, loss = 1.27843650\n",
      "Iteration 344, loss = 1.27811189\n",
      "Iteration 345, loss = 1.27778807\n",
      "Iteration 346, loss = 1.27746502\n",
      "Iteration 347, loss = 1.27714275\n",
      "Iteration 348, loss = 1.27682126\n",
      "Iteration 349, loss = 1.27650054\n",
      "Iteration 350, loss = 1.27618060\n",
      "Iteration 351, loss = 1.27586143\n",
      "Iteration 352, loss = 1.27554303\n",
      "Iteration 353, loss = 1.27522540\n",
      "Iteration 354, loss = 1.27490853\n",
      "Iteration 355, loss = 1.27459244\n",
      "Iteration 356, loss = 1.27427711\n",
      "Iteration 357, loss = 1.27396255\n",
      "Iteration 358, loss = 1.27364876\n",
      "Iteration 359, loss = 1.27333572\n",
      "Iteration 360, loss = 1.27302345\n",
      "Iteration 361, loss = 1.27271194\n",
      "Iteration 362, loss = 1.27240119\n",
      "Iteration 363, loss = 1.27209120\n",
      "Iteration 364, loss = 1.27178196\n",
      "Iteration 365, loss = 1.27147349\n",
      "Iteration 366, loss = 1.27116576\n",
      "Iteration 367, loss = 1.27085879\n",
      "Iteration 368, loss = 1.27055258\n",
      "Iteration 369, loss = 1.27024711\n",
      "Iteration 370, loss = 1.26994240\n",
      "Iteration 371, loss = 1.26963843\n",
      "Iteration 372, loss = 1.26933522\n",
      "Iteration 373, loss = 1.26903275\n",
      "Iteration 374, loss = 1.26873103\n",
      "Iteration 375, loss = 1.26843005\n",
      "Iteration 376, loss = 1.26812981\n",
      "Iteration 377, loss = 1.26783032\n",
      "Iteration 378, loss = 1.26753157\n",
      "Iteration 379, loss = 1.26723356\n",
      "Iteration 380, loss = 1.26693628\n",
      "Iteration 381, loss = 1.26663975\n",
      "Iteration 382, loss = 1.26634395\n",
      "Iteration 383, loss = 1.26604889\n",
      "Iteration 384, loss = 1.26575456\n",
      "Iteration 385, loss = 1.26546096\n",
      "Iteration 386, loss = 1.26516810\n",
      "Iteration 387, loss = 1.26487597\n",
      "Iteration 388, loss = 1.26458456\n",
      "Iteration 389, loss = 1.26429389\n",
      "Iteration 390, loss = 1.26400394\n",
      "Iteration 391, loss = 1.26371472\n",
      "Iteration 392, loss = 1.26342622\n",
      "Iteration 393, loss = 1.26313845\n",
      "Iteration 394, loss = 1.26285140\n",
      "Iteration 395, loss = 1.26256507\n",
      "Iteration 396, loss = 1.26227946\n",
      "Iteration 397, loss = 1.26199456\n",
      "Iteration 398, loss = 1.26171039\n",
      "Iteration 399, loss = 1.26142694\n",
      "Iteration 400, loss = 1.26114419\n",
      "Iteration 401, loss = 1.26086217\n",
      "Iteration 402, loss = 1.26058085\n",
      "Iteration 403, loss = 1.26030025\n",
      "Iteration 404, loss = 1.26002036\n",
      "Iteration 405, loss = 1.25974118\n",
      "Iteration 406, loss = 1.25946270\n",
      "Iteration 407, loss = 1.25918494\n",
      "Iteration 408, loss = 1.25890788\n",
      "Iteration 409, loss = 1.25863152\n",
      "Iteration 410, loss = 1.25835587\n",
      "Iteration 411, loss = 1.25808092\n",
      "Iteration 412, loss = 1.25780667\n",
      "Iteration 413, loss = 1.25753312\n",
      "Iteration 414, loss = 1.25726027\n",
      "Iteration 415, loss = 1.25698812\n",
      "Iteration 416, loss = 1.25671667\n",
      "Iteration 417, loss = 1.25644591\n",
      "Iteration 418, loss = 1.25617584\n",
      "Iteration 419, loss = 1.25590647\n",
      "Iteration 420, loss = 1.25563779\n",
      "Iteration 421, loss = 1.25538086\n",
      "Iteration 422, loss = 1.25510249\n",
      "Iteration 423, loss = 1.25483588\n",
      "Iteration 424, loss = 1.25456995\n",
      "Iteration 425, loss = 1.25430471\n",
      "Iteration 426, loss = 1.25404015\n",
      "Iteration 427, loss = 1.25377628\n",
      "Iteration 428, loss = 1.25351309\n",
      "Iteration 429, loss = 1.25325058\n",
      "Iteration 430, loss = 1.25298875\n",
      "Iteration 431, loss = 1.25272760\n",
      "Iteration 432, loss = 1.25246713\n",
      "Iteration 433, loss = 1.25220733\n",
      "Iteration 434, loss = 1.25194821\n",
      "Iteration 435, loss = 1.25168976\n",
      "Iteration 436, loss = 1.25143199\n",
      "Iteration 437, loss = 1.25117489\n",
      "Iteration 438, loss = 1.25091845\n",
      "Iteration 439, loss = 1.25066269\n",
      "Iteration 440, loss = 1.25040760\n",
      "Iteration 441, loss = 1.25015317\n",
      "Iteration 442, loss = 1.24989941\n",
      "Iteration 443, loss = 1.24964631\n",
      "Iteration 444, loss = 1.24939388\n",
      "Iteration 445, loss = 1.24914210\n",
      "Iteration 446, loss = 1.24889099\n",
      "Iteration 447, loss = 1.24864054\n",
      "Iteration 448, loss = 1.24839075\n",
      "Iteration 449, loss = 1.24814161\n",
      "Iteration 450, loss = 1.24789313\n",
      "Iteration 451, loss = 1.24764531\n",
      "Iteration 452, loss = 1.24739814\n",
      "Iteration 453, loss = 1.24715162\n",
      "Iteration 454, loss = 1.24690575\n",
      "Iteration 455, loss = 1.24666054\n",
      "Iteration 456, loss = 1.24641597\n",
      "Iteration 457, loss = 1.24617205\n",
      "Iteration 458, loss = 1.24592878\n",
      "Iteration 459, loss = 1.24568615\n",
      "Iteration 460, loss = 1.24544416\n",
      "Iteration 461, loss = 1.24520282\n",
      "Iteration 462, loss = 1.24496212\n",
      "Iteration 463, loss = 1.24472206\n",
      "Iteration 464, loss = 1.24448265\n",
      "Iteration 465, loss = 1.24424387\n",
      "Iteration 466, loss = 1.24400572\n",
      "Iteration 467, loss = 1.24376821\n",
      "Iteration 468, loss = 1.24353134\n",
      "Iteration 469, loss = 1.24329510\n",
      "Iteration 470, loss = 1.24305949\n",
      "Iteration 471, loss = 1.24282452\n",
      "Iteration 472, loss = 1.24259017\n",
      "Iteration 473, loss = 1.24235645\n",
      "Iteration 474, loss = 1.24212336\n",
      "Iteration 475, loss = 1.24189090\n",
      "Iteration 476, loss = 1.24165906\n",
      "Iteration 477, loss = 1.24142785\n",
      "Iteration 478, loss = 1.24119726\n",
      "Iteration 479, loss = 1.24096729\n",
      "Iteration 480, loss = 1.24073794\n",
      "Iteration 481, loss = 1.24050921\n",
      "Iteration 482, loss = 1.24028110\n",
      "Iteration 483, loss = 1.24005360\n",
      "Iteration 484, loss = 1.23982672\n",
      "Iteration 485, loss = 1.23960046\n",
      "Iteration 486, loss = 1.23937481\n",
      "Iteration 487, loss = 1.23914977\n",
      "Iteration 488, loss = 1.23892534\n",
      "Iteration 489, loss = 1.23870152\n",
      "Iteration 490, loss = 1.23847831\n",
      "Iteration 491, loss = 1.23825571\n",
      "Iteration 492, loss = 1.23803371\n",
      "Iteration 493, loss = 1.23781232\n",
      "Iteration 494, loss = 1.23759153\n",
      "Iteration 495, loss = 1.23737135\n",
      "Iteration 496, loss = 1.23715176\n",
      "Iteration 497, loss = 1.23693278\n",
      "Iteration 498, loss = 1.23671440\n",
      "Iteration 499, loss = 1.23649661\n",
      "Iteration 500, loss = 1.23627942\n",
      "Iteration 501, loss = 1.23606283\n",
      "Iteration 502, loss = 1.23584683\n",
      "Iteration 503, loss = 1.23563143\n",
      "Iteration 504, loss = 1.23541661\n",
      "Iteration 505, loss = 1.23520239\n",
      "Iteration 506, loss = 1.23498876\n",
      "Iteration 507, loss = 1.23477572\n",
      "Iteration 508, loss = 1.23456326\n",
      "Iteration 509, loss = 1.23435139\n",
      "Iteration 510, loss = 1.23414010\n",
      "Iteration 511, loss = 1.23392940\n",
      "Iteration 512, loss = 1.23371928\n",
      "Iteration 513, loss = 1.23350975\n",
      "Iteration 514, loss = 1.23330079\n",
      "Iteration 515, loss = 1.23309241\n",
      "Iteration 516, loss = 1.23288461\n",
      "Iteration 517, loss = 1.23267739\n",
      "Iteration 518, loss = 1.23247074\n",
      "Iteration 519, loss = 1.23226467\n",
      "Iteration 520, loss = 1.23205917\n",
      "Iteration 521, loss = 1.23185425\n",
      "Iteration 522, loss = 1.23164989\n",
      "Iteration 523, loss = 1.23144610\n",
      "Iteration 524, loss = 1.23124289\n",
      "Iteration 525, loss = 1.23104024\n",
      "Iteration 526, loss = 1.23083815\n",
      "Iteration 527, loss = 1.23063664\n",
      "Iteration 528, loss = 1.23043568\n",
      "Iteration 529, loss = 1.23023529\n",
      "Iteration 530, loss = 1.23003546\n",
      "Iteration 531, loss = 1.22983619\n",
      "Iteration 532, loss = 1.22963748\n",
      "Iteration 533, loss = 1.22943933\n",
      "Iteration 534, loss = 1.22924174\n",
      "Iteration 535, loss = 1.22904470\n",
      "Iteration 536, loss = 1.22884822\n",
      "Iteration 537, loss = 1.22865229\n",
      "Iteration 538, loss = 1.22845691\n",
      "Iteration 539, loss = 1.22826208\n",
      "Iteration 540, loss = 1.22806781\n",
      "Iteration 541, loss = 1.22787408\n",
      "Iteration 542, loss = 1.22768090\n",
      "Iteration 543, loss = 1.22748827\n",
      "Iteration 544, loss = 1.22729618\n",
      "Iteration 545, loss = 1.22710464\n",
      "Iteration 546, loss = 1.22691364\n",
      "Iteration 547, loss = 1.22672318\n",
      "Iteration 548, loss = 1.22653326\n",
      "Iteration 549, loss = 1.22634389\n",
      "Iteration 550, loss = 1.22615505\n",
      "Iteration 551, loss = 1.22596675\n",
      "Iteration 552, loss = 1.22577899\n",
      "Iteration 553, loss = 1.22559176\n",
      "Iteration 554, loss = 1.22540506\n",
      "Iteration 555, loss = 1.22521890\n",
      "Iteration 556, loss = 1.22503327\n",
      "Iteration 557, loss = 1.22484817\n",
      "Iteration 558, loss = 1.22466360\n",
      "Iteration 559, loss = 1.22447956\n",
      "Iteration 560, loss = 1.22429605\n",
      "Iteration 561, loss = 1.22411306\n",
      "Iteration 562, loss = 1.22393059\n",
      "Iteration 563, loss = 1.22374866\n",
      "Iteration 564, loss = 1.22356724\n",
      "Iteration 565, loss = 1.22338634\n",
      "Iteration 566, loss = 1.22320597\n",
      "Iteration 567, loss = 1.22302611\n",
      "Iteration 568, loss = 1.22284678\n",
      "Iteration 569, loss = 1.22266796\n",
      "Iteration 570, loss = 1.22248965\n",
      "Iteration 571, loss = 1.22231186\n",
      "Iteration 572, loss = 1.22213459\n",
      "Iteration 573, loss = 1.22195782\n",
      "Iteration 574, loss = 1.22178157\n",
      "Iteration 575, loss = 1.22160583\n",
      "Iteration 576, loss = 1.22143059\n",
      "Iteration 577, loss = 1.22125587\n",
      "Iteration 578, loss = 1.22108165\n",
      "Iteration 579, loss = 1.22090794\n",
      "Iteration 580, loss = 1.22073473\n",
      "Iteration 581, loss = 1.22056203\n",
      "Iteration 582, loss = 1.22038983\n",
      "Iteration 583, loss = 1.22021813\n",
      "Iteration 584, loss = 1.22004693\n",
      "Iteration 585, loss = 1.21987623\n",
      "Iteration 586, loss = 1.21970602\n",
      "Iteration 587, loss = 1.21953632\n",
      "Iteration 588, loss = 1.21936711\n",
      "Iteration 589, loss = 1.21919839\n",
      "Iteration 590, loss = 1.21903017\n",
      "Iteration 591, loss = 1.21886244\n",
      "Iteration 592, loss = 1.21869520\n",
      "Iteration 593, loss = 1.21852845\n",
      "Iteration 594, loss = 1.21836219\n",
      "Iteration 595, loss = 1.21819642\n",
      "Iteration 596, loss = 1.21803114\n",
      "Iteration 597, loss = 1.21786634\n",
      "Iteration 598, loss = 1.21770203\n",
      "Iteration 599, loss = 1.21753820\n",
      "Iteration 600, loss = 1.21737485\n",
      "Iteration 601, loss = 1.21721198\n",
      "Iteration 602, loss = 1.21704959\n",
      "Iteration 603, loss = 1.21688769\n",
      "Iteration 604, loss = 1.21672626\n",
      "Iteration 605, loss = 1.21656531\n",
      "Iteration 606, loss = 1.21640483\n",
      "Iteration 607, loss = 1.21624483\n",
      "Iteration 608, loss = 1.21608530\n",
      "Iteration 609, loss = 1.21592625\n",
      "Iteration 610, loss = 1.21576766\n",
      "Iteration 611, loss = 1.21560955\n",
      "Iteration 612, loss = 1.21545191\n",
      "Iteration 613, loss = 1.21529473\n",
      "Iteration 614, loss = 1.21513803\n",
      "Iteration 615, loss = 1.21498178\n",
      "Iteration 616, loss = 1.21482601\n",
      "Iteration 617, loss = 1.21467069\n",
      "Iteration 618, loss = 1.21451584\n",
      "Iteration 619, loss = 1.21436146\n",
      "Iteration 620, loss = 1.21420753\n",
      "Iteration 621, loss = 1.21405406\n",
      "Iteration 622, loss = 1.21390105\n",
      "Iteration 623, loss = 1.21374850\n",
      "Iteration 624, loss = 1.21359641\n",
      "Iteration 625, loss = 1.21344477\n",
      "Iteration 626, loss = 1.21329358\n",
      "Iteration 627, loss = 1.21314285\n",
      "Iteration 628, loss = 1.21299257\n",
      "Iteration 629, loss = 1.21284274\n",
      "Iteration 630, loss = 1.21269336\n",
      "Iteration 631, loss = 1.21254443\n",
      "Iteration 632, loss = 1.21239595\n",
      "Iteration 633, loss = 1.21224792\n",
      "Iteration 634, loss = 1.21210033\n",
      "Iteration 635, loss = 1.21195318\n",
      "Iteration 636, loss = 1.21180648\n",
      "Iteration 637, loss = 1.21166023\n",
      "Iteration 638, loss = 1.21151441\n",
      "Iteration 639, loss = 1.21136903\n",
      "Iteration 640, loss = 1.21122410\n",
      "Iteration 641, loss = 1.21107960\n",
      "Iteration 642, loss = 1.21093554\n",
      "Iteration 643, loss = 1.21079191\n",
      "Iteration 644, loss = 1.21064873\n",
      "Iteration 645, loss = 1.21050597\n",
      "Iteration 646, loss = 1.21036365\n",
      "Iteration 647, loss = 1.21022176\n",
      "Iteration 648, loss = 1.21008030\n",
      "Iteration 649, loss = 1.20993928\n",
      "Iteration 650, loss = 1.20979868\n",
      "Iteration 651, loss = 1.20965851\n",
      "Iteration 652, loss = 1.20951876\n",
      "Iteration 653, loss = 1.20937944\n",
      "Iteration 654, loss = 1.20924055\n",
      "Iteration 655, loss = 1.20910208\n",
      "Iteration 656, loss = 1.20896404\n",
      "Iteration 657, loss = 1.20882641\n",
      "Iteration 658, loss = 1.20868921\n",
      "Iteration 659, loss = 1.20855242\n",
      "Iteration 660, loss = 1.20841606\n",
      "Iteration 661, loss = 1.20828011\n",
      "Iteration 662, loss = 1.20814458\n",
      "Iteration 663, loss = 1.20800947\n",
      "Iteration 664, loss = 1.20787477\n",
      "Iteration 665, loss = 1.20774048\n",
      "Iteration 666, loss = 1.20760661\n",
      "Iteration 667, loss = 1.20747314\n",
      "Iteration 668, loss = 1.20734009\n",
      "Iteration 669, loss = 1.20720745\n",
      "Iteration 670, loss = 1.20707522\n",
      "Iteration 671, loss = 1.20694339\n",
      "Iteration 672, loss = 1.20681197\n",
      "Iteration 673, loss = 1.20668096\n",
      "Iteration 674, loss = 1.20655035\n",
      "Iteration 675, loss = 1.20642014\n",
      "Iteration 676, loss = 1.20629034\n",
      "Iteration 677, loss = 1.20616094\n",
      "Iteration 678, loss = 1.20603193\n",
      "Iteration 679, loss = 1.20590333\n",
      "Iteration 680, loss = 1.20577513\n",
      "Iteration 681, loss = 1.20564732\n",
      "Iteration 682, loss = 1.20551992\n",
      "Iteration 683, loss = 1.20539290\n",
      "Iteration 684, loss = 1.20526628\n",
      "Iteration 685, loss = 1.20514006\n",
      "Iteration 686, loss = 1.20501422\n",
      "Iteration 687, loss = 1.20488878\n",
      "Iteration 688, loss = 1.20476373\n",
      "Iteration 689, loss = 1.20463907\n",
      "Iteration 690, loss = 1.20451480\n",
      "Iteration 691, loss = 1.20439092\n",
      "Iteration 692, loss = 1.20426742\n",
      "Iteration 693, loss = 1.20414431\n",
      "Iteration 694, loss = 1.20402158\n",
      "Iteration 695, loss = 1.20389924\n",
      "Iteration 696, loss = 1.20377727\n",
      "Iteration 697, loss = 1.20365570\n",
      "Iteration 698, loss = 1.20353450\n",
      "Iteration 699, loss = 1.20341368\n",
      "Iteration 700, loss = 1.20329324\n",
      "Iteration 701, loss = 1.20317318\n",
      "Iteration 702, loss = 1.20305349\n",
      "Iteration 703, loss = 1.20293419\n",
      "Iteration 704, loss = 1.20281525\n",
      "Iteration 705, loss = 1.20269669\n",
      "Iteration 706, loss = 1.20257851\n",
      "Iteration 707, loss = 1.20246069\n",
      "Iteration 708, loss = 1.20234325\n",
      "Iteration 709, loss = 1.20222618\n",
      "Iteration 710, loss = 1.20210948\n",
      "Iteration 711, loss = 1.20199314\n",
      "Iteration 712, loss = 1.20187717\n",
      "Iteration 713, loss = 1.20176157\n",
      "Iteration 714, loss = 1.20164634\n",
      "Iteration 715, loss = 1.20153147\n",
      "Iteration 716, loss = 1.20141696\n",
      "Iteration 717, loss = 1.20130281\n",
      "Iteration 718, loss = 1.20118903\n",
      "Iteration 719, loss = 1.20107561\n",
      "Iteration 720, loss = 1.20096255\n",
      "Iteration 721, loss = 1.20084984\n",
      "Iteration 722, loss = 1.20073750\n",
      "Iteration 723, loss = 1.20062551\n",
      "Iteration 724, loss = 1.20051388\n",
      "Iteration 725, loss = 1.20040260\n",
      "Iteration 726, loss = 1.20029168\n",
      "Iteration 727, loss = 1.20018111\n",
      "Iteration 728, loss = 1.20007089\n",
      "Iteration 729, loss = 1.19996103\n",
      "Iteration 730, loss = 1.19985151\n",
      "Iteration 731, loss = 1.19974235\n",
      "Iteration 732, loss = 1.19963353\n",
      "Iteration 733, loss = 1.19952506\n",
      "Iteration 734, loss = 1.19941694\n",
      "Iteration 735, loss = 1.19930917\n",
      "Iteration 736, loss = 1.19920174\n",
      "Iteration 737, loss = 1.19909465\n",
      "Iteration 738, loss = 1.19898791\n",
      "Iteration 739, loss = 1.19888150\n",
      "Iteration 740, loss = 1.19877544\n",
      "Iteration 741, loss = 1.19866973\n",
      "Iteration 742, loss = 1.19856435\n",
      "Iteration 743, loss = 1.19845931\n",
      "Iteration 744, loss = 1.19835460\n",
      "Iteration 745, loss = 1.19825024\n",
      "Iteration 746, loss = 1.19814621\n",
      "Iteration 747, loss = 1.19804251\n",
      "Iteration 748, loss = 1.19793915\n",
      "Iteration 749, loss = 1.19783612\n",
      "Iteration 750, loss = 1.19773343\n",
      "Iteration 751, loss = 1.19763106\n",
      "Iteration 752, loss = 1.19752903\n",
      "Iteration 753, loss = 1.19742733\n",
      "Iteration 754, loss = 1.19732595\n",
      "Iteration 755, loss = 1.19722491\n",
      "Iteration 756, loss = 1.19712419\n",
      "Iteration 757, loss = 1.19702379\n",
      "Iteration 758, loss = 1.19692373\n",
      "Iteration 759, loss = 1.19682398\n",
      "Iteration 760, loss = 1.19672456\n",
      "Iteration 761, loss = 1.19662547\n",
      "Iteration 762, loss = 1.19652669\n",
      "Iteration 763, loss = 1.19642824\n",
      "Iteration 764, loss = 1.19633010\n",
      "Iteration 765, loss = 1.19623229\n",
      "Iteration 766, loss = 1.19613479\n",
      "Iteration 767, loss = 1.19603761\n",
      "Iteration 768, loss = 1.19594075\n",
      "Iteration 769, loss = 1.19584420\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "MLP: 0.487500 (0.247313)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7) # definindo uma semente global\n",
    "\n",
    "# Avaliação dos modelos\n",
    "results = []\n",
    "names = []\n",
    "for name, model in models:\n",
    "    kfold = KFold(n_splits=num_folds)\n",
    "    cv_results = cross_val_score(\n",
    "        model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Comparação dos modelos\n",
    "fig = plt.figure()\n",
    "fig.suptitle('Comparação dos Modelos')\n",
    "ax = fig.add_subplot(111)\n",
    "plt.boxplot(results)\n",
    "ax.set_xticklabels(names)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.67504074\n",
      "Iteration 2, loss = 1.67381157\n",
      "Iteration 3, loss = 1.67258411\n",
      "Iteration 4, loss = 1.67135841\n",
      "Iteration 5, loss = 1.67013449\n",
      "Iteration 6, loss = 1.66891239\n",
      "Iteration 7, loss = 1.66769216\n",
      "Iteration 8, loss = 1.66647381\n",
      "Iteration 9, loss = 1.66525739\n",
      "Iteration 10, loss = 1.66404292\n",
      "Iteration 11, loss = 1.66283042\n",
      "Iteration 12, loss = 1.66161994\n",
      "Iteration 13, loss = 1.66041149\n",
      "Iteration 14, loss = 1.65920509\n",
      "Iteration 15, loss = 1.65800078\n",
      "Iteration 16, loss = 1.65679856\n",
      "Iteration 17, loss = 1.65559846\n",
      "Iteration 18, loss = 1.65440050\n",
      "Iteration 19, loss = 1.65320468\n",
      "Iteration 20, loss = 1.65201104\n",
      "Iteration 21, loss = 1.65081957\n",
      "Iteration 22, loss = 1.64963029\n",
      "Iteration 23, loss = 1.64844320\n",
      "Iteration 24, loss = 1.64725833\n",
      "Iteration 25, loss = 1.64607566\n",
      "Iteration 26, loss = 1.64489522\n",
      "Iteration 27, loss = 1.64371700\n",
      "Iteration 28, loss = 1.64254100\n",
      "Iteration 29, loss = 1.64136724\n",
      "Iteration 30, loss = 1.64019570\n",
      "Iteration 31, loss = 1.63902639\n",
      "Iteration 32, loss = 1.63785930\n",
      "Iteration 33, loss = 1.63669444\n",
      "Iteration 34, loss = 1.63553180\n",
      "Iteration 35, loss = 1.63437138\n",
      "Iteration 36, loss = 1.63321317\n",
      "Iteration 37, loss = 1.63205717\n",
      "Iteration 38, loss = 1.63090337\n",
      "Iteration 39, loss = 1.62975177\n",
      "Iteration 40, loss = 1.62860236\n",
      "Iteration 41, loss = 1.62745513\n",
      "Iteration 42, loss = 1.62631008\n",
      "Iteration 43, loss = 1.62516720\n",
      "Iteration 44, loss = 1.62402648\n",
      "Iteration 45, loss = 1.62288791\n",
      "Iteration 46, loss = 1.62175149\n",
      "Iteration 47, loss = 1.62061721\n",
      "Iteration 48, loss = 1.61948506\n",
      "Iteration 49, loss = 1.61835503\n",
      "Iteration 50, loss = 1.61722712\n",
      "Iteration 51, loss = 1.61610132\n",
      "Iteration 52, loss = 1.61497763\n",
      "Iteration 53, loss = 1.61385603\n",
      "Iteration 54, loss = 1.61273653\n",
      "Iteration 55, loss = 1.61161911\n",
      "Iteration 56, loss = 1.61050378\n",
      "Iteration 57, loss = 1.60939052\n",
      "Iteration 58, loss = 1.60827934\n",
      "Iteration 59, loss = 1.60717023\n",
      "Iteration 60, loss = 1.60606318\n",
      "Iteration 61, loss = 1.60495820\n",
      "Iteration 62, loss = 1.60385528\n",
      "Iteration 63, loss = 1.60275442\n",
      "Iteration 64, loss = 1.60165562\n",
      "Iteration 65, loss = 1.60055889\n",
      "Iteration 66, loss = 1.59946421\n",
      "Iteration 67, loss = 1.59837159\n",
      "Iteration 68, loss = 1.59728104\n",
      "Iteration 69, loss = 1.59619255\n",
      "Iteration 70, loss = 1.59510613\n",
      "Iteration 71, loss = 1.59402178\n",
      "Iteration 72, loss = 1.59293950\n",
      "Iteration 73, loss = 1.59185929\n",
      "Iteration 74, loss = 1.59078117\n",
      "Iteration 75, loss = 1.58970513\n",
      "Iteration 76, loss = 1.58863118\n",
      "Iteration 77, loss = 1.58755933\n",
      "Iteration 78, loss = 1.58648958\n",
      "Iteration 79, loss = 1.58542193\n",
      "Iteration 80, loss = 1.58435640\n",
      "Iteration 81, loss = 1.58329299\n",
      "Iteration 82, loss = 1.58223171\n",
      "Iteration 83, loss = 1.58117256\n",
      "Iteration 84, loss = 1.58011555\n",
      "Iteration 85, loss = 1.57906070\n",
      "Iteration 86, loss = 1.57800800\n",
      "Iteration 87, loss = 1.57695747\n",
      "Iteration 88, loss = 1.57590911\n",
      "Iteration 89, loss = 1.57486293\n",
      "Iteration 90, loss = 1.57381894\n",
      "Iteration 91, loss = 1.57277716\n",
      "Iteration 92, loss = 1.57173758\n",
      "Iteration 93, loss = 1.57070022\n",
      "Iteration 94, loss = 1.56966508\n",
      "Iteration 95, loss = 1.56863218\n",
      "Iteration 96, loss = 1.56760152\n",
      "Iteration 97, loss = 1.56657311\n",
      "Iteration 98, loss = 1.56554696\n",
      "Iteration 99, loss = 1.56452309\n",
      "Iteration 100, loss = 1.56350149\n",
      "Iteration 101, loss = 1.56248217\n",
      "Iteration 102, loss = 1.56146515\n",
      "Iteration 103, loss = 1.56045044\n",
      "Iteration 104, loss = 1.55943803\n",
      "Iteration 105, loss = 1.55842795\n",
      "Iteration 106, loss = 1.55742020\n",
      "Iteration 107, loss = 1.55641478\n",
      "Iteration 108, loss = 1.55541170\n",
      "Iteration 109, loss = 1.55441098\n",
      "Iteration 110, loss = 1.55341262\n",
      "Iteration 111, loss = 1.55241663\n",
      "Iteration 112, loss = 1.55142301\n",
      "Iteration 113, loss = 1.55043177\n",
      "Iteration 114, loss = 1.54944292\n",
      "Iteration 115, loss = 1.54845647\n",
      "Iteration 116, loss = 1.54747242\n",
      "Iteration 117, loss = 1.54649078\n",
      "Iteration 118, loss = 1.54551156\n",
      "Iteration 119, loss = 1.54453476\n",
      "Iteration 120, loss = 1.54356038\n",
      "Iteration 121, loss = 1.54258844\n",
      "Iteration 122, loss = 1.54161894\n",
      "Iteration 123, loss = 1.54065188\n",
      "Iteration 124, loss = 1.53968727\n",
      "Iteration 125, loss = 1.53872511\n",
      "Iteration 126, loss = 1.53776541\n",
      "Iteration 127, loss = 1.53680818\n",
      "Iteration 128, loss = 1.53585341\n",
      "Iteration 129, loss = 1.53490111\n",
      "Iteration 130, loss = 1.53395129\n",
      "Iteration 131, loss = 1.53300395\n",
      "Iteration 132, loss = 1.53205909\n",
      "Iteration 133, loss = 1.53111671\n",
      "Iteration 134, loss = 1.53017682\n",
      "Iteration 135, loss = 1.52923942\n",
      "Iteration 136, loss = 1.52830452\n",
      "Iteration 137, loss = 1.52737210\n",
      "Iteration 138, loss = 1.52644219\n",
      "Iteration 139, loss = 1.52551477\n",
      "Iteration 140, loss = 1.52458985\n",
      "Iteration 141, loss = 1.52366742\n",
      "Iteration 142, loss = 1.52274750\n",
      "Iteration 143, loss = 1.52183009\n",
      "Iteration 144, loss = 1.52091517\n",
      "Iteration 145, loss = 1.52000276\n",
      "Iteration 146, loss = 1.51909284\n",
      "Iteration 147, loss = 1.51818543\n",
      "Iteration 148, loss = 1.51728052\n",
      "Iteration 149, loss = 1.51637811\n",
      "Iteration 150, loss = 1.51547820\n",
      "Iteration 151, loss = 1.51458079\n",
      "Iteration 152, loss = 1.51368587\n",
      "Iteration 153, loss = 1.51279345\n",
      "Iteration 154, loss = 1.51190352\n",
      "Iteration 155, loss = 1.51101608\n",
      "Iteration 156, loss = 1.51013113\n",
      "Iteration 157, loss = 1.50924867\n",
      "Iteration 158, loss = 1.50836868\n",
      "Iteration 159, loss = 1.50749118\n",
      "Iteration 160, loss = 1.50661615\n",
      "Iteration 161, loss = 1.50574360\n",
      "Iteration 162, loss = 1.50487413\n",
      "Iteration 163, loss = 1.50400763\n",
      "Iteration 164, loss = 1.50314364\n",
      "Iteration 165, loss = 1.50228213\n",
      "Iteration 166, loss = 1.50142310\n",
      "Iteration 167, loss = 1.50056653\n",
      "Iteration 168, loss = 1.49967902\n",
      "Iteration 169, loss = 1.49872397\n",
      "Iteration 170, loss = 1.49775192\n",
      "Iteration 171, loss = 1.49676702\n",
      "Iteration 172, loss = 1.49577225\n",
      "Iteration 173, loss = 1.49476987\n",
      "Iteration 174, loss = 1.49376169\n",
      "Iteration 175, loss = 1.49274917\n",
      "Iteration 176, loss = 1.49173355\n",
      "Iteration 177, loss = 1.49071586\n",
      "Iteration 178, loss = 1.48969698\n",
      "Iteration 179, loss = 1.48867765\n",
      "Iteration 180, loss = 1.48765853\n",
      "Iteration 181, loss = 1.48664015\n",
      "Iteration 182, loss = 1.48562299\n",
      "Iteration 183, loss = 1.48460747\n",
      "Iteration 184, loss = 1.48359393\n",
      "Iteration 185, loss = 1.48258268\n",
      "Iteration 186, loss = 1.48157398\n",
      "Iteration 187, loss = 1.48056805\n",
      "Iteration 188, loss = 1.47956509\n",
      "Iteration 189, loss = 1.47856525\n",
      "Iteration 190, loss = 1.47756869\n",
      "Iteration 191, loss = 1.47657551\n",
      "Iteration 192, loss = 1.47558582\n",
      "Iteration 193, loss = 1.47459970\n",
      "Iteration 194, loss = 1.47361723\n",
      "Iteration 195, loss = 1.47263846\n",
      "Iteration 196, loss = 1.47166343\n",
      "Iteration 197, loss = 1.47069219\n",
      "Iteration 198, loss = 1.46972476\n",
      "Iteration 199, loss = 1.46876117\n",
      "Iteration 200, loss = 1.46780143\n",
      "Iteration 201, loss = 1.46684555\n",
      "Iteration 202, loss = 1.46589353\n",
      "Iteration 203, loss = 1.46494538\n",
      "Iteration 204, loss = 1.46400109\n",
      "Iteration 205, loss = 1.46306066\n",
      "Iteration 206, loss = 1.46212407\n",
      "Iteration 207, loss = 1.46119133\n",
      "Iteration 208, loss = 1.46026241\n",
      "Iteration 209, loss = 1.45933729\n",
      "Iteration 210, loss = 1.45841598\n",
      "Iteration 211, loss = 1.45749843\n",
      "Iteration 212, loss = 1.45658465\n",
      "Iteration 213, loss = 1.45567460\n",
      "Iteration 214, loss = 1.45476828\n",
      "Iteration 215, loss = 1.45386565\n",
      "Iteration 216, loss = 1.45296669\n",
      "Iteration 217, loss = 1.45207139\n",
      "Iteration 218, loss = 1.45117972\n",
      "Iteration 219, loss = 1.45029167\n",
      "Iteration 220, loss = 1.44940720\n",
      "Iteration 221, loss = 1.44852630\n",
      "Iteration 222, loss = 1.44764894\n",
      "Iteration 223, loss = 1.44677510\n",
      "Iteration 224, loss = 1.44590476\n",
      "Iteration 225, loss = 1.44503790\n",
      "Iteration 226, loss = 1.44417449\n",
      "Iteration 227, loss = 1.44331451\n",
      "Iteration 228, loss = 1.44245795\n",
      "Iteration 229, loss = 1.44160477\n",
      "Iteration 230, loss = 1.44075495\n",
      "Iteration 231, loss = 1.43990848\n",
      "Iteration 232, loss = 1.43906533\n",
      "Iteration 233, loss = 1.43822548\n",
      "Iteration 234, loss = 1.43738891\n",
      "Iteration 235, loss = 1.43655560\n",
      "Iteration 236, loss = 1.43572553\n",
      "Iteration 237, loss = 1.43489867\n",
      "Iteration 238, loss = 1.43407501\n",
      "Iteration 239, loss = 1.43325452\n",
      "Iteration 240, loss = 1.43243720\n",
      "Iteration 241, loss = 1.43162300\n",
      "Iteration 242, loss = 1.43081192\n",
      "Iteration 243, loss = 1.43000394\n",
      "Iteration 244, loss = 1.42919903\n",
      "Iteration 245, loss = 1.42839718\n",
      "Iteration 246, loss = 1.42759837\n",
      "Iteration 247, loss = 1.42680257\n",
      "Iteration 248, loss = 1.42600978\n",
      "Iteration 249, loss = 1.42521996\n",
      "Iteration 250, loss = 1.42443311\n",
      "Iteration 251, loss = 1.42364920\n",
      "Iteration 252, loss = 1.42286821\n",
      "Iteration 253, loss = 1.42209013\n",
      "Iteration 254, loss = 1.42131494\n",
      "Iteration 255, loss = 1.42054262\n",
      "Iteration 256, loss = 1.41977315\n",
      "Iteration 257, loss = 1.41900652\n",
      "Iteration 258, loss = 1.41824270\n",
      "Iteration 259, loss = 1.41748168\n",
      "Iteration 260, loss = 1.41672344\n",
      "Iteration 261, loss = 1.41596796\n",
      "Iteration 262, loss = 1.41521524\n",
      "Iteration 263, loss = 1.41446524\n",
      "Iteration 264, loss = 1.41371795\n",
      "Iteration 265, loss = 1.41297336\n",
      "Iteration 266, loss = 1.41223145\n",
      "Iteration 267, loss = 1.41149220\n",
      "Iteration 268, loss = 1.41075560\n",
      "Iteration 269, loss = 1.41002162\n",
      "Iteration 270, loss = 1.40929025\n",
      "Iteration 271, loss = 1.40856149\n",
      "Iteration 272, loss = 1.40783529\n",
      "Iteration 273, loss = 1.40711167\n",
      "Iteration 274, loss = 1.40639059\n",
      "Iteration 275, loss = 1.40567204\n",
      "Iteration 276, loss = 1.40495600\n",
      "Iteration 277, loss = 1.40424246\n",
      "Iteration 278, loss = 1.40353141\n",
      "Iteration 279, loss = 1.40282282\n",
      "Iteration 280, loss = 1.40211669\n",
      "Iteration 281, loss = 1.40141298\n",
      "Iteration 282, loss = 1.40071170\n",
      "Iteration 283, loss = 1.40001283\n",
      "Iteration 284, loss = 1.39931634\n",
      "Iteration 285, loss = 1.39862223\n",
      "Iteration 286, loss = 1.39793047\n",
      "Iteration 287, loss = 1.39724106\n",
      "Iteration 288, loss = 1.39655398\n",
      "Iteration 289, loss = 1.39586921\n",
      "Iteration 290, loss = 1.39518673\n",
      "Iteration 291, loss = 1.39450655\n",
      "Iteration 292, loss = 1.39382863\n",
      "Iteration 293, loss = 1.39315296\n",
      "Iteration 294, loss = 1.39247953\n",
      "Iteration 295, loss = 1.39180833\n",
      "Iteration 296, loss = 1.39113934\n",
      "Iteration 297, loss = 1.39047255\n",
      "Iteration 298, loss = 1.38980793\n",
      "Iteration 299, loss = 1.38914548\n",
      "Iteration 300, loss = 1.38848519\n",
      "Iteration 301, loss = 1.38782704\n",
      "Iteration 302, loss = 1.38717100\n",
      "Iteration 303, loss = 1.38651709\n",
      "Iteration 304, loss = 1.38586526\n",
      "Iteration 305, loss = 1.38521552\n",
      "Iteration 306, loss = 1.38456785\n",
      "Iteration 307, loss = 1.38392224\n",
      "Iteration 308, loss = 1.38327867\n",
      "Iteration 309, loss = 1.38263713\n",
      "Iteration 310, loss = 1.38199760\n",
      "Iteration 311, loss = 1.38136008\n",
      "Iteration 312, loss = 1.38072454\n",
      "Iteration 313, loss = 1.38009098\n",
      "Iteration 314, loss = 1.37945938\n",
      "Iteration 315, loss = 1.37882973\n",
      "Iteration 316, loss = 1.37820202\n",
      "Iteration 317, loss = 1.37757624\n",
      "Iteration 318, loss = 1.37695236\n",
      "Iteration 319, loss = 1.37633038\n",
      "Iteration 320, loss = 1.37571029\n",
      "Iteration 321, loss = 1.37509207\n",
      "Iteration 322, loss = 1.37447572\n",
      "Iteration 323, loss = 1.37386121\n",
      "Iteration 324, loss = 1.37324854\n",
      "Iteration 325, loss = 1.37263769\n",
      "Iteration 326, loss = 1.37202866\n",
      "Iteration 327, loss = 1.37142142\n",
      "Iteration 328, loss = 1.37081597\n",
      "Iteration 329, loss = 1.37021231\n",
      "Iteration 330, loss = 1.36961040\n",
      "Iteration 331, loss = 1.36901025\n",
      "Iteration 332, loss = 1.36841184\n",
      "Iteration 333, loss = 1.36781516\n",
      "Iteration 334, loss = 1.36722020\n",
      "Iteration 335, loss = 1.36662695\n",
      "Iteration 336, loss = 1.36603539\n",
      "Iteration 337, loss = 1.36544552\n",
      "Iteration 338, loss = 1.36485732\n",
      "Iteration 339, loss = 1.36427079\n",
      "Iteration 340, loss = 1.36368590\n",
      "Iteration 341, loss = 1.36310266\n",
      "Iteration 342, loss = 1.36252105\n",
      "Iteration 343, loss = 1.36194107\n",
      "Iteration 344, loss = 1.36136269\n",
      "Iteration 345, loss = 1.36078591\n",
      "Iteration 346, loss = 1.36021072\n",
      "Iteration 347, loss = 1.35963711\n",
      "Iteration 348, loss = 1.35906506\n",
      "Iteration 349, loss = 1.35849458\n",
      "Iteration 350, loss = 1.35792565\n",
      "Iteration 351, loss = 1.35735825\n",
      "Iteration 352, loss = 1.35679238\n",
      "Iteration 353, loss = 1.35622803\n",
      "Iteration 354, loss = 1.35566520\n",
      "Iteration 355, loss = 1.35510386\n",
      "Iteration 356, loss = 1.35454401\n",
      "Iteration 357, loss = 1.35398565\n",
      "Iteration 358, loss = 1.35342875\n",
      "Iteration 359, loss = 1.35287332\n",
      "Iteration 360, loss = 1.35231934\n",
      "Iteration 361, loss = 1.35176681\n",
      "Iteration 362, loss = 1.35121572\n",
      "Iteration 363, loss = 1.35066605\n",
      "Iteration 364, loss = 1.35011780\n",
      "Iteration 365, loss = 1.34957095\n",
      "Iteration 366, loss = 1.34902551\n",
      "Iteration 367, loss = 1.34848146\n",
      "Iteration 368, loss = 1.34793880\n",
      "Iteration 369, loss = 1.34739751\n",
      "Iteration 370, loss = 1.34685759\n",
      "Iteration 371, loss = 1.34631903\n",
      "Iteration 372, loss = 1.34578182\n",
      "Iteration 373, loss = 1.34524595\n",
      "Iteration 374, loss = 1.34471141\n",
      "Iteration 375, loss = 1.34417821\n",
      "Iteration 376, loss = 1.34364632\n",
      "Iteration 377, loss = 1.34311575\n",
      "Iteration 378, loss = 1.34258647\n",
      "Iteration 379, loss = 1.34205850\n",
      "Iteration 380, loss = 1.34153181\n",
      "Iteration 381, loss = 1.34100641\n",
      "Iteration 382, loss = 1.34048228\n",
      "Iteration 383, loss = 1.33995942\n",
      "Iteration 384, loss = 1.33943781\n",
      "Iteration 385, loss = 1.33891746\n",
      "Iteration 386, loss = 1.33839836\n",
      "Iteration 387, loss = 1.33788049\n",
      "Iteration 388, loss = 1.33736386\n",
      "Iteration 389, loss = 1.33684845\n",
      "Iteration 390, loss = 1.33633426\n",
      "Iteration 391, loss = 1.33582128\n",
      "Iteration 392, loss = 1.33530951\n",
      "Iteration 393, loss = 1.33479893\n",
      "Iteration 394, loss = 1.33428955\n",
      "Iteration 395, loss = 1.33378135\n",
      "Iteration 396, loss = 1.33327434\n",
      "Iteration 397, loss = 1.33276849\n",
      "Iteration 398, loss = 1.33226382\n",
      "Iteration 399, loss = 1.33176030\n",
      "Iteration 400, loss = 1.33125794\n",
      "Iteration 401, loss = 1.33075673\n",
      "Iteration 402, loss = 1.33025666\n",
      "Iteration 403, loss = 1.32975772\n",
      "Iteration 404, loss = 1.32925992\n",
      "Iteration 405, loss = 1.32876325\n",
      "Iteration 406, loss = 1.32826769\n",
      "Iteration 407, loss = 1.32777325\n",
      "Iteration 408, loss = 1.32727992\n",
      "Iteration 409, loss = 1.32678770\n",
      "Iteration 410, loss = 1.32629657\n",
      "Iteration 411, loss = 1.32580653\n",
      "Iteration 412, loss = 1.32531758\n",
      "Iteration 413, loss = 1.32482971\n",
      "Iteration 414, loss = 1.32434292\n",
      "Iteration 415, loss = 1.32385720\n",
      "Iteration 416, loss = 1.32337255\n",
      "Iteration 417, loss = 1.32288896\n",
      "Iteration 418, loss = 1.32240466\n",
      "Iteration 419, loss = 1.32191929\n",
      "Iteration 420, loss = 1.32143452\n",
      "Iteration 421, loss = 1.32095038\n",
      "Iteration 422, loss = 1.32046692\n",
      "Iteration 423, loss = 1.31998417\n",
      "Iteration 424, loss = 1.31950216\n",
      "Iteration 425, loss = 1.31902093\n",
      "Iteration 426, loss = 1.31854049\n",
      "Iteration 427, loss = 1.31806087\n",
      "Iteration 428, loss = 1.31758208\n",
      "Iteration 429, loss = 1.31710415\n",
      "Iteration 430, loss = 1.31662710\n",
      "Iteration 431, loss = 1.31615092\n",
      "Iteration 432, loss = 1.31567564\n",
      "Iteration 433, loss = 1.31520126\n",
      "Iteration 434, loss = 1.31472779\n",
      "Iteration 435, loss = 1.31425525\n",
      "Iteration 436, loss = 1.31378363\n",
      "Iteration 437, loss = 1.31331293\n",
      "Iteration 438, loss = 1.31284317\n",
      "Iteration 439, loss = 1.31237435\n",
      "Iteration 440, loss = 1.31190647\n",
      "Iteration 441, loss = 1.31143953\n",
      "Iteration 442, loss = 1.31097353\n",
      "Iteration 443, loss = 1.31050847\n",
      "Iteration 444, loss = 1.31004436\n",
      "Iteration 445, loss = 1.30958119\n",
      "Iteration 446, loss = 1.30911896\n",
      "Iteration 447, loss = 1.30865767\n",
      "Iteration 448, loss = 1.30819733\n",
      "Iteration 449, loss = 1.30773792\n",
      "Iteration 450, loss = 1.30727945\n",
      "Iteration 451, loss = 1.30682192\n",
      "Iteration 452, loss = 1.30636531\n",
      "Iteration 453, loss = 1.30590964\n",
      "Iteration 454, loss = 1.30545490\n",
      "Iteration 455, loss = 1.30500109\n",
      "Iteration 456, loss = 1.30454820\n",
      "Iteration 457, loss = 1.30409623\n",
      "Iteration 458, loss = 1.30364517\n",
      "Iteration 459, loss = 1.30319504\n",
      "Iteration 460, loss = 1.30274581\n",
      "Iteration 461, loss = 1.30229750\n",
      "Iteration 462, loss = 1.30185009\n",
      "Iteration 463, loss = 1.30140359\n",
      "Iteration 464, loss = 1.30095799\n",
      "Iteration 465, loss = 1.30051329\n",
      "Iteration 466, loss = 1.30006948\n",
      "Iteration 467, loss = 1.29962656\n",
      "Iteration 468, loss = 1.29918453\n",
      "Iteration 469, loss = 1.29874339\n",
      "Iteration 470, loss = 1.29830313\n",
      "Iteration 471, loss = 1.29786375\n",
      "Iteration 472, loss = 1.29742525\n",
      "Iteration 473, loss = 1.29698762\n",
      "Iteration 474, loss = 1.29655087\n",
      "Iteration 475, loss = 1.29611498\n",
      "Iteration 476, loss = 1.29567995\n",
      "Iteration 477, loss = 1.29524579\n",
      "Iteration 478, loss = 1.29481249\n",
      "Iteration 479, loss = 1.29438004\n",
      "Iteration 480, loss = 1.29394844\n",
      "Iteration 481, loss = 1.29351770\n",
      "Iteration 482, loss = 1.29308781\n",
      "Iteration 483, loss = 1.29265875\n",
      "Iteration 484, loss = 1.29223055\n",
      "Iteration 485, loss = 1.29180318\n",
      "Iteration 486, loss = 1.29137664\n",
      "Iteration 487, loss = 1.29095094\n",
      "Iteration 488, loss = 1.29052607\n",
      "Iteration 489, loss = 1.29010204\n",
      "Iteration 490, loss = 1.28967882\n",
      "Iteration 491, loss = 1.28925643\n",
      "Iteration 492, loss = 1.28883486\n",
      "Iteration 493, loss = 1.28841411\n",
      "Iteration 494, loss = 1.28799417\n",
      "Iteration 495, loss = 1.28757505\n",
      "Iteration 496, loss = 1.28715673\n",
      "Iteration 497, loss = 1.28673923\n",
      "Iteration 498, loss = 1.28632253\n",
      "Iteration 499, loss = 1.28590663\n",
      "Iteration 500, loss = 1.28549153\n",
      "Iteration 501, loss = 1.28507723\n",
      "Iteration 502, loss = 1.28466373\n",
      "Iteration 503, loss = 1.28425102\n",
      "Iteration 504, loss = 1.28383910\n",
      "Iteration 505, loss = 1.28342797\n",
      "Iteration 506, loss = 1.28301763\n",
      "Iteration 507, loss = 1.28260807\n",
      "Iteration 508, loss = 1.28219929\n",
      "Iteration 509, loss = 1.28179129\n",
      "Iteration 510, loss = 1.28138407\n",
      "Iteration 511, loss = 1.28097763\n",
      "Iteration 512, loss = 1.28057195\n",
      "Iteration 513, loss = 1.28016705\n",
      "Iteration 514, loss = 1.27976292\n",
      "Iteration 515, loss = 1.27934039\n",
      "Iteration 516, loss = 1.27888972\n",
      "Iteration 517, loss = 1.27842559\n",
      "Iteration 518, loss = 1.27794972\n",
      "Iteration 519, loss = 1.27746367\n",
      "Iteration 520, loss = 1.27696885\n",
      "Iteration 521, loss = 1.27646652\n",
      "Iteration 522, loss = 1.27595781\n",
      "Iteration 523, loss = 1.27544373\n",
      "Iteration 524, loss = 1.27492518\n",
      "Iteration 525, loss = 1.27440298\n",
      "Iteration 526, loss = 1.27387784\n",
      "Iteration 527, loss = 1.27335041\n",
      "Iteration 528, loss = 1.27282126\n",
      "Iteration 529, loss = 1.27229089\n",
      "Iteration 530, loss = 1.27175977\n",
      "Iteration 531, loss = 1.27122828\n",
      "Iteration 532, loss = 1.27069679\n",
      "Iteration 533, loss = 1.27016559\n",
      "Iteration 534, loss = 1.26963498\n",
      "Iteration 535, loss = 1.26910520\n",
      "Iteration 536, loss = 1.26857644\n",
      "Iteration 537, loss = 1.26804892\n",
      "Iteration 538, loss = 1.26752278\n",
      "Iteration 539, loss = 1.26699817\n",
      "Iteration 540, loss = 1.26647521\n",
      "Iteration 541, loss = 1.26595402\n",
      "Iteration 542, loss = 1.26543468\n",
      "Iteration 543, loss = 1.26491728\n",
      "Iteration 544, loss = 1.26440188\n",
      "Iteration 545, loss = 1.26388854\n",
      "Iteration 546, loss = 1.26337731\n",
      "Iteration 547, loss = 1.26286822\n",
      "Iteration 548, loss = 1.26236132\n",
      "Iteration 549, loss = 1.26185662\n",
      "Iteration 550, loss = 1.26135416\n",
      "Iteration 551, loss = 1.26085393\n",
      "Iteration 552, loss = 1.26035597\n",
      "Iteration 553, loss = 1.25986026\n",
      "Iteration 554, loss = 1.25936682\n",
      "Iteration 555, loss = 1.25887565\n",
      "Iteration 556, loss = 1.25838675\n",
      "Iteration 557, loss = 1.25789192\n",
      "Iteration 558, loss = 1.25739679\n",
      "Iteration 559, loss = 1.25690285\n",
      "Iteration 560, loss = 1.25641021\n",
      "Iteration 561, loss = 1.25592491\n",
      "Iteration 562, loss = 1.25546412\n",
      "Iteration 563, loss = 1.25500611\n",
      "Iteration 564, loss = 1.25455077\n",
      "Iteration 565, loss = 1.25409800\n",
      "Iteration 566, loss = 1.25364771\n",
      "Iteration 567, loss = 1.25319983\n",
      "Iteration 568, loss = 1.25275428\n",
      "Iteration 569, loss = 1.25231100\n",
      "Iteration 570, loss = 1.25186993\n",
      "Iteration 571, loss = 1.25143102\n",
      "Iteration 572, loss = 1.25099422\n",
      "Iteration 573, loss = 1.25055947\n",
      "Iteration 574, loss = 1.25012675\n",
      "Iteration 575, loss = 1.24969600\n",
      "Iteration 576, loss = 1.24926720\n",
      "Iteration 577, loss = 1.24884031\n",
      "Iteration 578, loss = 1.24841529\n",
      "Iteration 579, loss = 1.24799212\n",
      "Iteration 580, loss = 1.24757077\n",
      "Iteration 581, loss = 1.24715121\n",
      "Iteration 582, loss = 1.24673341\n",
      "Iteration 583, loss = 1.24631736\n",
      "Iteration 584, loss = 1.24590304\n",
      "Iteration 585, loss = 1.24549041\n",
      "Iteration 586, loss = 1.24507946\n",
      "Iteration 587, loss = 1.24467017\n",
      "Iteration 588, loss = 1.24426251\n",
      "Iteration 589, loss = 1.24385649\n",
      "Iteration 590, loss = 1.24345207\n",
      "Iteration 591, loss = 1.24304923\n",
      "Iteration 592, loss = 1.24264797\n",
      "Iteration 593, loss = 1.24224827\n",
      "Iteration 594, loss = 1.24185010\n",
      "Iteration 595, loss = 1.24145347\n",
      "Iteration 596, loss = 1.24105834\n",
      "Iteration 597, loss = 1.24066472\n",
      "Iteration 598, loss = 1.24027258\n",
      "Iteration 599, loss = 1.23988191\n",
      "Iteration 600, loss = 1.23949271\n",
      "Iteration 601, loss = 1.23910495\n",
      "Iteration 602, loss = 1.23871862\n",
      "Iteration 603, loss = 1.23833372\n",
      "Iteration 604, loss = 1.23795023\n",
      "Iteration 605, loss = 1.23756813\n",
      "Iteration 606, loss = 1.23718743\n",
      "Iteration 607, loss = 1.23680810\n",
      "Iteration 608, loss = 1.23643014\n",
      "Iteration 609, loss = 1.23605353\n",
      "Iteration 610, loss = 1.23567827\n",
      "Iteration 611, loss = 1.23531147\n",
      "Iteration 612, loss = 1.23494494\n",
      "Iteration 613, loss = 1.23457788\n",
      "Iteration 614, loss = 1.23421049\n",
      "Iteration 615, loss = 1.23384293\n",
      "Iteration 616, loss = 1.23347538\n",
      "Iteration 617, loss = 1.23310796\n",
      "Iteration 618, loss = 1.23274079\n",
      "Iteration 619, loss = 1.23237932\n",
      "Iteration 620, loss = 1.23202030\n",
      "Iteration 621, loss = 1.23166206\n",
      "Iteration 622, loss = 1.23130466\n",
      "Iteration 623, loss = 1.23094810\n",
      "Iteration 624, loss = 1.23059243\n",
      "Iteration 625, loss = 1.23023767\n",
      "Iteration 626, loss = 1.22988383\n",
      "Iteration 627, loss = 1.22953094\n",
      "Iteration 628, loss = 1.22917902\n",
      "Iteration 629, loss = 1.22882807\n",
      "Iteration 630, loss = 1.22847811\n",
      "Iteration 631, loss = 1.22812915\n",
      "Iteration 632, loss = 1.22778120\n",
      "Iteration 633, loss = 1.22743427\n",
      "Iteration 634, loss = 1.22708838\n",
      "Iteration 635, loss = 1.22674586\n",
      "Iteration 636, loss = 1.22640283\n",
      "Iteration 637, loss = 1.22606050\n",
      "Iteration 638, loss = 1.22572027\n",
      "Iteration 639, loss = 1.22538089\n",
      "Iteration 640, loss = 1.22504237\n",
      "Iteration 641, loss = 1.22470473\n",
      "Iteration 642, loss = 1.22436798\n",
      "Iteration 643, loss = 1.22403300\n",
      "Iteration 644, loss = 1.22369835\n",
      "Iteration 645, loss = 1.22336506\n",
      "Iteration 646, loss = 1.22303306\n",
      "Iteration 647, loss = 1.22270186\n",
      "Iteration 648, loss = 1.22237150\n",
      "Iteration 649, loss = 1.22204198\n",
      "Iteration 650, loss = 1.22171332\n",
      "Iteration 651, loss = 1.22138553\n",
      "Iteration 652, loss = 1.22106033\n",
      "Iteration 653, loss = 1.22073477\n",
      "Iteration 654, loss = 1.22040921\n",
      "Iteration 655, loss = 1.22008599\n",
      "Iteration 656, loss = 1.21976355\n",
      "Iteration 657, loss = 1.21944192\n",
      "Iteration 658, loss = 1.21912110\n",
      "Iteration 659, loss = 1.21880111\n",
      "Iteration 660, loss = 1.21848271\n",
      "Iteration 661, loss = 1.21816431\n",
      "Iteration 662, loss = 1.21784785\n",
      "Iteration 663, loss = 1.21753221\n",
      "Iteration 664, loss = 1.21721731\n",
      "Iteration 665, loss = 1.21690319\n",
      "Iteration 666, loss = 1.21658984\n",
      "Iteration 667, loss = 1.21627728\n",
      "Iteration 668, loss = 1.21596553\n",
      "Iteration 669, loss = 1.21565458\n",
      "Iteration 670, loss = 1.21534446\n",
      "Iteration 671, loss = 1.21503706\n",
      "Iteration 672, loss = 1.21472901\n",
      "Iteration 673, loss = 1.21442059\n",
      "Iteration 674, loss = 1.21411467\n",
      "Iteration 675, loss = 1.21380948\n",
      "Iteration 676, loss = 1.21350504\n",
      "Iteration 677, loss = 1.21320136\n",
      "Iteration 678, loss = 1.21289845\n",
      "Iteration 679, loss = 1.21259631\n",
      "Iteration 680, loss = 1.21229553\n",
      "Iteration 681, loss = 1.21199489\n",
      "Iteration 682, loss = 1.21169555\n",
      "Iteration 683, loss = 1.21139696\n",
      "Iteration 684, loss = 1.21109943\n",
      "Iteration 685, loss = 1.21080252\n",
      "Iteration 686, loss = 1.21050663\n",
      "Iteration 687, loss = 1.21021146\n",
      "Iteration 688, loss = 1.20991702\n",
      "Iteration 689, loss = 1.20962332\n",
      "Iteration 690, loss = 1.20933037\n",
      "Iteration 691, loss = 1.20903817\n",
      "Iteration 692, loss = 1.20874804\n",
      "Iteration 693, loss = 1.20845423\n",
      "Iteration 694, loss = 1.20811904\n",
      "Iteration 695, loss = 1.20777479\n",
      "Iteration 696, loss = 1.20742857\n",
      "Iteration 697, loss = 1.20709346\n",
      "Iteration 698, loss = 1.20675634\n",
      "Iteration 699, loss = 1.20641768\n",
      "Iteration 700, loss = 1.20607786\n",
      "Iteration 701, loss = 1.20574038\n",
      "Iteration 702, loss = 1.20540197\n",
      "Iteration 703, loss = 1.20506289\n",
      "Iteration 704, loss = 1.20472347\n",
      "Iteration 705, loss = 1.20438405\n",
      "Iteration 706, loss = 1.20404831\n",
      "Iteration 707, loss = 1.20371308\n",
      "Iteration 708, loss = 1.20337815\n",
      "Iteration 709, loss = 1.20304368\n",
      "Iteration 710, loss = 1.20270982\n",
      "Iteration 711, loss = 1.20237715\n",
      "Iteration 712, loss = 1.20204805\n",
      "Iteration 713, loss = 1.20171977\n",
      "Iteration 714, loss = 1.20139248\n",
      "Iteration 715, loss = 1.20106672\n",
      "Iteration 716, loss = 1.20074384\n",
      "Iteration 717, loss = 1.20044773\n",
      "Iteration 718, loss = 1.20015531\n",
      "Iteration 719, loss = 1.19986572\n",
      "Iteration 720, loss = 1.19957881\n",
      "Iteration 721, loss = 1.19929443\n",
      "Iteration 722, loss = 1.19901247\n",
      "Iteration 723, loss = 1.19873282\n",
      "Iteration 724, loss = 1.19845536\n",
      "Iteration 725, loss = 1.19817999\n",
      "Iteration 726, loss = 1.19790664\n",
      "Iteration 727, loss = 1.19763520\n",
      "Iteration 728, loss = 1.19736560\n",
      "Iteration 729, loss = 1.19709821\n",
      "Iteration 730, loss = 1.19683171\n",
      "Iteration 731, loss = 1.19656723\n",
      "Iteration 732, loss = 1.19630428\n",
      "Iteration 733, loss = 1.19604282\n",
      "Iteration 734, loss = 1.19578278\n",
      "Iteration 735, loss = 1.19552414\n",
      "Iteration 736, loss = 1.19526684\n",
      "Iteration 737, loss = 1.19501084\n",
      "Iteration 738, loss = 1.19475611\n",
      "Iteration 739, loss = 1.19450260\n",
      "Iteration 740, loss = 1.19425029\n",
      "Iteration 741, loss = 1.19399914\n",
      "Iteration 742, loss = 1.19374913\n",
      "Iteration 743, loss = 1.19350021\n",
      "Iteration 744, loss = 1.19325236\n",
      "Iteration 745, loss = 1.19300556\n",
      "Iteration 746, loss = 1.19275977\n",
      "Iteration 747, loss = 1.19251498\n",
      "Iteration 748, loss = 1.19227116\n",
      "Iteration 749, loss = 1.19202829\n",
      "Iteration 750, loss = 1.19178634\n",
      "Iteration 751, loss = 1.19154530\n",
      "Iteration 752, loss = 1.19130515\n",
      "Iteration 753, loss = 1.19106586\n",
      "Iteration 754, loss = 1.19082742\n",
      "Iteration 755, loss = 1.19058981\n",
      "Iteration 756, loss = 1.19035302\n",
      "Iteration 757, loss = 1.19011703\n",
      "Iteration 758, loss = 1.18988182\n",
      "Iteration 759, loss = 1.18964737\n",
      "Iteration 760, loss = 1.18941369\n",
      "Iteration 761, loss = 1.18918075\n",
      "Iteration 762, loss = 1.18894853\n",
      "Iteration 763, loss = 1.18871703\n",
      "Iteration 764, loss = 1.18848624\n",
      "Iteration 765, loss = 1.18825614\n",
      "Iteration 766, loss = 1.18802673\n",
      "Iteration 767, loss = 1.18779798\n",
      "Iteration 768, loss = 1.18756990\n",
      "Iteration 769, loss = 1.18734247\n",
      "Iteration 770, loss = 1.18711569\n",
      "Iteration 771, loss = 1.18688953\n",
      "Iteration 772, loss = 1.18666401\n",
      "Iteration 773, loss = 1.18643910\n",
      "Iteration 774, loss = 1.18621480\n",
      "Iteration 775, loss = 1.18599110\n",
      "Iteration 776, loss = 1.18576800\n",
      "Iteration 777, loss = 1.18554549\n",
      "Iteration 778, loss = 1.18532355\n",
      "Iteration 779, loss = 1.18510219\n",
      "Iteration 780, loss = 1.18488140\n",
      "Iteration 781, loss = 1.18466117\n",
      "Iteration 782, loss = 1.18444150\n",
      "Iteration 783, loss = 1.18422237\n",
      "Iteration 784, loss = 1.18400379\n",
      "Iteration 785, loss = 1.18378576\n",
      "Iteration 786, loss = 1.18356825\n",
      "Iteration 787, loss = 1.18335128\n",
      "Iteration 788, loss = 1.18313483\n",
      "Iteration 789, loss = 1.18291891\n",
      "Iteration 790, loss = 1.18270350\n",
      "Iteration 791, loss = 1.18248861\n",
      "Iteration 792, loss = 1.18227422\n",
      "Iteration 793, loss = 1.18206034\n",
      "Iteration 794, loss = 1.18184696\n",
      "Iteration 795, loss = 1.18163408\n",
      "Iteration 796, loss = 1.18142169\n",
      "Iteration 797, loss = 1.18120979\n",
      "Iteration 798, loss = 1.18099838\n",
      "Iteration 799, loss = 1.18078746\n",
      "Iteration 800, loss = 1.18057702\n",
      "Iteration 801, loss = 1.18036705\n",
      "Iteration 802, loss = 1.18015756\n",
      "Iteration 803, loss = 1.17994855\n",
      "Iteration 804, loss = 1.17974001\n",
      "Iteration 805, loss = 1.17953193\n",
      "Iteration 806, loss = 1.17932432\n",
      "Iteration 807, loss = 1.17911718\n",
      "Iteration 808, loss = 1.17891049\n",
      "Iteration 809, loss = 1.17870427\n",
      "Iteration 810, loss = 1.17849850\n",
      "Iteration 811, loss = 1.17829318\n",
      "Iteration 812, loss = 1.17808832\n",
      "Iteration 813, loss = 1.17788391\n",
      "Iteration 814, loss = 1.17768268\n",
      "Iteration 815, loss = 1.17748386\n",
      "Iteration 816, loss = 1.17728340\n",
      "Iteration 817, loss = 1.17708152\n",
      "Iteration 818, loss = 1.17687842\n",
      "Iteration 819, loss = 1.17667690\n",
      "Iteration 820, loss = 1.17647852\n",
      "Iteration 821, loss = 1.17628033\n",
      "Iteration 822, loss = 1.17608234\n",
      "Iteration 823, loss = 1.17588458\n",
      "Iteration 824, loss = 1.17568705\n",
      "Iteration 825, loss = 1.17548977\n",
      "Iteration 826, loss = 1.17529275\n",
      "Iteration 827, loss = 1.17509601\n",
      "Iteration 828, loss = 1.17489956\n",
      "Iteration 829, loss = 1.17470341\n",
      "Iteration 830, loss = 1.17450942\n",
      "Iteration 831, loss = 1.17431593\n",
      "Iteration 832, loss = 1.17412110\n",
      "Iteration 833, loss = 1.17392682\n",
      "Iteration 834, loss = 1.17373444\n",
      "Iteration 835, loss = 1.17354220\n",
      "Iteration 836, loss = 1.17335014\n",
      "Iteration 837, loss = 1.17315827\n",
      "Iteration 838, loss = 1.17296661\n",
      "Iteration 839, loss = 1.17277516\n",
      "Iteration 840, loss = 1.17258643\n",
      "Iteration 841, loss = 1.17239680\n",
      "Iteration 842, loss = 1.17220594\n",
      "Iteration 843, loss = 1.17201714\n",
      "Iteration 844, loss = 1.17182932\n",
      "Iteration 845, loss = 1.17164159\n",
      "Iteration 846, loss = 1.17145398\n",
      "Iteration 847, loss = 1.17126649\n",
      "Iteration 848, loss = 1.17107917\n",
      "Iteration 849, loss = 1.17089202\n",
      "Iteration 850, loss = 1.17070507\n",
      "Iteration 851, loss = 1.17052174\n",
      "Iteration 852, loss = 1.17033728\n",
      "Iteration 853, loss = 1.17015157\n",
      "Iteration 854, loss = 1.16996513\n",
      "Iteration 855, loss = 1.16978184\n",
      "Iteration 856, loss = 1.16959860\n",
      "Iteration 857, loss = 1.16941544\n",
      "Iteration 858, loss = 1.16923239\n",
      "Iteration 859, loss = 1.16904947\n",
      "Iteration 860, loss = 1.16886955\n",
      "Iteration 861, loss = 1.16868830\n",
      "Iteration 862, loss = 1.16850587\n",
      "Iteration 863, loss = 1.16832534\n",
      "Iteration 864, loss = 1.16814594\n",
      "Iteration 865, loss = 1.16796653\n",
      "Iteration 866, loss = 1.16778716\n",
      "Iteration 867, loss = 1.16760785\n",
      "Iteration 868, loss = 1.16742863\n",
      "Iteration 869, loss = 1.16724953\n",
      "Iteration 870, loss = 1.16707284\n",
      "Iteration 871, loss = 1.16689608\n",
      "Iteration 872, loss = 1.16671812\n",
      "Iteration 873, loss = 1.16654082\n",
      "Iteration 874, loss = 1.16636529\n",
      "Iteration 875, loss = 1.16618974\n",
      "Iteration 876, loss = 1.16601420\n",
      "Iteration 877, loss = 1.16583870\n",
      "Iteration 878, loss = 1.16566328\n",
      "Iteration 879, loss = 1.16549095\n",
      "Iteration 880, loss = 1.16531757\n",
      "Iteration 881, loss = 1.16514303\n",
      "Iteration 882, loss = 1.16496916\n",
      "Iteration 883, loss = 1.16479724\n",
      "Iteration 884, loss = 1.16462526\n",
      "Iteration 885, loss = 1.16445326\n",
      "Iteration 886, loss = 1.16428128\n",
      "Iteration 887, loss = 1.16410934\n",
      "Iteration 888, loss = 1.16394032\n",
      "Iteration 889, loss = 1.16377049\n",
      "Iteration 890, loss = 1.16359950\n",
      "Iteration 891, loss = 1.16342913\n",
      "Iteration 892, loss = 1.16326068\n",
      "Iteration 893, loss = 1.16309214\n",
      "Iteration 894, loss = 1.16292355\n",
      "Iteration 895, loss = 1.16275496\n",
      "Iteration 896, loss = 1.16258664\n",
      "Iteration 897, loss = 1.16241944\n",
      "Iteration 898, loss = 1.16225292\n",
      "Iteration 899, loss = 1.16208666\n",
      "Iteration 900, loss = 1.16192034\n",
      "Iteration 901, loss = 1.16175400\n",
      "Iteration 902, loss = 1.16159007\n",
      "Iteration 903, loss = 1.16142518\n",
      "Iteration 904, loss = 1.16125919\n",
      "Iteration 905, loss = 1.16109587\n",
      "Iteration 906, loss = 1.16093286\n",
      "Iteration 907, loss = 1.16076972\n",
      "Iteration 908, loss = 1.16060648\n",
      "Iteration 909, loss = 1.16044319\n",
      "Iteration 910, loss = 1.16027989\n",
      "Iteration 911, loss = 1.16011739\n",
      "Iteration 912, loss = 1.15995635\n",
      "Iteration 913, loss = 1.15979417\n",
      "Iteration 914, loss = 1.15963416\n",
      "Iteration 915, loss = 1.15947429\n",
      "Iteration 916, loss = 1.15931429\n",
      "Iteration 917, loss = 1.15915418\n",
      "Iteration 918, loss = 1.15899402\n",
      "Iteration 919, loss = 1.15883423\n",
      "Iteration 920, loss = 1.15867584\n",
      "Iteration 921, loss = 1.15851724\n",
      "Iteration 922, loss = 1.15835938\n",
      "Iteration 923, loss = 1.15820141\n",
      "Iteration 924, loss = 1.15804526\n",
      "Iteration 925, loss = 1.15788815\n",
      "Iteration 926, loss = 1.15773098\n",
      "Iteration 927, loss = 1.15757517\n",
      "Iteration 928, loss = 1.15741922\n",
      "Iteration 929, loss = 1.15726321\n",
      "Iteration 930, loss = 1.15710836\n",
      "Iteration 931, loss = 1.15695341\n",
      "Iteration 932, loss = 1.15680005\n",
      "Iteration 933, loss = 1.15664565\n",
      "Iteration 934, loss = 1.15649192\n",
      "Iteration 935, loss = 1.15633906\n",
      "Iteration 936, loss = 1.15618602\n",
      "Iteration 937, loss = 1.15603287\n",
      "Iteration 938, loss = 1.15588116\n",
      "Iteration 939, loss = 1.15572938\n",
      "Iteration 940, loss = 1.15557692\n",
      "Iteration 941, loss = 1.15542595\n",
      "Iteration 942, loss = 1.15527482\n",
      "Iteration 943, loss = 1.15512548\n",
      "Iteration 944, loss = 1.15497511\n",
      "Iteration 945, loss = 1.15482478\n",
      "Iteration 946, loss = 1.15467572\n",
      "Iteration 947, loss = 1.15452648\n",
      "Iteration 948, loss = 1.15437719\n",
      "Iteration 949, loss = 1.15422897\n",
      "Iteration 950, loss = 1.15408080\n",
      "Iteration 951, loss = 1.15393357\n",
      "Iteration 952, loss = 1.15378626\n",
      "Iteration 953, loss = 1.15363878\n",
      "Iteration 954, loss = 1.15349345\n",
      "Iteration 955, loss = 1.15334716\n",
      "Iteration 956, loss = 1.15319979\n",
      "Iteration 957, loss = 1.15305573\n",
      "Iteration 958, loss = 1.15291139\n",
      "Iteration 959, loss = 1.15276678\n",
      "Iteration 960, loss = 1.15262195\n",
      "Iteration 961, loss = 1.15247696\n",
      "Iteration 962, loss = 1.15233185\n",
      "Iteration 963, loss = 1.15218908\n",
      "Iteration 964, loss = 1.15204680\n",
      "Iteration 965, loss = 1.15190336\n",
      "Iteration 966, loss = 1.15175891\n",
      "Iteration 967, loss = 1.15161857\n",
      "Iteration 968, loss = 1.15147788\n",
      "Iteration 969, loss = 1.15133687\n",
      "Iteration 970, loss = 1.15119560\n",
      "Iteration 971, loss = 1.15105412\n",
      "Iteration 972, loss = 1.15091249\n",
      "Iteration 973, loss = 1.15077075\n",
      "Iteration 974, loss = 1.15063304\n",
      "Iteration 975, loss = 1.15049487\n",
      "Iteration 976, loss = 1.15035549\n",
      "Iteration 977, loss = 1.15021507\n",
      "Iteration 978, loss = 1.15007471\n",
      "Iteration 979, loss = 1.14993752\n",
      "Iteration 980, loss = 1.14979999\n",
      "Iteration 981, loss = 1.14966220\n",
      "Iteration 982, loss = 1.14952420\n",
      "Iteration 983, loss = 1.14938730\n",
      "Iteration 984, loss = 1.14925112\n",
      "Iteration 985, loss = 1.14911383\n",
      "Iteration 986, loss = 1.14897919\n",
      "Iteration 987, loss = 1.14884437\n",
      "Iteration 988, loss = 1.14870922\n",
      "Iteration 989, loss = 1.14857381\n",
      "Iteration 990, loss = 1.14843819\n",
      "Iteration 991, loss = 1.14830401\n",
      "Iteration 992, loss = 1.14817070\n",
      "Iteration 993, loss = 1.14803626\n",
      "Iteration 994, loss = 1.14790287\n",
      "Iteration 995, loss = 1.14777050\n",
      "Iteration 996, loss = 1.14763779\n",
      "Iteration 997, loss = 1.14750483\n",
      "Iteration 998, loss = 1.14737238\n",
      "Iteration 999, loss = 1.14724081\n",
      "Iteration 1000, loss = 1.14710918\n",
      "Iteration 1, loss = 1.54064569\n",
      "Iteration 2, loss = 1.53981524\n",
      "Iteration 3, loss = 1.53898573\n",
      "Iteration 4, loss = 1.53815718\n",
      "Iteration 5, loss = 1.53732960\n",
      "Iteration 6, loss = 1.53650301\n",
      "Iteration 7, loss = 1.53567742\n",
      "Iteration 8, loss = 1.53485285\n",
      "Iteration 9, loss = 1.53402931\n",
      "Iteration 10, loss = 1.53320682\n",
      "Iteration 11, loss = 1.53238540\n",
      "Iteration 12, loss = 1.53156506\n",
      "Iteration 13, loss = 1.53074581\n",
      "Iteration 14, loss = 1.52992767\n",
      "Iteration 15, loss = 1.52911064\n",
      "Iteration 16, loss = 1.52829475\n",
      "Iteration 17, loss = 1.52748001\n",
      "Iteration 18, loss = 1.52666642\n",
      "Iteration 19, loss = 1.52585400\n",
      "Iteration 20, loss = 1.52504276\n",
      "Iteration 21, loss = 1.52423272\n",
      "Iteration 22, loss = 1.52342387\n",
      "Iteration 23, loss = 1.52261624\n",
      "Iteration 24, loss = 1.52180982\n",
      "Iteration 25, loss = 1.52100463\n",
      "Iteration 26, loss = 1.52020068\n",
      "Iteration 27, loss = 1.51939798\n",
      "Iteration 28, loss = 1.51859653\n",
      "Iteration 29, loss = 1.51779633\n",
      "Iteration 30, loss = 1.51699741\n",
      "Iteration 31, loss = 1.51619975\n",
      "Iteration 32, loss = 1.51540338\n",
      "Iteration 33, loss = 1.51460828\n",
      "Iteration 34, loss = 1.51381448\n",
      "Iteration 35, loss = 1.51302197\n",
      "Iteration 36, loss = 1.51223076\n",
      "Iteration 37, loss = 1.51144085\n",
      "Iteration 38, loss = 1.51065224\n",
      "Iteration 39, loss = 1.50986494\n",
      "Iteration 40, loss = 1.50907896\n",
      "Iteration 41, loss = 1.50829429\n",
      "Iteration 42, loss = 1.50751094\n",
      "Iteration 43, loss = 1.50672890\n",
      "Iteration 44, loss = 1.50594819\n",
      "Iteration 45, loss = 1.50516881\n",
      "Iteration 46, loss = 1.50439075\n",
      "Iteration 47, loss = 1.50361401\n",
      "Iteration 48, loss = 1.50283861\n",
      "Iteration 49, loss = 1.50206453\n",
      "Iteration 50, loss = 1.50129179\n",
      "Iteration 51, loss = 1.50052038\n",
      "Iteration 52, loss = 1.49975030\n",
      "Iteration 53, loss = 1.49898156\n",
      "Iteration 54, loss = 1.49821414\n",
      "Iteration 55, loss = 1.49744807\n",
      "Iteration 56, loss = 1.49668332\n",
      "Iteration 57, loss = 1.49591991\n",
      "Iteration 58, loss = 1.49515784\n",
      "Iteration 59, loss = 1.49439710\n",
      "Iteration 60, loss = 1.49363769\n",
      "Iteration 61, loss = 1.49287962\n",
      "Iteration 62, loss = 1.49212288\n",
      "Iteration 63, loss = 1.49136747\n",
      "Iteration 64, loss = 1.49061340\n",
      "Iteration 65, loss = 1.48986066\n",
      "Iteration 66, loss = 1.48910926\n",
      "Iteration 67, loss = 1.48835918\n",
      "Iteration 68, loss = 1.48761044\n",
      "Iteration 69, loss = 1.48686302\n",
      "Iteration 70, loss = 1.48611694\n",
      "Iteration 71, loss = 1.48537218\n",
      "Iteration 72, loss = 1.48462875\n",
      "Iteration 73, loss = 1.48388665\n",
      "Iteration 74, loss = 1.48314588\n",
      "Iteration 75, loss = 1.48240643\n",
      "Iteration 76, loss = 1.48166831\n",
      "Iteration 77, loss = 1.48093151\n",
      "Iteration 78, loss = 1.48019603\n",
      "Iteration 79, loss = 1.47946188\n",
      "Iteration 80, loss = 1.47872904\n",
      "Iteration 81, loss = 1.47799753\n",
      "Iteration 82, loss = 1.47726733\n",
      "Iteration 83, loss = 1.47653846\n",
      "Iteration 84, loss = 1.47581090\n",
      "Iteration 85, loss = 1.47508465\n",
      "Iteration 86, loss = 1.47435972\n",
      "Iteration 87, loss = 1.47363611\n",
      "Iteration 88, loss = 1.47291380\n",
      "Iteration 89, loss = 1.47219281\n",
      "Iteration 90, loss = 1.47147313\n",
      "Iteration 91, loss = 1.47075476\n",
      "Iteration 92, loss = 1.47003770\n",
      "Iteration 93, loss = 1.46932194\n",
      "Iteration 94, loss = 1.46860749\n",
      "Iteration 95, loss = 1.46789434\n",
      "Iteration 96, loss = 1.46718250\n",
      "Iteration 97, loss = 1.46647196\n",
      "Iteration 98, loss = 1.46576272\n",
      "Iteration 99, loss = 1.46505478\n",
      "Iteration 100, loss = 1.46434814\n",
      "Iteration 101, loss = 1.46364279\n",
      "Iteration 102, loss = 1.46293875\n",
      "Iteration 103, loss = 1.46223599\n",
      "Iteration 104, loss = 1.46153453\n",
      "Iteration 105, loss = 1.46083437\n",
      "Iteration 106, loss = 1.46013549\n",
      "Iteration 107, loss = 1.45943791\n",
      "Iteration 108, loss = 1.45874161\n",
      "Iteration 109, loss = 1.45804660\n",
      "Iteration 110, loss = 1.45735288\n",
      "Iteration 111, loss = 1.45666044\n",
      "Iteration 112, loss = 1.45596928\n",
      "Iteration 113, loss = 1.45527941\n",
      "Iteration 114, loss = 1.45459082\n",
      "Iteration 115, loss = 1.45390351\n",
      "Iteration 116, loss = 1.45321748\n",
      "Iteration 117, loss = 1.45253272\n",
      "Iteration 118, loss = 1.45184924\n",
      "Iteration 119, loss = 1.45116704\n",
      "Iteration 120, loss = 1.45048610\n",
      "Iteration 121, loss = 1.44980644\n",
      "Iteration 122, loss = 1.44912805\n",
      "Iteration 123, loss = 1.44845093\n",
      "Iteration 124, loss = 1.44777508\n",
      "Iteration 125, loss = 1.44710049\n",
      "Iteration 126, loss = 1.44642717\n",
      "Iteration 127, loss = 1.44575512\n",
      "Iteration 128, loss = 1.44508432\n",
      "Iteration 129, loss = 1.44441479\n",
      "Iteration 130, loss = 1.44374651\n",
      "Iteration 131, loss = 1.44307950\n",
      "Iteration 132, loss = 1.44241374\n",
      "Iteration 133, loss = 1.44174924\n",
      "Iteration 134, loss = 1.44108599\n",
      "Iteration 135, loss = 1.44042399\n",
      "Iteration 136, loss = 1.43976325\n",
      "Iteration 137, loss = 1.43910375\n",
      "Iteration 138, loss = 1.43844551\n",
      "Iteration 139, loss = 1.43778851\n",
      "Iteration 140, loss = 1.43713276\n",
      "Iteration 141, loss = 1.43647825\n",
      "Iteration 142, loss = 1.43582498\n",
      "Iteration 143, loss = 1.43517296\n",
      "Iteration 144, loss = 1.43452218\n",
      "Iteration 145, loss = 1.43387263\n",
      "Iteration 146, loss = 1.43322433\n",
      "Iteration 147, loss = 1.43257726\n",
      "Iteration 148, loss = 1.43193142\n",
      "Iteration 149, loss = 1.43128682\n",
      "Iteration 150, loss = 1.43064345\n",
      "Iteration 151, loss = 1.43000131\n",
      "Iteration 152, loss = 1.42936039\n",
      "Iteration 153, loss = 1.42872071\n",
      "Iteration 154, loss = 1.42808225\n",
      "Iteration 155, loss = 1.42744502\n",
      "Iteration 156, loss = 1.42680900\n",
      "Iteration 157, loss = 1.42617421\n",
      "Iteration 158, loss = 1.42554064\n",
      "Iteration 159, loss = 1.42490829\n",
      "Iteration 160, loss = 1.42427716\n",
      "Iteration 161, loss = 1.42364724\n",
      "Iteration 162, loss = 1.42301853\n",
      "Iteration 163, loss = 1.42239104\n",
      "Iteration 164, loss = 1.42176476\n",
      "Iteration 165, loss = 1.42113969\n",
      "Iteration 166, loss = 1.42051583\n",
      "Iteration 167, loss = 1.41989317\n",
      "Iteration 168, loss = 1.41927172\n",
      "Iteration 169, loss = 1.41865147\n",
      "Iteration 170, loss = 1.41803243\n",
      "Iteration 171, loss = 1.41741458\n",
      "Iteration 172, loss = 1.41679794\n",
      "Iteration 173, loss = 1.41618249\n",
      "Iteration 174, loss = 1.41556824\n",
      "Iteration 175, loss = 1.41495518\n",
      "Iteration 176, loss = 1.41434332\n",
      "Iteration 177, loss = 1.41373265\n",
      "Iteration 178, loss = 1.41312317\n",
      "Iteration 179, loss = 1.41251487\n",
      "Iteration 180, loss = 1.41190777\n",
      "Iteration 181, loss = 1.41130185\n",
      "Iteration 182, loss = 1.41069711\n",
      "Iteration 183, loss = 1.41009356\n",
      "Iteration 184, loss = 1.40949119\n",
      "Iteration 185, loss = 1.40888999\n",
      "Iteration 186, loss = 1.40828998\n",
      "Iteration 187, loss = 1.40769114\n",
      "Iteration 188, loss = 1.40709347\n",
      "Iteration 189, loss = 1.40649698\n",
      "Iteration 190, loss = 1.40590166\n",
      "Iteration 191, loss = 1.40530751\n",
      "Iteration 192, loss = 1.40471453\n",
      "Iteration 193, loss = 1.40412272\n",
      "Iteration 194, loss = 1.40353207\n",
      "Iteration 195, loss = 1.40294258\n",
      "Iteration 196, loss = 1.40235426\n",
      "Iteration 197, loss = 1.40176710\n",
      "Iteration 198, loss = 1.40118109\n",
      "Iteration 199, loss = 1.40059625\n",
      "Iteration 200, loss = 1.40001256\n",
      "Iteration 201, loss = 1.39943002\n",
      "Iteration 202, loss = 1.39884864\n",
      "Iteration 203, loss = 1.39826841\n",
      "Iteration 204, loss = 1.39768933\n",
      "Iteration 205, loss = 1.39711140\n",
      "Iteration 206, loss = 1.39653461\n",
      "Iteration 207, loss = 1.39595897\n",
      "Iteration 208, loss = 1.39538447\n",
      "Iteration 209, loss = 1.39481112\n",
      "Iteration 210, loss = 1.39423890\n",
      "Iteration 211, loss = 1.39366782\n",
      "Iteration 212, loss = 1.39309788\n",
      "Iteration 213, loss = 1.39252907\n",
      "Iteration 214, loss = 1.39196140\n",
      "Iteration 215, loss = 1.39139486\n",
      "Iteration 216, loss = 1.39082945\n",
      "Iteration 217, loss = 1.39026517\n",
      "Iteration 218, loss = 1.38970202\n",
      "Iteration 219, loss = 1.38914000\n",
      "Iteration 220, loss = 1.38857909\n",
      "Iteration 221, loss = 1.38801931\n",
      "Iteration 222, loss = 1.38746065\n",
      "Iteration 223, loss = 1.38690311\n",
      "Iteration 224, loss = 1.38634669\n",
      "Iteration 225, loss = 1.38579138\n",
      "Iteration 226, loss = 1.38523719\n",
      "Iteration 227, loss = 1.38468411\n",
      "Iteration 228, loss = 1.38413215\n",
      "Iteration 229, loss = 1.38358129\n",
      "Iteration 230, loss = 1.38303154\n",
      "Iteration 231, loss = 1.38248289\n",
      "Iteration 232, loss = 1.38193535\n",
      "Iteration 233, loss = 1.38138892\n",
      "Iteration 234, loss = 1.38084358\n",
      "Iteration 235, loss = 1.38029935\n",
      "Iteration 236, loss = 1.37975621\n",
      "Iteration 237, loss = 1.37921417\n",
      "Iteration 238, loss = 1.37867323\n",
      "Iteration 239, loss = 1.37813338\n",
      "Iteration 240, loss = 1.37759462\n",
      "Iteration 241, loss = 1.37705695\n",
      "Iteration 242, loss = 1.37652037\n",
      "Iteration 243, loss = 1.37598487\n",
      "Iteration 244, loss = 1.37545046\n",
      "Iteration 245, loss = 1.37491714\n",
      "Iteration 246, loss = 1.37438489\n",
      "Iteration 247, loss = 1.37385373\n",
      "Iteration 248, loss = 1.37332364\n",
      "Iteration 249, loss = 1.37279464\n",
      "Iteration 250, loss = 1.37226670\n",
      "Iteration 251, loss = 1.37173984\n",
      "Iteration 252, loss = 1.37121406\n",
      "Iteration 253, loss = 1.37068934\n",
      "Iteration 254, loss = 1.37016569\n",
      "Iteration 255, loss = 1.36964311\n",
      "Iteration 256, loss = 1.36912160\n",
      "Iteration 257, loss = 1.36860115\n",
      "Iteration 258, loss = 1.36808176\n",
      "Iteration 259, loss = 1.36756343\n",
      "Iteration 260, loss = 1.36704616\n",
      "Iteration 261, loss = 1.36652995\n",
      "Iteration 262, loss = 1.36601479\n",
      "Iteration 263, loss = 1.36550069\n",
      "Iteration 264, loss = 1.36498764\n",
      "Iteration 265, loss = 1.36447564\n",
      "Iteration 266, loss = 1.36396469\n",
      "Iteration 267, loss = 1.36345479\n",
      "Iteration 268, loss = 1.36294593\n",
      "Iteration 269, loss = 1.36243812\n",
      "Iteration 270, loss = 1.36193134\n",
      "Iteration 271, loss = 1.36142562\n",
      "Iteration 272, loss = 1.36092092\n",
      "Iteration 273, loss = 1.36041727\n",
      "Iteration 274, loss = 1.35991465\n",
      "Iteration 275, loss = 1.35941307\n",
      "Iteration 276, loss = 1.35891252\n",
      "Iteration 277, loss = 1.35841300\n",
      "Iteration 278, loss = 1.35791451\n",
      "Iteration 279, loss = 1.35741705\n",
      "Iteration 280, loss = 1.35692061\n",
      "Iteration 281, loss = 1.35642520\n",
      "Iteration 282, loss = 1.35593081\n",
      "Iteration 283, loss = 1.35543745\n",
      "Iteration 284, loss = 1.35494510\n",
      "Iteration 285, loss = 1.35445377\n",
      "Iteration 286, loss = 1.35396345\n",
      "Iteration 287, loss = 1.35347415\n",
      "Iteration 288, loss = 1.35298586\n",
      "Iteration 289, loss = 1.35249859\n",
      "Iteration 290, loss = 1.35201232\n",
      "Iteration 291, loss = 1.35152706\n",
      "Iteration 292, loss = 1.35104281\n",
      "Iteration 293, loss = 1.35055956\n",
      "Iteration 294, loss = 1.35007732\n",
      "Iteration 295, loss = 1.34959608\n",
      "Iteration 296, loss = 1.34911583\n",
      "Iteration 297, loss = 1.34863659\n",
      "Iteration 298, loss = 1.34815834\n",
      "Iteration 299, loss = 1.34768109\n",
      "Iteration 300, loss = 1.34720482\n",
      "Iteration 301, loss = 1.34672956\n",
      "Iteration 302, loss = 1.34625528\n",
      "Iteration 303, loss = 1.34578199\n",
      "Iteration 304, loss = 1.34530968\n",
      "Iteration 305, loss = 1.34483836\n",
      "Iteration 306, loss = 1.34436803\n",
      "Iteration 307, loss = 1.34389867\n",
      "Iteration 308, loss = 1.34343030\n",
      "Iteration 309, loss = 1.34296290\n",
      "Iteration 310, loss = 1.34249648\n",
      "Iteration 311, loss = 1.34203104\n",
      "Iteration 312, loss = 1.34156657\n",
      "Iteration 313, loss = 1.34110307\n",
      "Iteration 314, loss = 1.34064055\n",
      "Iteration 315, loss = 1.34017899\n",
      "Iteration 316, loss = 1.33971840\n",
      "Iteration 317, loss = 1.33925877\n",
      "Iteration 318, loss = 1.33880011\n",
      "Iteration 319, loss = 1.33834241\n",
      "Iteration 320, loss = 1.33788567\n",
      "Iteration 321, loss = 1.33742989\n",
      "Iteration 322, loss = 1.33697506\n",
      "Iteration 323, loss = 1.33652120\n",
      "Iteration 324, loss = 1.33606828\n",
      "Iteration 325, loss = 1.33561632\n",
      "Iteration 326, loss = 1.33516531\n",
      "Iteration 327, loss = 1.33471525\n",
      "Iteration 328, loss = 1.33426614\n",
      "Iteration 329, loss = 1.33381797\n",
      "Iteration 330, loss = 1.33337075\n",
      "Iteration 331, loss = 1.33292447\n",
      "Iteration 332, loss = 1.33247914\n",
      "Iteration 333, loss = 1.33203474\n",
      "Iteration 334, loss = 1.33159128\n",
      "Iteration 335, loss = 1.33114875\n",
      "Iteration 336, loss = 1.33070716\n",
      "Iteration 337, loss = 1.33026651\n",
      "Iteration 338, loss = 1.32982679\n",
      "Iteration 339, loss = 1.32938799\n",
      "Iteration 340, loss = 1.32895013\n",
      "Iteration 341, loss = 1.32851319\n",
      "Iteration 342, loss = 1.32807717\n",
      "Iteration 343, loss = 1.32764208\n",
      "Iteration 344, loss = 1.32720792\n",
      "Iteration 345, loss = 1.32677467\n",
      "Iteration 346, loss = 1.32634234\n",
      "Iteration 347, loss = 1.32591093\n",
      "Iteration 348, loss = 1.32548044\n",
      "Iteration 349, loss = 1.32505086\n",
      "Iteration 350, loss = 1.32462219\n",
      "Iteration 351, loss = 1.32419443\n",
      "Iteration 352, loss = 1.32376758\n",
      "Iteration 353, loss = 1.32334164\n",
      "Iteration 354, loss = 1.32291661\n",
      "Iteration 355, loss = 1.32249248\n",
      "Iteration 356, loss = 1.32206925\n",
      "Iteration 357, loss = 1.32164693\n",
      "Iteration 358, loss = 1.32122551\n",
      "Iteration 359, loss = 1.32080498\n",
      "Iteration 360, loss = 1.32038535\n",
      "Iteration 361, loss = 1.31996662\n",
      "Iteration 362, loss = 1.31954878\n",
      "Iteration 363, loss = 1.31913183\n",
      "Iteration 364, loss = 1.31871577\n",
      "Iteration 365, loss = 1.31830061\n",
      "Iteration 366, loss = 1.31788632\n",
      "Iteration 367, loss = 1.31747293\n",
      "Iteration 368, loss = 1.31706042\n",
      "Iteration 369, loss = 1.31664879\n",
      "Iteration 370, loss = 1.31623805\n",
      "Iteration 371, loss = 1.31582818\n",
      "Iteration 372, loss = 1.31541919\n",
      "Iteration 373, loss = 1.31501108\n",
      "Iteration 374, loss = 1.31460385\n",
      "Iteration 375, loss = 1.31419748\n",
      "Iteration 376, loss = 1.31379199\n",
      "Iteration 377, loss = 1.31338737\n",
      "Iteration 378, loss = 1.31298362\n",
      "Iteration 379, loss = 1.31258074\n",
      "Iteration 380, loss = 1.31217872\n",
      "Iteration 381, loss = 1.31177757\n",
      "Iteration 382, loss = 1.31137728\n",
      "Iteration 383, loss = 1.31097785\n",
      "Iteration 384, loss = 1.31057928\n",
      "Iteration 385, loss = 1.31018157\n",
      "Iteration 386, loss = 1.30978472\n",
      "Iteration 387, loss = 1.30938872\n",
      "Iteration 388, loss = 1.30899358\n",
      "Iteration 389, loss = 1.30859929\n",
      "Iteration 390, loss = 1.30820584\n",
      "Iteration 391, loss = 1.30781325\n",
      "Iteration 392, loss = 1.30742151\n",
      "Iteration 393, loss = 1.30703061\n",
      "Iteration 394, loss = 1.30664056\n",
      "Iteration 395, loss = 1.30625135\n",
      "Iteration 396, loss = 1.30586298\n",
      "Iteration 397, loss = 1.30547545\n",
      "Iteration 398, loss = 1.30508876\n",
      "Iteration 399, loss = 1.30470291\n",
      "Iteration 400, loss = 1.30431790\n",
      "Iteration 401, loss = 1.30393372\n",
      "Iteration 402, loss = 1.30355037\n",
      "Iteration 403, loss = 1.30316785\n",
      "Iteration 404, loss = 1.30278616\n",
      "Iteration 405, loss = 1.30240530\n",
      "Iteration 406, loss = 1.30202527\n",
      "Iteration 407, loss = 1.30164607\n",
      "Iteration 408, loss = 1.30126768\n",
      "Iteration 409, loss = 1.30089012\n",
      "Iteration 410, loss = 1.30051339\n",
      "Iteration 411, loss = 1.30013747\n",
      "Iteration 412, loss = 1.29976237\n",
      "Iteration 413, loss = 1.29938808\n",
      "Iteration 414, loss = 1.29901461\n",
      "Iteration 415, loss = 1.29864196\n",
      "Iteration 416, loss = 1.29827012\n",
      "Iteration 417, loss = 1.29789908\n",
      "Iteration 418, loss = 1.29752886\n",
      "Iteration 419, loss = 1.29715945\n",
      "Iteration 420, loss = 1.29679084\n",
      "Iteration 421, loss = 1.29642304\n",
      "Iteration 422, loss = 1.29605604\n",
      "Iteration 423, loss = 1.29568984\n",
      "Iteration 424, loss = 1.29532444\n",
      "Iteration 425, loss = 1.29495985\n",
      "Iteration 426, loss = 1.29459605\n",
      "Iteration 427, loss = 1.29423305\n",
      "Iteration 428, loss = 1.29387084\n",
      "Iteration 429, loss = 1.29350943\n",
      "Iteration 430, loss = 1.29314880\n",
      "Iteration 431, loss = 1.29278897\n",
      "Iteration 432, loss = 1.29242993\n",
      "Iteration 433, loss = 1.29207168\n",
      "Iteration 434, loss = 1.29171421\n",
      "Iteration 435, loss = 1.29135753\n",
      "Iteration 436, loss = 1.29100163\n",
      "Iteration 437, loss = 1.29064652\n",
      "Iteration 438, loss = 1.29029219\n",
      "Iteration 439, loss = 1.28993863\n",
      "Iteration 440, loss = 1.28958585\n",
      "Iteration 441, loss = 1.28923385\n",
      "Iteration 442, loss = 1.28888263\n",
      "Iteration 443, loss = 1.28853218\n",
      "Iteration 444, loss = 1.28818250\n",
      "Iteration 445, loss = 1.28783360\n",
      "Iteration 446, loss = 1.28748546\n",
      "Iteration 447, loss = 1.28713809\n",
      "Iteration 448, loss = 1.28679149\n",
      "Iteration 449, loss = 1.28644565\n",
      "Iteration 450, loss = 1.28610058\n",
      "Iteration 451, loss = 1.28575627\n",
      "Iteration 452, loss = 1.28541273\n",
      "Iteration 453, loss = 1.28506994\n",
      "Iteration 454, loss = 1.28472791\n",
      "Iteration 455, loss = 1.28438664\n",
      "Iteration 456, loss = 1.28404613\n",
      "Iteration 457, loss = 1.28370637\n",
      "Iteration 458, loss = 1.28336736\n",
      "Iteration 459, loss = 1.28302911\n",
      "Iteration 460, loss = 1.28269161\n",
      "Iteration 461, loss = 1.28235485\n",
      "Iteration 462, loss = 1.28201885\n",
      "Iteration 463, loss = 1.28168359\n",
      "Iteration 464, loss = 1.28134907\n",
      "Iteration 465, loss = 1.28101530\n",
      "Iteration 466, loss = 1.28068227\n",
      "Iteration 467, loss = 1.28034999\n",
      "Iteration 468, loss = 1.28001844\n",
      "Iteration 469, loss = 1.27968764\n",
      "Iteration 470, loss = 1.27935756\n",
      "Iteration 471, loss = 1.27902823\n",
      "Iteration 472, loss = 1.27869963\n",
      "Iteration 473, loss = 1.27837176\n",
      "Iteration 474, loss = 1.27804463\n",
      "Iteration 475, loss = 1.27771823\n",
      "Iteration 476, loss = 1.27739255\n",
      "Iteration 477, loss = 1.27706761\n",
      "Iteration 478, loss = 1.27674339\n",
      "Iteration 479, loss = 1.27641989\n",
      "Iteration 480, loss = 1.27609712\n",
      "Iteration 481, loss = 1.27577508\n",
      "Iteration 482, loss = 1.27545375\n",
      "Iteration 483, loss = 1.27513315\n",
      "Iteration 484, loss = 1.27481326\n",
      "Iteration 485, loss = 1.27449409\n",
      "Iteration 486, loss = 1.27417564\n",
      "Iteration 487, loss = 1.27385790\n",
      "Iteration 488, loss = 1.27354088\n",
      "Iteration 489, loss = 1.27322457\n",
      "Iteration 490, loss = 1.27290897\n",
      "Iteration 491, loss = 1.27259408\n",
      "Iteration 492, loss = 1.27227990\n",
      "Iteration 493, loss = 1.27196642\n",
      "Iteration 494, loss = 1.27165365\n",
      "Iteration 495, loss = 1.27134159\n",
      "Iteration 496, loss = 1.27103023\n",
      "Iteration 497, loss = 1.27071957\n",
      "Iteration 498, loss = 1.27040962\n",
      "Iteration 499, loss = 1.27010036\n",
      "Iteration 500, loss = 1.26979180\n",
      "Iteration 501, loss = 1.26948394\n",
      "Iteration 502, loss = 1.26917678\n",
      "Iteration 503, loss = 1.26887031\n",
      "Iteration 504, loss = 1.26856453\n",
      "Iteration 505, loss = 1.26825945\n",
      "Iteration 506, loss = 1.26795506\n",
      "Iteration 507, loss = 1.26765135\n",
      "Iteration 508, loss = 1.26734834\n",
      "Iteration 509, loss = 1.26704601\n",
      "Iteration 510, loss = 1.26674437\n",
      "Iteration 511, loss = 1.26644341\n",
      "Iteration 512, loss = 1.26614314\n",
      "Iteration 513, loss = 1.26584355\n",
      "Iteration 514, loss = 1.26554464\n",
      "Iteration 515, loss = 1.26524641\n",
      "Iteration 516, loss = 1.26494885\n",
      "Iteration 517, loss = 1.26465198\n",
      "Iteration 518, loss = 1.26435578\n",
      "Iteration 519, loss = 1.26406026\n",
      "Iteration 520, loss = 1.26376541\n",
      "Iteration 521, loss = 1.26347123\n",
      "Iteration 522, loss = 1.26317773\n",
      "Iteration 523, loss = 1.26288489\n",
      "Iteration 524, loss = 1.26259272\n",
      "Iteration 525, loss = 1.26230123\n",
      "Iteration 526, loss = 1.26201039\n",
      "Iteration 527, loss = 1.26172023\n",
      "Iteration 528, loss = 1.26143072\n",
      "Iteration 529, loss = 1.26114188\n",
      "Iteration 530, loss = 1.26085370\n",
      "Iteration 531, loss = 1.26056619\n",
      "Iteration 532, loss = 1.26027933\n",
      "Iteration 533, loss = 1.25999313\n",
      "Iteration 534, loss = 1.25970758\n",
      "Iteration 535, loss = 1.25942269\n",
      "Iteration 536, loss = 1.25913846\n",
      "Iteration 537, loss = 1.25885488\n",
      "Iteration 538, loss = 1.25857195\n",
      "Iteration 539, loss = 1.25828968\n",
      "Iteration 540, loss = 1.25800805\n",
      "Iteration 541, loss = 1.25772707\n",
      "Iteration 542, loss = 1.25744674\n",
      "Iteration 543, loss = 1.25716705\n",
      "Iteration 544, loss = 1.25688801\n",
      "Iteration 545, loss = 1.25660962\n",
      "Iteration 546, loss = 1.25633187\n",
      "Iteration 547, loss = 1.25605475\n",
      "Iteration 548, loss = 1.25577828\n",
      "Iteration 549, loss = 1.25550245\n",
      "Iteration 550, loss = 1.25522726\n",
      "Iteration 551, loss = 1.25495270\n",
      "Iteration 552, loss = 1.25467878\n",
      "Iteration 553, loss = 1.25440550\n",
      "Iteration 554, loss = 1.25413284\n",
      "Iteration 555, loss = 1.25386082\n",
      "Iteration 556, loss = 1.25358943\n",
      "Iteration 557, loss = 1.25331868\n",
      "Iteration 558, loss = 1.25304855\n",
      "Iteration 559, loss = 1.25277905\n",
      "Iteration 560, loss = 1.25251017\n",
      "Iteration 561, loss = 1.25224192\n",
      "Iteration 562, loss = 1.25197430\n",
      "Iteration 563, loss = 1.25170729\n",
      "Iteration 564, loss = 1.25144091\n",
      "Iteration 565, loss = 1.25117516\n",
      "Iteration 566, loss = 1.25091002\n",
      "Iteration 567, loss = 1.25064550\n",
      "Iteration 568, loss = 1.25038160\n",
      "Iteration 569, loss = 1.25011831\n",
      "Iteration 570, loss = 1.24985564\n",
      "Iteration 571, loss = 1.24959358\n",
      "Iteration 572, loss = 1.24933214\n",
      "Iteration 573, loss = 1.24907131\n",
      "Iteration 574, loss = 1.24881109\n",
      "Iteration 575, loss = 1.24855148\n",
      "Iteration 576, loss = 1.24829248\n",
      "Iteration 577, loss = 1.24803409\n",
      "Iteration 578, loss = 1.24777630\n",
      "Iteration 579, loss = 1.24751912\n",
      "Iteration 580, loss = 1.24726254\n",
      "Iteration 581, loss = 1.24700657\n",
      "Iteration 582, loss = 1.24675119\n",
      "Iteration 583, loss = 1.24649642\n",
      "Iteration 584, loss = 1.24624225\n",
      "Iteration 585, loss = 1.24598868\n",
      "Iteration 586, loss = 1.24573571\n",
      "Iteration 587, loss = 1.24548333\n",
      "Iteration 588, loss = 1.24523155\n",
      "Iteration 589, loss = 1.24498036\n",
      "Iteration 590, loss = 1.24472976\n",
      "Iteration 591, loss = 1.24447976\n",
      "Iteration 592, loss = 1.24423035\n",
      "Iteration 593, loss = 1.24398153\n",
      "Iteration 594, loss = 1.24373330\n",
      "Iteration 595, loss = 1.24348566\n",
      "Iteration 596, loss = 1.24323860\n",
      "Iteration 597, loss = 1.24299213\n",
      "Iteration 598, loss = 1.24274624\n",
      "Iteration 599, loss = 1.24250094\n",
      "Iteration 600, loss = 1.24225622\n",
      "Iteration 601, loss = 1.24201208\n",
      "Iteration 602, loss = 1.24176852\n",
      "Iteration 603, loss = 1.24152555\n",
      "Iteration 604, loss = 1.24128315\n",
      "Iteration 605, loss = 1.24104132\n",
      "Iteration 606, loss = 1.24080008\n",
      "Iteration 607, loss = 1.24055941\n",
      "Iteration 608, loss = 1.24031931\n",
      "Iteration 609, loss = 1.24007979\n",
      "Iteration 610, loss = 1.23984084\n",
      "Iteration 611, loss = 1.23960246\n",
      "Iteration 612, loss = 1.23936465\n",
      "Iteration 613, loss = 1.23912740\n",
      "Iteration 614, loss = 1.23889073\n",
      "Iteration 615, loss = 1.23865462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\projeto1\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 616, loss = 1.23841908\n",
      "Iteration 617, loss = 1.23818410\n",
      "Iteration 618, loss = 1.23794969\n",
      "Iteration 619, loss = 1.23771584\n",
      "Iteration 620, loss = 1.23748255\n",
      "Iteration 621, loss = 1.23724983\n",
      "Iteration 622, loss = 1.23701766\n",
      "Iteration 623, loss = 1.23678605\n",
      "Iteration 624, loss = 1.23655499\n",
      "Iteration 625, loss = 1.23632450\n",
      "Iteration 626, loss = 1.23609456\n",
      "Iteration 627, loss = 1.23586517\n",
      "Iteration 628, loss = 1.23563634\n",
      "Iteration 629, loss = 1.23540806\n",
      "Iteration 630, loss = 1.23518033\n",
      "Iteration 631, loss = 1.23495315\n",
      "Iteration 632, loss = 1.23472652\n",
      "Iteration 633, loss = 1.23450044\n",
      "Iteration 634, loss = 1.23427491\n",
      "Iteration 635, loss = 1.23404992\n",
      "Iteration 636, loss = 1.23382548\n",
      "Iteration 637, loss = 1.23360158\n",
      "Iteration 638, loss = 1.23337822\n",
      "Iteration 639, loss = 1.23315541\n",
      "Iteration 640, loss = 1.23293314\n",
      "Iteration 641, loss = 1.23271141\n",
      "Iteration 642, loss = 1.23249021\n",
      "Iteration 643, loss = 1.23226956\n",
      "Iteration 644, loss = 1.23204944\n",
      "Iteration 645, loss = 1.23182986\n",
      "Iteration 646, loss = 1.23161082\n",
      "Iteration 647, loss = 1.23139231\n",
      "Iteration 648, loss = 1.23117433\n",
      "Iteration 649, loss = 1.23095688\n",
      "Iteration 650, loss = 1.23073997\n",
      "Iteration 651, loss = 1.23052358\n",
      "Iteration 652, loss = 1.23030773\n",
      "Iteration 653, loss = 1.23009240\n",
      "Iteration 654, loss = 1.22987760\n",
      "Iteration 655, loss = 1.22966332\n",
      "Iteration 656, loss = 1.22944958\n",
      "Iteration 657, loss = 1.22923635\n",
      "Iteration 658, loss = 1.22902365\n",
      "Iteration 659, loss = 1.22881147\n",
      "Iteration 660, loss = 1.22859981\n",
      "Iteration 661, loss = 1.22838868\n",
      "Iteration 662, loss = 1.22817806\n",
      "Iteration 663, loss = 1.22796796\n",
      "Iteration 664, loss = 1.22775838\n",
      "Iteration 665, loss = 1.22754931\n",
      "Iteration 666, loss = 1.22734076\n",
      "Iteration 667, loss = 1.22713272\n",
      "Iteration 668, loss = 1.22692520\n",
      "Iteration 669, loss = 1.22671819\n",
      "Iteration 670, loss = 1.22651169\n",
      "Iteration 671, loss = 1.22630571\n",
      "Iteration 672, loss = 1.22610023\n",
      "Iteration 673, loss = 1.22589526\n",
      "Iteration 674, loss = 1.22569080\n",
      "Iteration 675, loss = 1.22548684\n",
      "Iteration 676, loss = 1.22528339\n",
      "Iteration 677, loss = 1.22508045\n",
      "Iteration 678, loss = 1.22487801\n",
      "Iteration 679, loss = 1.22467607\n",
      "Iteration 680, loss = 1.22447464\n",
      "Iteration 681, loss = 1.22427370\n",
      "Iteration 682, loss = 1.22407327\n",
      "Iteration 683, loss = 1.22387333\n",
      "Iteration 684, loss = 1.22367390\n",
      "Iteration 685, loss = 1.22347496\n",
      "Iteration 686, loss = 1.22327651\n",
      "Iteration 687, loss = 1.22307856\n",
      "Iteration 688, loss = 1.22288111\n",
      "Iteration 689, loss = 1.22268415\n",
      "Iteration 690, loss = 1.22248768\n",
      "Iteration 691, loss = 1.22229170\n",
      "Iteration 692, loss = 1.22209622\n",
      "Iteration 693, loss = 1.22190122\n",
      "Iteration 694, loss = 1.22170671\n",
      "Iteration 695, loss = 1.22151269\n",
      "Iteration 696, loss = 1.22131916\n",
      "Iteration 697, loss = 1.22112611\n",
      "Iteration 698, loss = 1.22093355\n",
      "Iteration 699, loss = 1.22074147\n",
      "Iteration 700, loss = 1.22054988\n",
      "Iteration 701, loss = 1.22035876\n",
      "Iteration 702, loss = 1.22016813\n",
      "Iteration 703, loss = 1.21997798\n",
      "Iteration 704, loss = 1.21978831\n",
      "Iteration 705, loss = 1.21959911\n",
      "Iteration 706, loss = 1.21941040\n",
      "Iteration 707, loss = 1.21922216\n",
      "Iteration 708, loss = 1.21903439\n",
      "Iteration 709, loss = 1.21884710\n",
      "Iteration 710, loss = 1.21866029\n",
      "Iteration 711, loss = 1.21847394\n",
      "Iteration 712, loss = 1.21828807\n",
      "Iteration 713, loss = 1.21810267\n",
      "Iteration 714, loss = 1.21791774\n",
      "Iteration 715, loss = 1.21773328\n",
      "Iteration 716, loss = 1.21754929\n",
      "Iteration 717, loss = 1.21736577\n",
      "Iteration 718, loss = 1.21718271\n",
      "Iteration 719, loss = 1.21700011\n",
      "Iteration 720, loss = 1.21681799\n",
      "Iteration 721, loss = 1.21663632\n",
      "Iteration 722, loss = 1.21645512\n",
      "Iteration 723, loss = 1.21627438\n",
      "Iteration 724, loss = 1.21609410\n",
      "Iteration 725, loss = 1.21591428\n",
      "Iteration 726, loss = 1.21573493\n",
      "Iteration 727, loss = 1.21555602\n",
      "Iteration 728, loss = 1.21537758\n",
      "Iteration 729, loss = 1.21519959\n",
      "Iteration 730, loss = 1.21502206\n",
      "Iteration 731, loss = 1.21484499\n",
      "Iteration 732, loss = 1.21466836\n",
      "Iteration 733, loss = 1.21449219\n",
      "Iteration 734, loss = 1.21431648\n",
      "Iteration 735, loss = 1.21414121\n",
      "Iteration 736, loss = 1.21396639\n",
      "Iteration 737, loss = 1.21379203\n",
      "Iteration 738, loss = 1.21361811\n",
      "Iteration 739, loss = 1.21344464\n",
      "Iteration 740, loss = 1.21327161\n",
      "Iteration 741, loss = 1.21309903\n",
      "Iteration 742, loss = 1.21292690\n",
      "Iteration 743, loss = 1.21275521\n",
      "Iteration 744, loss = 1.21258396\n",
      "Iteration 745, loss = 1.21241316\n",
      "Iteration 746, loss = 1.21224280\n",
      "Iteration 747, loss = 1.21207287\n",
      "Iteration 748, loss = 1.21190339\n",
      "Iteration 749, loss = 1.21173435\n",
      "Iteration 750, loss = 1.21156574\n",
      "Iteration 751, loss = 1.21139757\n",
      "Iteration 752, loss = 1.21122984\n",
      "Iteration 753, loss = 1.21106254\n",
      "Iteration 754, loss = 1.21089568\n",
      "Iteration 755, loss = 1.21072925\n",
      "Iteration 756, loss = 1.21056325\n",
      "Iteration 757, loss = 1.21039768\n",
      "Iteration 758, loss = 1.21023255\n",
      "Iteration 759, loss = 1.21006784\n",
      "Iteration 760, loss = 1.20990357\n",
      "Iteration 761, loss = 1.20973972\n",
      "Iteration 762, loss = 1.20957630\n",
      "Iteration 763, loss = 1.20941331\n",
      "Iteration 764, loss = 1.20925074\n",
      "Iteration 765, loss = 1.20908860\n",
      "Iteration 766, loss = 1.20892688\n",
      "Iteration 767, loss = 1.20876558\n",
      "Iteration 768, loss = 1.20860471\n",
      "Iteration 769, loss = 1.20844425\n",
      "Iteration 770, loss = 1.20828422\n",
      "Iteration 771, loss = 1.20812461\n",
      "Iteration 772, loss = 1.20796542\n",
      "Iteration 773, loss = 1.20780664\n",
      "Iteration 774, loss = 1.20764828\n",
      "Iteration 775, loss = 1.20749034\n",
      "Iteration 776, loss = 1.20733281\n",
      "Iteration 777, loss = 1.20717570\n",
      "Iteration 778, loss = 1.20701900\n",
      "Iteration 779, loss = 1.20686271\n",
      "Iteration 780, loss = 1.20670684\n",
      "Iteration 781, loss = 1.20655138\n",
      "Iteration 782, loss = 1.20639632\n",
      "Iteration 783, loss = 1.20624168\n",
      "Iteration 784, loss = 1.20608745\n",
      "Iteration 785, loss = 1.20593362\n",
      "Iteration 786, loss = 1.20578020\n",
      "Iteration 787, loss = 1.20562719\n",
      "Iteration 788, loss = 1.20547458\n",
      "Iteration 789, loss = 1.20532237\n",
      "Iteration 790, loss = 1.20517057\n",
      "Iteration 791, loss = 1.20501917\n",
      "Iteration 792, loss = 1.20486818\n",
      "Iteration 793, loss = 1.20471758\n",
      "Iteration 794, loss = 1.20456739\n",
      "Iteration 795, loss = 1.20441759\n",
      "Iteration 796, loss = 1.20426819\n",
      "Iteration 797, loss = 1.20411919\n",
      "Iteration 798, loss = 1.20397059\n",
      "Iteration 799, loss = 1.20382239\n",
      "Iteration 800, loss = 1.20367457\n",
      "Iteration 801, loss = 1.20352716\n",
      "Iteration 802, loss = 1.20338013\n",
      "Iteration 803, loss = 1.20323350\n",
      "Iteration 804, loss = 1.20308727\n",
      "Iteration 805, loss = 1.20294142\n",
      "Iteration 806, loss = 1.20279596\n",
      "Iteration 807, loss = 1.20265089\n",
      "Iteration 808, loss = 1.20250621\n",
      "Iteration 809, loss = 1.20236192\n",
      "Iteration 810, loss = 1.20221802\n",
      "Iteration 811, loss = 1.20207450\n",
      "Iteration 812, loss = 1.20193137\n",
      "Iteration 813, loss = 1.20178862\n",
      "Iteration 814, loss = 1.20164625\n",
      "Iteration 815, loss = 1.20150427\n",
      "Iteration 816, loss = 1.20136267\n",
      "Iteration 817, loss = 1.20122146\n",
      "Iteration 818, loss = 1.20108062\n",
      "Iteration 819, loss = 1.20094016\n",
      "Iteration 820, loss = 1.20080008\n",
      "Iteration 821, loss = 1.20066038\n",
      "Iteration 822, loss = 1.20052106\n",
      "Iteration 823, loss = 1.20038211\n",
      "Iteration 824, loss = 1.20024354\n",
      "Iteration 825, loss = 1.20010534\n",
      "Iteration 826, loss = 1.19996752\n",
      "Iteration 827, loss = 1.19983007\n",
      "Iteration 828, loss = 1.19969299\n",
      "Iteration 829, loss = 1.19955629\n",
      "Iteration 830, loss = 1.19941995\n",
      "Iteration 831, loss = 1.19928399\n",
      "Iteration 832, loss = 1.19914839\n",
      "Iteration 833, loss = 1.19901317\n",
      "Iteration 834, loss = 1.19887831\n",
      "Iteration 835, loss = 1.19874381\n",
      "Iteration 836, loss = 1.19860969\n",
      "Iteration 837, loss = 1.19847593\n",
      "Iteration 838, loss = 1.19834253\n",
      "Iteration 839, loss = 1.19820950\n",
      "Iteration 840, loss = 1.19807683\n",
      "Iteration 841, loss = 1.19794452\n",
      "Iteration 842, loss = 1.19781258\n",
      "Iteration 843, loss = 1.19768099\n",
      "Iteration 844, loss = 1.19754976\n",
      "Iteration 845, loss = 1.19741890\n",
      "Iteration 846, loss = 1.19728839\n",
      "Iteration 847, loss = 1.19715824\n",
      "Iteration 848, loss = 1.19702844\n",
      "Iteration 849, loss = 1.19689901\n",
      "Iteration 850, loss = 1.19676992\n",
      "Iteration 851, loss = 1.19664119\n",
      "Iteration 852, loss = 1.19651282\n",
      "Iteration 853, loss = 1.19638479\n",
      "Iteration 854, loss = 1.19625712\n",
      "Iteration 855, loss = 1.19612980\n",
      "Iteration 856, loss = 1.19600284\n",
      "Iteration 857, loss = 1.19587622\n",
      "Iteration 858, loss = 1.19574995\n",
      "Iteration 859, loss = 1.19562402\n",
      "Iteration 860, loss = 1.19549845\n",
      "Iteration 861, loss = 1.19537322\n",
      "Iteration 862, loss = 1.19524834\n",
      "Iteration 863, loss = 1.19512380\n",
      "Iteration 864, loss = 1.19499961\n",
      "Iteration 865, loss = 1.19487576\n",
      "Iteration 866, loss = 1.19475225\n",
      "Iteration 867, loss = 1.19462909\n",
      "Iteration 868, loss = 1.19450626\n",
      "Iteration 869, loss = 1.19438378\n",
      "Iteration 870, loss = 1.19426164\n",
      "Iteration 871, loss = 1.19413984\n",
      "Iteration 872, loss = 1.19401837\n",
      "Iteration 873, loss = 1.19389724\n",
      "Iteration 874, loss = 1.19377645\n",
      "Iteration 875, loss = 1.19365599\n",
      "Iteration 876, loss = 1.19353587\n",
      "Iteration 877, loss = 1.19341609\n",
      "Iteration 878, loss = 1.19329664\n",
      "Iteration 879, loss = 1.19317752\n",
      "Iteration 880, loss = 1.19305873\n",
      "Iteration 881, loss = 1.19294028\n",
      "Iteration 882, loss = 1.19282215\n",
      "Iteration 883, loss = 1.19270436\n",
      "Iteration 884, loss = 1.19258689\n",
      "Iteration 885, loss = 1.19246975\n",
      "Iteration 886, loss = 1.19235295\n",
      "Iteration 887, loss = 1.19223646\n",
      "Iteration 888, loss = 1.19212031\n",
      "Iteration 889, loss = 1.19200448\n",
      "Iteration 890, loss = 1.19188897\n",
      "Iteration 891, loss = 1.19177379\n",
      "Iteration 892, loss = 1.19165893\n",
      "Iteration 893, loss = 1.19154440\n",
      "Iteration 894, loss = 1.19143019\n",
      "Iteration 895, loss = 1.19131629\n",
      "Iteration 896, loss = 1.19120272\n",
      "Iteration 897, loss = 1.19108947\n",
      "Iteration 898, loss = 1.19097654\n",
      "Iteration 899, loss = 1.19086392\n",
      "Iteration 900, loss = 1.19075163\n",
      "Iteration 901, loss = 1.19063965\n",
      "Iteration 902, loss = 1.19052798\n",
      "Iteration 903, loss = 1.19041663\n",
      "Iteration 904, loss = 1.19030560\n",
      "Iteration 905, loss = 1.19019488\n",
      "Iteration 906, loss = 1.19008447\n",
      "Iteration 907, loss = 1.18997438\n",
      "Iteration 908, loss = 1.18986459\n",
      "Iteration 909, loss = 1.18975512\n",
      "Iteration 910, loss = 1.18964596\n",
      "Iteration 911, loss = 1.18953711\n",
      "Iteration 912, loss = 1.18942856\n",
      "Iteration 913, loss = 1.18932033\n",
      "Iteration 914, loss = 1.18921240\n",
      "Iteration 915, loss = 1.18910478\n",
      "Iteration 916, loss = 1.18899747\n",
      "Iteration 917, loss = 1.18889046\n",
      "Iteration 918, loss = 1.18878375\n",
      "Iteration 919, loss = 1.18867735\n",
      "Iteration 920, loss = 1.18857125\n",
      "Iteration 921, loss = 1.18846546\n",
      "Iteration 922, loss = 1.18835996\n",
      "Iteration 923, loss = 1.18825477\n",
      "Iteration 924, loss = 1.18814988\n",
      "Iteration 925, loss = 1.18804529\n",
      "Iteration 926, loss = 1.18794099\n",
      "Iteration 927, loss = 1.18783700\n",
      "Iteration 928, loss = 1.18773330\n",
      "Iteration 929, loss = 1.18762990\n",
      "Iteration 930, loss = 1.18752679\n",
      "Iteration 931, loss = 1.18742399\n",
      "Iteration 932, loss = 1.18732147\n",
      "Iteration 933, loss = 1.18721925\n",
      "Iteration 934, loss = 1.18711732\n",
      "Iteration 935, loss = 1.18701569\n",
      "Iteration 936, loss = 1.18691435\n",
      "Iteration 937, loss = 1.18681330\n",
      "Iteration 938, loss = 1.18671254\n",
      "Iteration 939, loss = 1.18661207\n",
      "Iteration 940, loss = 1.18651189\n",
      "Iteration 941, loss = 1.18641199\n",
      "Iteration 942, loss = 1.18631239\n",
      "Iteration 943, loss = 1.18621307\n",
      "Iteration 944, loss = 1.18611404\n",
      "Iteration 945, loss = 1.18601530\n",
      "Iteration 946, loss = 1.18591684\n",
      "Iteration 947, loss = 1.18581867\n",
      "Iteration 948, loss = 1.18572077\n",
      "Iteration 949, loss = 1.18562317\n",
      "Iteration 950, loss = 1.18552584\n",
      "Iteration 951, loss = 1.18542880\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.74140056\n",
      "Iteration 2, loss = 1.73800648\n",
      "Iteration 3, loss = 1.73463266\n",
      "Iteration 4, loss = 1.73127938\n",
      "Iteration 5, loss = 1.72794693\n",
      "Iteration 6, loss = 1.72463557\n",
      "Iteration 7, loss = 1.72134555\n",
      "Iteration 8, loss = 1.71807711\n",
      "Iteration 9, loss = 1.71483048\n",
      "Iteration 10, loss = 1.71160586\n",
      "Iteration 11, loss = 1.70840343\n",
      "Iteration 12, loss = 1.70522339\n",
      "Iteration 13, loss = 1.70206587\n",
      "Iteration 14, loss = 1.69893102\n",
      "Iteration 15, loss = 1.69581896\n",
      "Iteration 16, loss = 1.69272979\n",
      "Iteration 17, loss = 1.68966359\n",
      "Iteration 18, loss = 1.68662043\n",
      "Iteration 19, loss = 1.68360034\n",
      "Iteration 20, loss = 1.68060335\n",
      "Iteration 21, loss = 1.67762946\n",
      "Iteration 22, loss = 1.67467867\n",
      "Iteration 23, loss = 1.67175093\n",
      "Iteration 24, loss = 1.66884620\n",
      "Iteration 25, loss = 1.66596440\n",
      "Iteration 26, loss = 1.66310546\n",
      "Iteration 27, loss = 1.66026926\n",
      "Iteration 28, loss = 1.65745568\n",
      "Iteration 29, loss = 1.65464252\n",
      "Iteration 30, loss = 1.65184269\n",
      "Iteration 31, loss = 1.64906185\n",
      "Iteration 32, loss = 1.64630049\n",
      "Iteration 33, loss = 1.64355894\n",
      "Iteration 34, loss = 1.64083738\n",
      "Iteration 35, loss = 1.63813594\n",
      "Iteration 36, loss = 1.63545465\n",
      "Iteration 37, loss = 1.63279351\n",
      "Iteration 38, loss = 1.63015247\n",
      "Iteration 39, loss = 1.62753146\n",
      "Iteration 40, loss = 1.62493038\n",
      "Iteration 41, loss = 1.62234910\n",
      "Iteration 42, loss = 1.61978485\n",
      "Iteration 43, loss = 1.61723771\n",
      "Iteration 44, loss = 1.61470907\n",
      "Iteration 45, loss = 1.61219883\n",
      "Iteration 46, loss = 1.60970689\n",
      "Iteration 47, loss = 1.60723313\n",
      "Iteration 48, loss = 1.60477742\n",
      "Iteration 49, loss = 1.60233963\n",
      "Iteration 50, loss = 1.59991960\n",
      "Iteration 51, loss = 1.59751720\n",
      "Iteration 52, loss = 1.59513226\n",
      "Iteration 53, loss = 1.59276461\n",
      "Iteration 54, loss = 1.59041411\n",
      "Iteration 55, loss = 1.58808056\n",
      "Iteration 56, loss = 1.58576381\n",
      "Iteration 57, loss = 1.58346366\n",
      "Iteration 58, loss = 1.58117995\n",
      "Iteration 59, loss = 1.57891250\n",
      "Iteration 60, loss = 1.57666111\n",
      "Iteration 61, loss = 1.57442560\n",
      "Iteration 62, loss = 1.57220580\n",
      "Iteration 63, loss = 1.57000151\n",
      "Iteration 64, loss = 1.56781112\n",
      "Iteration 65, loss = 1.56562896\n",
      "Iteration 66, loss = 1.56346110\n",
      "Iteration 67, loss = 1.56130741\n",
      "Iteration 68, loss = 1.55916779\n",
      "Iteration 69, loss = 1.55704210\n",
      "Iteration 70, loss = 1.55493021\n",
      "Iteration 71, loss = 1.55283198\n",
      "Iteration 72, loss = 1.55074726\n",
      "Iteration 73, loss = 1.54867591\n",
      "Iteration 74, loss = 1.54661777\n",
      "Iteration 75, loss = 1.54457270\n",
      "Iteration 76, loss = 1.54254697\n",
      "Iteration 77, loss = 1.54053743\n",
      "Iteration 78, loss = 1.53854099\n",
      "Iteration 79, loss = 1.53655741\n",
      "Iteration 80, loss = 1.53458649\n",
      "Iteration 81, loss = 1.53262799\n",
      "Iteration 82, loss = 1.53068172\n",
      "Iteration 83, loss = 1.52874746\n",
      "Iteration 84, loss = 1.52682502\n",
      "Iteration 85, loss = 1.52491421\n",
      "Iteration 86, loss = 1.52301483\n",
      "Iteration 87, loss = 1.52112339\n",
      "Iteration 88, loss = 1.51923776\n",
      "Iteration 89, loss = 1.51736237\n",
      "Iteration 90, loss = 1.51549712\n",
      "Iteration 91, loss = 1.51364192\n",
      "Iteration 92, loss = 1.51179664\n",
      "Iteration 93, loss = 1.50996117\n",
      "Iteration 94, loss = 1.50813541\n",
      "Iteration 95, loss = 1.50631923\n",
      "Iteration 96, loss = 1.50451251\n",
      "Iteration 97, loss = 1.50271513\n",
      "Iteration 98, loss = 1.50092697\n",
      "Iteration 99, loss = 1.49914790\n",
      "Iteration 100, loss = 1.49737781\n",
      "Iteration 101, loss = 1.49561657\n",
      "Iteration 102, loss = 1.49386406\n",
      "Iteration 103, loss = 1.49212015\n",
      "Iteration 104, loss = 1.49038472\n",
      "Iteration 105, loss = 1.48865765\n",
      "Iteration 106, loss = 1.48693882\n",
      "Iteration 107, loss = 1.48522810\n",
      "Iteration 108, loss = 1.48352538\n",
      "Iteration 109, loss = 1.48182610\n",
      "Iteration 110, loss = 1.48013235\n",
      "Iteration 111, loss = 1.47844583\n",
      "Iteration 112, loss = 1.47676650\n",
      "Iteration 113, loss = 1.47509430\n",
      "Iteration 114, loss = 1.47342917\n",
      "Iteration 115, loss = 1.47177104\n",
      "Iteration 116, loss = 1.47011985\n",
      "Iteration 117, loss = 1.46847552\n",
      "Iteration 118, loss = 1.46683798\n",
      "Iteration 119, loss = 1.46520715\n",
      "Iteration 120, loss = 1.46358294\n",
      "Iteration 121, loss = 1.46196529\n",
      "Iteration 122, loss = 1.46035410\n",
      "Iteration 123, loss = 1.45874930\n",
      "Iteration 124, loss = 1.45715080\n",
      "Iteration 125, loss = 1.45555851\n",
      "Iteration 126, loss = 1.45397236\n",
      "Iteration 127, loss = 1.45239226\n",
      "Iteration 128, loss = 1.45081813\n",
      "Iteration 129, loss = 1.44924988\n",
      "Iteration 130, loss = 1.44768744\n",
      "Iteration 131, loss = 1.44613072\n",
      "Iteration 132, loss = 1.44457965\n",
      "Iteration 133, loss = 1.44303414\n",
      "Iteration 134, loss = 1.44149412\n",
      "Iteration 135, loss = 1.43995951\n",
      "Iteration 136, loss = 1.43843024\n",
      "Iteration 137, loss = 1.43690623\n",
      "Iteration 138, loss = 1.43538741\n",
      "Iteration 139, loss = 1.43387371\n",
      "Iteration 140, loss = 1.43236506\n",
      "Iteration 141, loss = 1.43086140\n",
      "Iteration 142, loss = 1.42936265\n",
      "Iteration 143, loss = 1.42786591\n",
      "Iteration 144, loss = 1.42636820\n",
      "Iteration 145, loss = 1.42486720\n",
      "Iteration 146, loss = 1.42336958\n",
      "Iteration 147, loss = 1.42187545\n",
      "Iteration 148, loss = 1.42038488\n",
      "Iteration 149, loss = 1.41889795\n",
      "Iteration 150, loss = 1.41741472\n",
      "Iteration 151, loss = 1.41593524\n",
      "Iteration 152, loss = 1.41445954\n",
      "Iteration 153, loss = 1.41298765\n",
      "Iteration 154, loss = 1.41151959\n",
      "Iteration 155, loss = 1.41005537\n",
      "Iteration 156, loss = 1.40859501\n",
      "Iteration 157, loss = 1.40713852\n",
      "Iteration 158, loss = 1.40568588\n",
      "Iteration 159, loss = 1.40423710\n",
      "Iteration 160, loss = 1.40279216\n",
      "Iteration 161, loss = 1.40135107\n",
      "Iteration 162, loss = 1.39991381\n",
      "Iteration 163, loss = 1.39848037\n",
      "Iteration 164, loss = 1.39705073\n",
      "Iteration 165, loss = 1.39562488\n",
      "Iteration 166, loss = 1.39420280\n",
      "Iteration 167, loss = 1.39278448\n",
      "Iteration 168, loss = 1.39136991\n",
      "Iteration 169, loss = 1.38995905\n",
      "Iteration 170, loss = 1.38855190\n",
      "Iteration 171, loss = 1.38715188\n",
      "Iteration 172, loss = 1.38576931\n",
      "Iteration 173, loss = 1.38439105\n",
      "Iteration 174, loss = 1.38301699\n",
      "Iteration 175, loss = 1.38164702\n",
      "Iteration 176, loss = 1.38028106\n",
      "Iteration 177, loss = 1.37891901\n",
      "Iteration 178, loss = 1.37756372\n",
      "Iteration 179, loss = 1.37622294\n",
      "Iteration 180, loss = 1.37488588\n",
      "Iteration 181, loss = 1.37355245\n",
      "Iteration 182, loss = 1.37222261\n",
      "Iteration 183, loss = 1.37089630\n",
      "Iteration 184, loss = 1.36957348\n",
      "Iteration 185, loss = 1.36825410\n",
      "Iteration 186, loss = 1.36693814\n",
      "Iteration 187, loss = 1.36562557\n",
      "Iteration 188, loss = 1.36431636\n",
      "Iteration 189, loss = 1.36301049\n",
      "Iteration 190, loss = 1.36170796\n",
      "Iteration 191, loss = 1.36040874\n",
      "Iteration 192, loss = 1.35911284\n",
      "Iteration 193, loss = 1.35782023\n",
      "Iteration 194, loss = 1.35653092\n",
      "Iteration 195, loss = 1.35524491\n",
      "Iteration 196, loss = 1.35396220\n",
      "Iteration 197, loss = 1.35268279\n",
      "Iteration 198, loss = 1.35140669\n",
      "Iteration 199, loss = 1.35013298\n",
      "Iteration 200, loss = 1.34884597\n",
      "Iteration 201, loss = 1.34756067\n",
      "Iteration 202, loss = 1.34627726\n",
      "Iteration 203, loss = 1.34497830\n",
      "Iteration 204, loss = 1.34368006\n",
      "Iteration 205, loss = 1.34238679\n",
      "Iteration 206, loss = 1.34109133\n",
      "Iteration 207, loss = 1.33979446\n",
      "Iteration 208, loss = 1.33849687\n",
      "Iteration 209, loss = 1.33719919\n",
      "Iteration 210, loss = 1.33590197\n",
      "Iteration 211, loss = 1.33460569\n",
      "Iteration 212, loss = 1.33331080\n",
      "Iteration 213, loss = 1.33201769\n",
      "Iteration 214, loss = 1.33072669\n",
      "Iteration 215, loss = 1.32943496\n",
      "Iteration 216, loss = 1.32811332\n",
      "Iteration 217, loss = 1.32679125\n",
      "Iteration 218, loss = 1.32546946\n",
      "Iteration 219, loss = 1.32414858\n",
      "Iteration 220, loss = 1.32282917\n",
      "Iteration 221, loss = 1.32151173\n",
      "Iteration 222, loss = 1.32019670\n",
      "Iteration 223, loss = 1.31888448\n",
      "Iteration 224, loss = 1.31757544\n",
      "Iteration 225, loss = 1.31626988\n",
      "Iteration 226, loss = 1.31496809\n",
      "Iteration 227, loss = 1.31367032\n",
      "Iteration 228, loss = 1.31237679\n",
      "Iteration 229, loss = 1.31108261\n",
      "Iteration 230, loss = 1.30978794\n",
      "Iteration 231, loss = 1.30849703\n",
      "Iteration 232, loss = 1.30721014\n",
      "Iteration 233, loss = 1.30592750\n",
      "Iteration 234, loss = 1.30464932\n",
      "Iteration 235, loss = 1.30337576\n",
      "Iteration 236, loss = 1.30210702\n",
      "Iteration 237, loss = 1.30084322\n",
      "Iteration 238, loss = 1.29958450\n",
      "Iteration 239, loss = 1.29833098\n",
      "Iteration 240, loss = 1.29708278\n",
      "Iteration 241, loss = 1.29583998\n",
      "Iteration 242, loss = 1.29460267\n",
      "Iteration 243, loss = 1.29337094\n",
      "Iteration 244, loss = 1.29214484\n",
      "Iteration 245, loss = 1.29092444\n",
      "Iteration 246, loss = 1.28970981\n",
      "Iteration 247, loss = 1.28850098\n",
      "Iteration 248, loss = 1.28729801\n",
      "Iteration 249, loss = 1.28610093\n",
      "Iteration 250, loss = 1.28490979\n",
      "Iteration 251, loss = 1.28372461\n",
      "Iteration 252, loss = 1.28254542\n",
      "Iteration 253, loss = 1.28137225\n",
      "Iteration 254, loss = 1.28020512\n",
      "Iteration 255, loss = 1.27904406\n",
      "Iteration 256, loss = 1.27788908\n",
      "Iteration 257, loss = 1.27674019\n",
      "Iteration 258, loss = 1.27559741\n",
      "Iteration 259, loss = 1.27446076\n",
      "Iteration 260, loss = 1.27333024\n",
      "Iteration 261, loss = 1.27220586\n",
      "Iteration 262, loss = 1.27108763\n",
      "Iteration 263, loss = 1.26997556\n",
      "Iteration 264, loss = 1.26886965\n",
      "Iteration 265, loss = 1.26776990\n",
      "Iteration 266, loss = 1.26667631\n",
      "Iteration 267, loss = 1.26558890\n",
      "Iteration 268, loss = 1.26450765\n",
      "Iteration 269, loss = 1.26343257\n",
      "Iteration 270, loss = 1.26236366\n",
      "Iteration 271, loss = 1.26130090\n",
      "Iteration 272, loss = 1.26024431\n",
      "Iteration 273, loss = 1.25919388\n",
      "Iteration 274, loss = 1.25814959\n",
      "Iteration 275, loss = 1.25711145\n",
      "Iteration 276, loss = 1.25607945\n",
      "Iteration 277, loss = 1.25505358\n",
      "Iteration 278, loss = 1.25403383\n",
      "Iteration 279, loss = 1.25302020\n",
      "Iteration 280, loss = 1.25201268\n",
      "Iteration 281, loss = 1.25101125\n",
      "Iteration 282, loss = 1.25001590\n",
      "Iteration 283, loss = 1.24902663\n",
      "Iteration 284, loss = 1.24804343\n",
      "Iteration 285, loss = 1.24706627\n",
      "Iteration 286, loss = 1.24609515\n",
      "Iteration 287, loss = 1.24513005\n",
      "Iteration 288, loss = 1.24417096\n",
      "Iteration 289, loss = 1.24321787\n",
      "Iteration 290, loss = 1.24227075\n",
      "Iteration 291, loss = 1.24132959\n",
      "Iteration 292, loss = 1.24039439\n",
      "Iteration 293, loss = 1.23946511\n",
      "Iteration 294, loss = 1.23854174\n",
      "Iteration 295, loss = 1.23762426\n",
      "Iteration 296, loss = 1.23671266\n",
      "Iteration 297, loss = 1.23580692\n",
      "Iteration 298, loss = 1.23490701\n",
      "Iteration 299, loss = 1.23401291\n",
      "Iteration 300, loss = 1.23312462\n",
      "Iteration 301, loss = 1.23224209\n",
      "Iteration 302, loss = 1.23136532\n",
      "Iteration 303, loss = 1.23049428\n",
      "Iteration 304, loss = 1.22962894\n",
      "Iteration 305, loss = 1.22876930\n",
      "Iteration 306, loss = 1.22791531\n",
      "Iteration 307, loss = 1.22706696\n",
      "Iteration 308, loss = 1.22622423\n",
      "Iteration 309, loss = 1.22538709\n",
      "Iteration 310, loss = 1.22455552\n",
      "Iteration 311, loss = 1.22372948\n",
      "Iteration 312, loss = 1.22290897\n",
      "Iteration 313, loss = 1.22209394\n",
      "Iteration 314, loss = 1.22128523\n",
      "Iteration 315, loss = 1.22048359\n",
      "Iteration 316, loss = 1.21968749\n",
      "Iteration 317, loss = 1.21889689\n",
      "Iteration 318, loss = 1.21811178\n",
      "Iteration 319, loss = 1.21733210\n",
      "Iteration 320, loss = 1.21655784\n",
      "Iteration 321, loss = 1.21578897\n",
      "Iteration 322, loss = 1.21502544\n",
      "Iteration 323, loss = 1.21426723\n",
      "Iteration 324, loss = 1.21351431\n",
      "Iteration 325, loss = 1.21276665\n",
      "Iteration 326, loss = 1.21202421\n",
      "Iteration 327, loss = 1.21128697\n",
      "Iteration 328, loss = 1.21055490\n",
      "Iteration 329, loss = 1.20982796\n",
      "Iteration 330, loss = 1.20910612\n",
      "Iteration 331, loss = 1.20838935\n",
      "Iteration 332, loss = 1.20767763\n",
      "Iteration 333, loss = 1.20697092\n",
      "Iteration 334, loss = 1.20626918\n",
      "Iteration 335, loss = 1.20557240\n",
      "Iteration 336, loss = 1.20488053\n",
      "Iteration 337, loss = 1.20419355\n",
      "Iteration 338, loss = 1.20351142\n",
      "Iteration 339, loss = 1.20283411\n",
      "Iteration 340, loss = 1.20216160\n",
      "Iteration 341, loss = 1.20149385\n",
      "Iteration 342, loss = 1.20083084\n",
      "Iteration 343, loss = 1.20017252\n",
      "Iteration 344, loss = 1.19951887\n",
      "Iteration 345, loss = 1.19886985\n",
      "Iteration 346, loss = 1.19822545\n",
      "Iteration 347, loss = 1.19758562\n",
      "Iteration 348, loss = 1.19695033\n",
      "Iteration 349, loss = 1.19631956\n",
      "Iteration 350, loss = 1.19569327\n",
      "Iteration 351, loss = 1.19507144\n",
      "Iteration 352, loss = 1.19445402\n",
      "Iteration 353, loss = 1.19384100\n",
      "Iteration 354, loss = 1.19323234\n",
      "Iteration 355, loss = 1.19262801\n",
      "Iteration 356, loss = 1.19202798\n",
      "Iteration 357, loss = 1.19143222\n",
      "Iteration 358, loss = 1.19084070\n",
      "Iteration 359, loss = 1.19025340\n",
      "Iteration 360, loss = 1.18967027\n",
      "Iteration 361, loss = 1.18909129\n",
      "Iteration 362, loss = 1.18851644\n",
      "Iteration 363, loss = 1.18794568\n",
      "Iteration 364, loss = 1.18737898\n",
      "Iteration 365, loss = 1.18681632\n",
      "Iteration 366, loss = 1.18625766\n",
      "Iteration 367, loss = 1.18570297\n",
      "Iteration 368, loss = 1.18515224\n",
      "Iteration 369, loss = 1.18460542\n",
      "Iteration 370, loss = 1.18406250\n",
      "Iteration 371, loss = 1.18352344\n",
      "Iteration 372, loss = 1.18298821\n",
      "Iteration 373, loss = 1.18245679\n",
      "Iteration 374, loss = 1.18192915\n",
      "Iteration 375, loss = 1.18140526\n",
      "Iteration 376, loss = 1.18088510\n",
      "Iteration 377, loss = 1.18036863\n",
      "Iteration 378, loss = 1.17985583\n",
      "Iteration 379, loss = 1.17934668\n",
      "Iteration 380, loss = 1.17884114\n",
      "Iteration 381, loss = 1.17833919\n",
      "Iteration 382, loss = 1.17784081\n",
      "Iteration 383, loss = 1.17734597\n",
      "Iteration 384, loss = 1.17685463\n",
      "Iteration 385, loss = 1.17636679\n",
      "Iteration 386, loss = 1.17588240\n",
      "Iteration 387, loss = 1.17540145\n",
      "Iteration 388, loss = 1.17492391\n",
      "Iteration 389, loss = 1.17444975\n",
      "Iteration 390, loss = 1.17397896\n",
      "Iteration 391, loss = 1.17351150\n",
      "Iteration 392, loss = 1.17304735\n",
      "Iteration 393, loss = 1.17258648\n",
      "Iteration 394, loss = 1.17212888\n",
      "Iteration 395, loss = 1.17167452\n",
      "Iteration 396, loss = 1.17122337\n",
      "Iteration 397, loss = 1.17077541\n",
      "Iteration 398, loss = 1.17033062\n",
      "Iteration 399, loss = 1.16988897\n",
      "Iteration 400, loss = 1.16945044\n",
      "Iteration 401, loss = 1.16901501\n",
      "Iteration 402, loss = 1.16858266\n",
      "Iteration 403, loss = 1.16815336\n",
      "Iteration 404, loss = 1.16772708\n",
      "Iteration 405, loss = 1.16730382\n",
      "Iteration 406, loss = 1.16688354\n",
      "Iteration 407, loss = 1.16646622\n",
      "Iteration 408, loss = 1.16605185\n",
      "Iteration 409, loss = 1.16564040\n",
      "Iteration 410, loss = 1.16523184\n",
      "Iteration 411, loss = 1.16482616\n",
      "Iteration 412, loss = 1.16442334\n",
      "Iteration 413, loss = 1.16402335\n",
      "Iteration 414, loss = 1.16362618\n",
      "Iteration 415, loss = 1.16323180\n",
      "Iteration 416, loss = 1.16284020\n",
      "Iteration 417, loss = 1.16245134\n",
      "Iteration 418, loss = 1.16206522\n",
      "Iteration 419, loss = 1.16168182\n",
      "Iteration 420, loss = 1.16130110\n",
      "Iteration 421, loss = 1.16092306\n",
      "Iteration 422, loss = 1.16054767\n",
      "Iteration 423, loss = 1.16017491\n",
      "Iteration 424, loss = 1.15980477\n",
      "Iteration 425, loss = 1.15943723\n",
      "Iteration 426, loss = 1.15907226\n",
      "Iteration 427, loss = 1.15870985\n",
      "Iteration 428, loss = 1.15834998\n",
      "Iteration 429, loss = 1.15799263\n",
      "Iteration 430, loss = 1.15763778\n",
      "Iteration 431, loss = 1.15728541\n",
      "Iteration 432, loss = 1.15693551\n",
      "Iteration 433, loss = 1.15658806\n",
      "Iteration 434, loss = 1.15624303\n",
      "Iteration 435, loss = 1.15590042\n",
      "Iteration 436, loss = 1.15556020\n",
      "Iteration 437, loss = 1.15522236\n",
      "Iteration 438, loss = 1.15488688\n",
      "Iteration 439, loss = 1.15455374\n",
      "Iteration 440, loss = 1.15422292\n",
      "Iteration 441, loss = 1.15389442\n",
      "Iteration 442, loss = 1.15356820\n",
      "Iteration 443, loss = 1.15324426\n",
      "Iteration 444, loss = 1.15292258\n",
      "Iteration 445, loss = 1.15260313\n",
      "Iteration 446, loss = 1.15228592\n",
      "Iteration 447, loss = 1.15197091\n",
      "Iteration 448, loss = 1.15165809\n",
      "Iteration 449, loss = 1.15134745\n",
      "Iteration 450, loss = 1.15103897\n",
      "Iteration 451, loss = 1.15073264\n",
      "Iteration 452, loss = 1.15042844\n",
      "Iteration 453, loss = 1.15012635\n",
      "Iteration 454, loss = 1.14982636\n",
      "Iteration 455, loss = 1.14952845\n",
      "Iteration 456, loss = 1.14923261\n",
      "Iteration 457, loss = 1.14893882\n",
      "Iteration 458, loss = 1.14864708\n",
      "Iteration 459, loss = 1.14835735\n",
      "Iteration 460, loss = 1.14806963\n",
      "Iteration 461, loss = 1.14778391\n",
      "Iteration 462, loss = 1.14750017\n",
      "Iteration 463, loss = 1.14721839\n",
      "Iteration 464, loss = 1.14693857\n",
      "Iteration 465, loss = 1.14666068\n",
      "Iteration 466, loss = 1.14638471\n",
      "Iteration 467, loss = 1.14611065\n",
      "Iteration 468, loss = 1.14583848\n",
      "Iteration 469, loss = 1.14556820\n",
      "Iteration 470, loss = 1.14529978\n",
      "Iteration 471, loss = 1.14503322\n",
      "Iteration 472, loss = 1.14476850\n",
      "Iteration 473, loss = 1.14450560\n",
      "Iteration 474, loss = 1.14424452\n",
      "Iteration 475, loss = 1.14398524\n",
      "Iteration 476, loss = 1.14372774\n",
      "Iteration 477, loss = 1.14347202\n",
      "Iteration 478, loss = 1.14321806\n",
      "Iteration 479, loss = 1.14296585\n",
      "Iteration 480, loss = 1.14271538\n",
      "Iteration 481, loss = 1.14246663\n",
      "Iteration 482, loss = 1.14221958\n",
      "Iteration 483, loss = 1.14197424\n",
      "Iteration 484, loss = 1.14173058\n",
      "Iteration 485, loss = 1.14148860\n",
      "Iteration 486, loss = 1.14124828\n",
      "Iteration 487, loss = 1.14100961\n",
      "Iteration 488, loss = 1.14077257\n",
      "Iteration 489, loss = 1.14053716\n",
      "Iteration 490, loss = 1.14030336\n",
      "Iteration 491, loss = 1.14007117\n",
      "Iteration 492, loss = 1.13984056\n",
      "Iteration 493, loss = 1.13961154\n",
      "Iteration 494, loss = 1.13938408\n",
      "Iteration 495, loss = 1.13915818\n",
      "Iteration 496, loss = 1.13893382\n",
      "Iteration 497, loss = 1.13871099\n",
      "Iteration 498, loss = 1.13848969\n",
      "Iteration 499, loss = 1.13826990\n",
      "Iteration 500, loss = 1.13805160\n",
      "Iteration 501, loss = 1.13783480\n",
      "Iteration 502, loss = 1.13761947\n",
      "Iteration 503, loss = 1.13740562\n",
      "Iteration 504, loss = 1.13719321\n",
      "Iteration 505, loss = 1.13698226\n",
      "Iteration 506, loss = 1.13677274\n",
      "Iteration 507, loss = 1.13656464\n",
      "Iteration 508, loss = 1.13635796\n",
      "Iteration 509, loss = 1.13615269\n",
      "Iteration 510, loss = 1.13594880\n",
      "Iteration 511, loss = 1.13574631\n",
      "Iteration 512, loss = 1.13554518\n",
      "Iteration 513, loss = 1.13534542\n",
      "Iteration 514, loss = 1.13514701\n",
      "Iteration 515, loss = 1.13494995\n",
      "Iteration 516, loss = 1.13475422\n",
      "Iteration 517, loss = 1.13455982\n",
      "Iteration 518, loss = 1.13436673\n",
      "Iteration 519, loss = 1.13417494\n",
      "Iteration 520, loss = 1.13398445\n",
      "Iteration 521, loss = 1.13379525\n",
      "Iteration 522, loss = 1.13360732\n",
      "Iteration 523, loss = 1.13342066\n",
      "Iteration 524, loss = 1.13323526\n",
      "Iteration 525, loss = 1.13305110\n",
      "Iteration 526, loss = 1.13286819\n",
      "Iteration 527, loss = 1.13268651\n",
      "Iteration 528, loss = 1.13250604\n",
      "Iteration 529, loss = 1.13232680\n",
      "Iteration 530, loss = 1.13214875\n",
      "Iteration 531, loss = 1.13197206\n",
      "Iteration 532, loss = 1.13179666\n",
      "Iteration 533, loss = 1.13162247\n",
      "Iteration 534, loss = 1.13144948\n",
      "Iteration 535, loss = 1.13127767\n",
      "Iteration 536, loss = 1.13110726\n",
      "Iteration 537, loss = 1.13093809\n",
      "Iteration 538, loss = 1.13077009\n",
      "Iteration 539, loss = 1.13060327\n",
      "Iteration 540, loss = 1.13043762\n",
      "Iteration 541, loss = 1.13027311\n",
      "Iteration 542, loss = 1.13010975\n",
      "Iteration 543, loss = 1.12994753\n",
      "Iteration 544, loss = 1.12978642\n",
      "Iteration 545, loss = 1.12962643\n",
      "Iteration 546, loss = 1.12946755\n",
      "Iteration 547, loss = 1.12930976\n",
      "Iteration 548, loss = 1.12915306\n",
      "Iteration 549, loss = 1.12899744\n",
      "Iteration 550, loss = 1.12884289\n",
      "Iteration 551, loss = 1.12868940\n",
      "Iteration 552, loss = 1.12853837\n",
      "Iteration 553, loss = 1.12839045\n",
      "Iteration 554, loss = 1.12824354\n",
      "Iteration 555, loss = 1.12809763\n",
      "Iteration 556, loss = 1.12795273\n",
      "Iteration 557, loss = 1.12780905\n",
      "Iteration 558, loss = 1.12766757\n",
      "Iteration 559, loss = 1.12752645\n",
      "Iteration 560, loss = 1.12738575\n",
      "Iteration 561, loss = 1.12724552\n",
      "Iteration 562, loss = 1.12710583\n",
      "Iteration 563, loss = 1.12696671\n",
      "Iteration 564, loss = 1.12682872\n",
      "Iteration 565, loss = 1.12669264\n",
      "Iteration 566, loss = 1.12655755\n",
      "Iteration 567, loss = 1.12642345\n",
      "Iteration 568, loss = 1.12629031\n",
      "Iteration 569, loss = 1.12615814\n",
      "Iteration 570, loss = 1.12602692\n",
      "Iteration 571, loss = 1.12589664\n",
      "Iteration 572, loss = 1.12576731\n",
      "Iteration 573, loss = 1.12563890\n",
      "Iteration 574, loss = 1.12551141\n",
      "Iteration 575, loss = 1.12538484\n",
      "Iteration 576, loss = 1.12525917\n",
      "Iteration 577, loss = 1.12513440\n",
      "Iteration 578, loss = 1.12501052\n",
      "Iteration 579, loss = 1.12488753\n",
      "Iteration 580, loss = 1.12476540\n",
      "Iteration 581, loss = 1.12464415\n",
      "Iteration 582, loss = 1.12452375\n",
      "Iteration 583, loss = 1.12440421\n",
      "Iteration 584, loss = 1.12428552\n",
      "Iteration 585, loss = 1.12416766\n",
      "Iteration 586, loss = 1.12405064\n",
      "Iteration 587, loss = 1.12393444\n",
      "Iteration 588, loss = 1.12381906\n",
      "Iteration 589, loss = 1.12370450\n",
      "Iteration 590, loss = 1.12359074\n",
      "Iteration 591, loss = 1.12347779\n",
      "Iteration 592, loss = 1.12336562\n",
      "Iteration 593, loss = 1.12325425\n",
      "Iteration 594, loss = 1.12314366\n",
      "Iteration 595, loss = 1.12303384\n",
      "Iteration 596, loss = 1.12292479\n",
      "Iteration 597, loss = 1.12281651\n",
      "Iteration 598, loss = 1.12270898\n",
      "Iteration 599, loss = 1.12260221\n",
      "Iteration 600, loss = 1.12249618\n",
      "Iteration 601, loss = 1.12239090\n",
      "Iteration 602, loss = 1.12228635\n",
      "Iteration 603, loss = 1.12218253\n",
      "Iteration 604, loss = 1.12207943\n",
      "Iteration 605, loss = 1.12197705\n",
      "Iteration 606, loss = 1.12187538\n",
      "Iteration 607, loss = 1.12177443\n",
      "Iteration 608, loss = 1.12167417\n",
      "Iteration 609, loss = 1.12157461\n",
      "Iteration 610, loss = 1.12147575\n",
      "Iteration 611, loss = 1.12137757\n",
      "Iteration 612, loss = 1.12128007\n",
      "Iteration 613, loss = 1.12118324\n",
      "Iteration 614, loss = 1.12108709\n",
      "Iteration 615, loss = 1.12099161\n",
      "Iteration 616, loss = 1.12089678\n",
      "Iteration 617, loss = 1.12080261\n",
      "Iteration 618, loss = 1.12070909\n",
      "Iteration 619, loss = 1.12061622\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.18343967\n",
      "Iteration 2, loss = 1.18157195\n",
      "Iteration 3, loss = 1.17974539\n",
      "Iteration 4, loss = 1.17796045\n",
      "Iteration 5, loss = 1.17621747\n",
      "Iteration 6, loss = 1.17451669\n",
      "Iteration 7, loss = 1.17285822\n",
      "Iteration 8, loss = 1.17124200\n",
      "Iteration 9, loss = 1.16966777\n",
      "Iteration 10, loss = 1.16813511\n",
      "Iteration 11, loss = 1.16664346\n",
      "Iteration 12, loss = 1.16519223\n",
      "Iteration 13, loss = 1.16378079\n",
      "Iteration 14, loss = 1.16240858\n",
      "Iteration 15, loss = 1.16107497\n",
      "Iteration 16, loss = 1.15977936\n",
      "Iteration 17, loss = 1.15852107\n",
      "Iteration 18, loss = 1.15729939\n",
      "Iteration 19, loss = 1.15611240\n",
      "Iteration 20, loss = 1.15495994\n",
      "Iteration 21, loss = 1.15384155\n",
      "Iteration 22, loss = 1.15275634\n",
      "Iteration 23, loss = 1.15170340\n",
      "Iteration 24, loss = 1.15068175\n",
      "Iteration 25, loss = 1.14969042\n",
      "Iteration 26, loss = 1.14872840\n",
      "Iteration 27, loss = 1.14779467\n",
      "Iteration 28, loss = 1.14688820\n",
      "Iteration 29, loss = 1.14600796\n",
      "Iteration 30, loss = 1.14515293\n",
      "Iteration 31, loss = 1.14432209\n",
      "Iteration 32, loss = 1.14351444\n",
      "Iteration 33, loss = 1.14272901\n",
      "Iteration 34, loss = 1.14196484\n",
      "Iteration 35, loss = 1.14122102\n",
      "Iteration 36, loss = 1.14049665\n",
      "Iteration 37, loss = 1.13979089\n",
      "Iteration 38, loss = 1.13910293\n",
      "Iteration 39, loss = 1.13843199\n",
      "Iteration 40, loss = 1.13777735\n",
      "Iteration 41, loss = 1.13713831\n",
      "Iteration 42, loss = 1.13651423\n",
      "Iteration 43, loss = 1.13590451\n",
      "Iteration 44, loss = 1.13530857\n",
      "Iteration 45, loss = 1.13472590\n",
      "Iteration 46, loss = 1.13415600\n",
      "Iteration 47, loss = 1.13359841\n",
      "Iteration 48, loss = 1.13305272\n",
      "Iteration 49, loss = 1.13251852\n",
      "Iteration 50, loss = 1.13199547\n",
      "Iteration 51, loss = 1.13148320\n",
      "Iteration 52, loss = 1.13098142\n",
      "Iteration 53, loss = 1.13048983\n",
      "Iteration 54, loss = 1.13000814\n",
      "Iteration 55, loss = 1.12953611\n",
      "Iteration 56, loss = 1.12907348\n",
      "Iteration 57, loss = 1.12862002\n",
      "Iteration 58, loss = 1.12817552\n",
      "Iteration 59, loss = 1.12773976\n",
      "Iteration 60, loss = 1.12731253\n",
      "Iteration 61, loss = 1.12689365\n",
      "Iteration 62, loss = 1.12648291\n",
      "Iteration 63, loss = 1.12608015\n",
      "Iteration 64, loss = 1.12568517\n",
      "Iteration 65, loss = 1.12529780\n",
      "Iteration 66, loss = 1.12491787\n",
      "Iteration 67, loss = 1.12454574\n",
      "Iteration 68, loss = 1.12418330\n",
      "Iteration 69, loss = 1.12382795\n",
      "Iteration 70, loss = 1.12347949\n",
      "Iteration 71, loss = 1.12313773\n",
      "Iteration 72, loss = 1.12280250\n",
      "Iteration 73, loss = 1.12247363\n",
      "Iteration 74, loss = 1.12215094\n",
      "Iteration 75, loss = 1.12183426\n",
      "Iteration 76, loss = 1.12152343\n",
      "Iteration 77, loss = 1.12121830\n",
      "Iteration 78, loss = 1.12091871\n",
      "Iteration 79, loss = 1.12062450\n",
      "Iteration 80, loss = 1.12033554\n",
      "Iteration 81, loss = 1.12005167\n",
      "Iteration 82, loss = 1.11977277\n",
      "Iteration 83, loss = 1.11949868\n",
      "Iteration 84, loss = 1.11922929\n",
      "Iteration 85, loss = 1.11896446\n",
      "Iteration 86, loss = 1.11870407\n",
      "Iteration 87, loss = 1.11844801\n",
      "Iteration 88, loss = 1.11819768\n",
      "Iteration 89, loss = 1.11795221\n",
      "Iteration 90, loss = 1.11771084\n",
      "Iteration 91, loss = 1.11747344\n",
      "Iteration 92, loss = 1.11723988\n",
      "Iteration 93, loss = 1.11701006\n",
      "Iteration 94, loss = 1.11678386\n",
      "Iteration 95, loss = 1.11656118\n",
      "Iteration 96, loss = 1.11634191\n",
      "Iteration 97, loss = 1.11612597\n",
      "Iteration 98, loss = 1.11591326\n",
      "Iteration 99, loss = 1.11570370\n",
      "Iteration 100, loss = 1.11549719\n",
      "Iteration 101, loss = 1.11529365\n",
      "Iteration 102, loss = 1.11509301\n",
      "Iteration 103, loss = 1.11489520\n",
      "Iteration 104, loss = 1.11470013\n",
      "Iteration 105, loss = 1.11450774\n",
      "Iteration 106, loss = 1.11431796\n",
      "Iteration 107, loss = 1.11413073\n",
      "Iteration 108, loss = 1.11394598\n",
      "Iteration 109, loss = 1.11376366\n",
      "Iteration 110, loss = 1.11358370\n",
      "Iteration 111, loss = 1.11340605\n",
      "Iteration 112, loss = 1.11323065\n",
      "Iteration 113, loss = 1.11305745\n",
      "Iteration 114, loss = 1.11288640\n",
      "Iteration 115, loss = 1.11271745\n",
      "Iteration 116, loss = 1.11255055\n",
      "Iteration 117, loss = 1.11238565\n",
      "Iteration 118, loss = 1.11222271\n",
      "Iteration 119, loss = 1.11206169\n",
      "Iteration 120, loss = 1.11190254\n",
      "Iteration 121, loss = 1.11174522\n",
      "Iteration 122, loss = 1.11158969\n",
      "Iteration 123, loss = 1.11143591\n",
      "Iteration 124, loss = 1.11128385\n",
      "Iteration 125, loss = 1.11113346\n",
      "Iteration 126, loss = 1.11098472\n",
      "Iteration 127, loss = 1.11083758\n",
      "Iteration 128, loss = 1.11069202\n",
      "Iteration 129, loss = 1.11054801\n",
      "Iteration 130, loss = 1.11040550\n",
      "Iteration 131, loss = 1.11026447\n",
      "Iteration 132, loss = 1.11012489\n",
      "Iteration 133, loss = 1.10998674\n",
      "Iteration 134, loss = 1.10984998\n",
      "Iteration 135, loss = 1.10971458\n",
      "Iteration 136, loss = 1.10958052\n",
      "Iteration 137, loss = 1.10944778\n",
      "Iteration 138, loss = 1.10931633\n",
      "Iteration 139, loss = 1.10918614\n",
      "Iteration 140, loss = 1.10905719\n",
      "Iteration 141, loss = 1.10892946\n",
      "Iteration 142, loss = 1.10880293\n",
      "Iteration 143, loss = 1.10867758\n",
      "Iteration 144, loss = 1.10855337\n",
      "Iteration 145, loss = 1.10843030\n",
      "Iteration 146, loss = 1.10830834\n",
      "Iteration 147, loss = 1.10818748\n",
      "Iteration 148, loss = 1.10806769\n",
      "Iteration 149, loss = 1.10794896\n",
      "Iteration 150, loss = 1.10783126\n",
      "Iteration 151, loss = 1.10771459\n",
      "Iteration 152, loss = 1.10759891\n",
      "Iteration 153, loss = 1.10748423\n",
      "Iteration 154, loss = 1.10737052\n",
      "Iteration 155, loss = 1.10725776\n",
      "Iteration 156, loss = 1.10714594\n",
      "Iteration 157, loss = 1.10703504\n",
      "Iteration 158, loss = 1.10692506\n",
      "Iteration 159, loss = 1.10681596\n",
      "Iteration 160, loss = 1.10670775\n",
      "Iteration 161, loss = 1.10660041\n",
      "Iteration 162, loss = 1.10649392\n",
      "Iteration 163, loss = 1.10638827\n",
      "Iteration 164, loss = 1.10628345\n",
      "Iteration 165, loss = 1.10617944\n",
      "Iteration 166, loss = 1.10607624\n",
      "Iteration 167, loss = 1.10597383\n",
      "Iteration 168, loss = 1.10587219\n",
      "Iteration 169, loss = 1.10577132\n",
      "Iteration 170, loss = 1.10567121\n",
      "Iteration 171, loss = 1.10557184\n",
      "Iteration 172, loss = 1.10547321\n",
      "Iteration 173, loss = 1.10537529\n",
      "Iteration 174, loss = 1.10527809\n",
      "Iteration 175, loss = 1.10518159\n",
      "Iteration 176, loss = 1.10508579\n",
      "Iteration 177, loss = 1.10499066\n",
      "Iteration 178, loss = 1.10489621\n",
      "Iteration 179, loss = 1.10480242\n",
      "Iteration 180, loss = 1.10470928\n",
      "Iteration 181, loss = 1.10461665\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.75478728\n",
      "Iteration 2, loss = 1.75063053\n",
      "Iteration 3, loss = 1.74649328\n",
      "Iteration 4, loss = 1.74237581\n",
      "Iteration 5, loss = 1.73827842\n",
      "Iteration 6, loss = 1.73420138\n",
      "Iteration 7, loss = 1.73014496\n",
      "Iteration 8, loss = 1.72610940\n",
      "Iteration 9, loss = 1.72209495\n",
      "Iteration 10, loss = 1.71810184\n",
      "Iteration 11, loss = 1.71413029\n",
      "Iteration 12, loss = 1.71018050\n",
      "Iteration 13, loss = 1.70625265\n",
      "Iteration 14, loss = 1.70234693\n",
      "Iteration 15, loss = 1.69846349\n",
      "Iteration 16, loss = 1.69460247\n",
      "Iteration 17, loss = 1.69076401\n",
      "Iteration 18, loss = 1.68694821\n",
      "Iteration 19, loss = 1.68315518\n",
      "Iteration 20, loss = 1.67938500\n",
      "Iteration 21, loss = 1.67563773\n",
      "Iteration 22, loss = 1.67191343\n",
      "Iteration 23, loss = 1.66821213\n",
      "Iteration 24, loss = 1.66453386\n",
      "Iteration 25, loss = 1.66087865\n",
      "Iteration 26, loss = 1.65724648\n",
      "Iteration 27, loss = 1.65363735\n",
      "Iteration 28, loss = 1.65005125\n",
      "Iteration 29, loss = 1.64648814\n",
      "Iteration 30, loss = 1.64294801\n",
      "Iteration 31, loss = 1.63943081\n",
      "Iteration 32, loss = 1.63593650\n",
      "Iteration 33, loss = 1.63246502\n",
      "Iteration 34, loss = 1.62901632\n",
      "Iteration 35, loss = 1.62559035\n",
      "Iteration 36, loss = 1.62218704\n",
      "Iteration 37, loss = 1.61880632\n",
      "Iteration 38, loss = 1.61544811\n",
      "Iteration 39, loss = 1.61211235\n",
      "Iteration 40, loss = 1.60879895\n",
      "Iteration 41, loss = 1.60550783\n",
      "Iteration 42, loss = 1.60223890\n",
      "Iteration 43, loss = 1.59899207\n",
      "Iteration 44, loss = 1.59576723\n",
      "Iteration 45, loss = 1.59256430\n",
      "Iteration 46, loss = 1.58938812\n",
      "Iteration 47, loss = 1.58623697\n",
      "Iteration 48, loss = 1.58310760\n",
      "Iteration 49, loss = 1.57999991\n",
      "Iteration 50, loss = 1.57691379\n",
      "Iteration 51, loss = 1.57384914\n",
      "Iteration 52, loss = 1.57080586\n",
      "Iteration 53, loss = 1.56778384\n",
      "Iteration 54, loss = 1.56478296\n",
      "Iteration 55, loss = 1.56180312\n",
      "Iteration 56, loss = 1.55884418\n",
      "Iteration 57, loss = 1.55590604\n",
      "Iteration 58, loss = 1.55298856\n",
      "Iteration 59, loss = 1.55009162\n",
      "Iteration 60, loss = 1.54721507\n",
      "Iteration 61, loss = 1.54435879\n",
      "Iteration 62, loss = 1.54152264\n",
      "Iteration 63, loss = 1.53870671\n",
      "Iteration 64, loss = 1.53591124\n",
      "Iteration 65, loss = 1.53313551\n",
      "Iteration 66, loss = 1.53037937\n",
      "Iteration 67, loss = 1.52764266\n",
      "Iteration 68, loss = 1.52492522\n",
      "Iteration 69, loss = 1.52222691\n",
      "Iteration 70, loss = 1.51954758\n",
      "Iteration 71, loss = 1.51688707\n",
      "Iteration 72, loss = 1.51424522\n",
      "Iteration 73, loss = 1.51162191\n",
      "Iteration 74, loss = 1.50901696\n",
      "Iteration 75, loss = 1.50643025\n",
      "Iteration 76, loss = 1.50386162\n",
      "Iteration 77, loss = 1.50131092\n",
      "Iteration 78, loss = 1.49877802\n",
      "Iteration 79, loss = 1.49626278\n",
      "Iteration 80, loss = 1.49376505\n",
      "Iteration 81, loss = 1.49128470\n",
      "Iteration 82, loss = 1.48882158\n",
      "Iteration 83, loss = 1.48637558\n",
      "Iteration 84, loss = 1.48394655\n",
      "Iteration 85, loss = 1.48153435\n",
      "Iteration 86, loss = 1.47913887\n",
      "Iteration 87, loss = 1.47675997\n",
      "Iteration 88, loss = 1.47439752\n",
      "Iteration 89, loss = 1.47205139\n",
      "Iteration 90, loss = 1.46972146\n",
      "Iteration 91, loss = 1.46740761\n",
      "Iteration 92, loss = 1.46510970\n",
      "Iteration 93, loss = 1.46282762\n",
      "Iteration 94, loss = 1.46056253\n",
      "Iteration 95, loss = 1.45831403\n",
      "Iteration 96, loss = 1.45608109\n",
      "Iteration 97, loss = 1.45386358\n",
      "Iteration 98, loss = 1.45166138\n",
      "Iteration 99, loss = 1.44947435\n",
      "Iteration 100, loss = 1.44730237\n",
      "Iteration 101, loss = 1.44514531\n",
      "Iteration 102, loss = 1.44300305\n",
      "Iteration 103, loss = 1.44087546\n",
      "Iteration 104, loss = 1.43876244\n",
      "Iteration 105, loss = 1.43666384\n",
      "Iteration 106, loss = 1.43457957\n",
      "Iteration 107, loss = 1.43250951\n",
      "Iteration 108, loss = 1.43045353\n",
      "Iteration 109, loss = 1.42841153\n",
      "Iteration 110, loss = 1.42638339\n",
      "Iteration 111, loss = 1.42436900\n",
      "Iteration 112, loss = 1.42236826\n",
      "Iteration 113, loss = 1.42038105\n",
      "Iteration 114, loss = 1.41840727\n",
      "Iteration 115, loss = 1.41644680\n",
      "Iteration 116, loss = 1.41449955\n",
      "Iteration 117, loss = 1.41256541\n",
      "Iteration 118, loss = 1.41064426\n",
      "Iteration 119, loss = 1.40873602\n",
      "Iteration 120, loss = 1.40684057\n",
      "Iteration 121, loss = 1.40495783\n",
      "Iteration 122, loss = 1.40308767\n",
      "Iteration 123, loss = 1.40123001\n",
      "Iteration 124, loss = 1.39938475\n",
      "Iteration 125, loss = 1.39755406\n",
      "Iteration 126, loss = 1.39573951\n",
      "Iteration 127, loss = 1.39393620\n",
      "Iteration 128, loss = 1.39214415\n",
      "Iteration 129, loss = 1.39036339\n",
      "Iteration 130, loss = 1.38859393\n",
      "Iteration 131, loss = 1.38683579\n",
      "Iteration 132, loss = 1.38508898\n",
      "Iteration 133, loss = 1.38335351\n",
      "Iteration 134, loss = 1.38162937\n",
      "Iteration 135, loss = 1.37991655\n",
      "Iteration 136, loss = 1.37821502\n",
      "Iteration 137, loss = 1.37652632\n",
      "Iteration 138, loss = 1.37485075\n",
      "Iteration 139, loss = 1.37318557\n",
      "Iteration 140, loss = 1.37153071\n",
      "Iteration 141, loss = 1.36988609\n",
      "Iteration 142, loss = 1.36825168\n",
      "Iteration 143, loss = 1.36662742\n",
      "Iteration 144, loss = 1.36501644\n",
      "Iteration 145, loss = 1.36341611\n",
      "Iteration 146, loss = 1.36182595\n",
      "Iteration 147, loss = 1.36024593\n",
      "Iteration 148, loss = 1.35867599\n",
      "Iteration 149, loss = 1.35711610\n",
      "Iteration 150, loss = 1.35556621\n",
      "Iteration 151, loss = 1.35402422\n",
      "Iteration 152, loss = 1.35248099\n",
      "Iteration 153, loss = 1.35094569\n",
      "Iteration 154, loss = 1.34942224\n",
      "Iteration 155, loss = 1.34790742\n",
      "Iteration 156, loss = 1.34640131\n",
      "Iteration 157, loss = 1.34490400\n",
      "Iteration 158, loss = 1.34341552\n",
      "Iteration 159, loss = 1.34193592\n",
      "Iteration 160, loss = 1.34046519\n",
      "Iteration 161, loss = 1.33900335\n",
      "Iteration 162, loss = 1.33755037\n",
      "Iteration 163, loss = 1.33610624\n",
      "Iteration 164, loss = 1.33467090\n",
      "Iteration 165, loss = 1.33324434\n",
      "Iteration 166, loss = 1.33182648\n",
      "Iteration 167, loss = 1.33041730\n",
      "Iteration 168, loss = 1.32901672\n",
      "Iteration 169, loss = 1.32762469\n",
      "Iteration 170, loss = 1.32624115\n",
      "Iteration 171, loss = 1.32486604\n",
      "Iteration 172, loss = 1.32349929\n",
      "Iteration 173, loss = 1.32214085\n",
      "Iteration 174, loss = 1.32079064\n",
      "Iteration 175, loss = 1.31944861\n",
      "Iteration 176, loss = 1.31811469\n",
      "Iteration 177, loss = 1.31678883\n",
      "Iteration 178, loss = 1.31547210\n",
      "Iteration 179, loss = 1.31417411\n",
      "Iteration 180, loss = 1.31287461\n",
      "Iteration 181, loss = 1.31159202\n",
      "Iteration 182, loss = 1.31032261\n",
      "Iteration 183, loss = 1.30905825\n",
      "Iteration 184, loss = 1.30779967\n",
      "Iteration 185, loss = 1.30655182\n",
      "Iteration 186, loss = 1.30531538\n",
      "Iteration 187, loss = 1.30408360\n",
      "Iteration 188, loss = 1.30285641\n",
      "Iteration 189, loss = 1.30164413\n",
      "Iteration 190, loss = 1.30043669\n",
      "Iteration 191, loss = 1.29923264\n",
      "Iteration 192, loss = 1.29803425\n",
      "Iteration 193, loss = 1.29684878\n",
      "Iteration 194, loss = 1.29566713\n",
      "Iteration 195, loss = 1.29449883\n",
      "Iteration 196, loss = 1.29333449\n",
      "Iteration 197, loss = 1.29217354\n",
      "Iteration 198, loss = 1.29102160\n",
      "Iteration 199, loss = 1.28987897\n",
      "Iteration 200, loss = 1.28873955\n",
      "Iteration 201, loss = 1.28760968\n",
      "Iteration 202, loss = 1.28648723\n",
      "Iteration 203, loss = 1.28536805\n",
      "Iteration 204, loss = 1.28425861\n",
      "Iteration 205, loss = 1.28315609\n",
      "Iteration 206, loss = 1.28206686\n",
      "Iteration 207, loss = 1.28098180\n",
      "Iteration 208, loss = 1.27990105\n",
      "Iteration 209, loss = 1.27882482\n",
      "Iteration 210, loss = 1.27775331\n",
      "Iteration 211, loss = 1.27669172\n",
      "Iteration 212, loss = 1.27563717\n",
      "Iteration 213, loss = 1.27458425\n",
      "Iteration 214, loss = 1.27354008\n",
      "Iteration 215, loss = 1.27250379\n",
      "Iteration 216, loss = 1.27147177\n",
      "Iteration 217, loss = 1.27044419\n",
      "Iteration 218, loss = 1.26942119\n",
      "Iteration 219, loss = 1.26840963\n",
      "Iteration 220, loss = 1.26740205\n",
      "Iteration 221, loss = 1.26639556\n",
      "Iteration 222, loss = 1.26541562\n",
      "Iteration 223, loss = 1.26442201\n",
      "Iteration 224, loss = 1.26343009\n",
      "Iteration 225, loss = 1.26245747\n",
      "Iteration 226, loss = 1.26148509\n",
      "Iteration 227, loss = 1.26051339\n",
      "Iteration 228, loss = 1.25955744\n",
      "Iteration 229, loss = 1.25860694\n",
      "Iteration 230, loss = 1.25765124\n",
      "Iteration 231, loss = 1.25670096\n",
      "Iteration 232, loss = 1.25576971\n",
      "Iteration 233, loss = 1.25483949\n",
      "Iteration 234, loss = 1.25390901\n",
      "Iteration 235, loss = 1.25297875\n",
      "Iteration 236, loss = 1.25205404\n",
      "Iteration 237, loss = 1.25115461\n",
      "Iteration 238, loss = 1.25024033\n",
      "Iteration 239, loss = 1.24933456\n",
      "Iteration 240, loss = 1.24844343\n",
      "Iteration 241, loss = 1.24755139\n",
      "Iteration 242, loss = 1.24665892\n",
      "Iteration 243, loss = 1.24577504\n",
      "Iteration 244, loss = 1.24490550\n",
      "Iteration 245, loss = 1.24402716\n",
      "Iteration 246, loss = 1.24315779\n",
      "Iteration 247, loss = 1.24229307\n",
      "Iteration 248, loss = 1.24143499\n",
      "Iteration 249, loss = 1.24058160\n",
      "Iteration 250, loss = 1.23973472\n",
      "Iteration 251, loss = 1.23888715\n",
      "Iteration 252, loss = 1.23804854\n",
      "Iteration 253, loss = 1.23721439\n",
      "Iteration 254, loss = 1.23638406\n",
      "Iteration 255, loss = 1.23555765\n",
      "Iteration 256, loss = 1.23474352\n",
      "Iteration 257, loss = 1.23392097\n",
      "Iteration 258, loss = 1.23311881\n",
      "Iteration 259, loss = 1.23231745\n",
      "Iteration 260, loss = 1.23151405\n",
      "Iteration 261, loss = 1.23070918\n",
      "Iteration 262, loss = 1.22990570\n",
      "Iteration 263, loss = 1.22912741\n",
      "Iteration 264, loss = 1.22833727\n",
      "Iteration 265, loss = 1.22755235\n",
      "Iteration 266, loss = 1.22677978\n",
      "Iteration 267, loss = 1.22600495\n",
      "Iteration 268, loss = 1.22522845\n",
      "Iteration 269, loss = 1.22447794\n",
      "Iteration 270, loss = 1.22371960\n",
      "Iteration 271, loss = 1.22294983\n",
      "Iteration 272, loss = 1.22220266\n",
      "Iteration 273, loss = 1.22146269\n",
      "Iteration 274, loss = 1.22071973\n",
      "Iteration 275, loss = 1.21997439\n",
      "Iteration 276, loss = 1.21922727\n",
      "Iteration 277, loss = 1.21849248\n",
      "Iteration 278, loss = 1.21776902\n",
      "Iteration 279, loss = 1.21703743\n",
      "Iteration 280, loss = 1.21631389\n",
      "Iteration 281, loss = 1.21559822\n",
      "Iteration 282, loss = 1.21488375\n",
      "Iteration 283, loss = 1.21416751\n",
      "Iteration 284, loss = 1.21345974\n",
      "Iteration 285, loss = 1.21275679\n",
      "Iteration 286, loss = 1.21205566\n",
      "Iteration 287, loss = 1.21135919\n",
      "Iteration 288, loss = 1.21066623\n",
      "Iteration 289, loss = 1.20997580\n",
      "Iteration 290, loss = 1.20929375\n",
      "Iteration 291, loss = 1.20860983\n",
      "Iteration 292, loss = 1.20792952\n",
      "Iteration 293, loss = 1.20725980\n",
      "Iteration 294, loss = 1.20657994\n",
      "Iteration 295, loss = 1.20592013\n",
      "Iteration 296, loss = 1.20526061\n",
      "Iteration 297, loss = 1.20459744\n",
      "Iteration 298, loss = 1.20393127\n",
      "Iteration 299, loss = 1.20326924\n",
      "Iteration 300, loss = 1.20261999\n",
      "Iteration 301, loss = 1.20196581\n",
      "Iteration 302, loss = 1.20132012\n",
      "Iteration 303, loss = 1.20067793\n",
      "Iteration 304, loss = 1.20003794\n",
      "Iteration 305, loss = 1.19940053\n",
      "Iteration 306, loss = 1.19877159\n",
      "Iteration 307, loss = 1.19813903\n",
      "Iteration 308, loss = 1.19751241\n",
      "Iteration 309, loss = 1.19688587\n",
      "Iteration 310, loss = 1.19626983\n",
      "Iteration 311, loss = 1.19565458\n",
      "Iteration 312, loss = 1.19503557\n",
      "Iteration 313, loss = 1.19441865\n",
      "Iteration 314, loss = 1.19380893\n",
      "Iteration 315, loss = 1.19320694\n",
      "Iteration 316, loss = 1.19260511\n",
      "Iteration 317, loss = 1.19199956\n",
      "Iteration 318, loss = 1.19140629\n",
      "Iteration 319, loss = 1.19081218\n",
      "Iteration 320, loss = 1.19021106\n",
      "Iteration 321, loss = 1.18962259\n",
      "Iteration 322, loss = 1.18903852\n",
      "Iteration 323, loss = 1.18845032\n",
      "Iteration 324, loss = 1.18787203\n",
      "Iteration 325, loss = 1.18729388\n",
      "Iteration 326, loss = 1.18671725\n",
      "Iteration 327, loss = 1.18614724\n",
      "Iteration 328, loss = 1.18557371\n",
      "Iteration 329, loss = 1.18501180\n",
      "Iteration 330, loss = 1.18444992\n",
      "Iteration 331, loss = 1.18388392\n",
      "Iteration 332, loss = 1.18331887\n",
      "Iteration 333, loss = 1.18277038\n",
      "Iteration 334, loss = 1.18221352\n",
      "Iteration 335, loss = 1.18165380\n",
      "Iteration 336, loss = 1.18110671\n",
      "Iteration 337, loss = 1.18055630\n",
      "Iteration 338, loss = 1.18001223\n",
      "Iteration 339, loss = 1.17947149\n",
      "Iteration 340, loss = 1.17893064\n",
      "Iteration 341, loss = 1.17839649\n",
      "Iteration 342, loss = 1.17785909\n",
      "Iteration 343, loss = 1.17733065\n",
      "Iteration 344, loss = 1.17680334\n",
      "Iteration 345, loss = 1.17627174\n",
      "Iteration 346, loss = 1.17574452\n",
      "Iteration 347, loss = 1.17522208\n",
      "Iteration 348, loss = 1.17470041\n",
      "Iteration 349, loss = 1.17418290\n",
      "Iteration 350, loss = 1.17366323\n",
      "Iteration 351, loss = 1.17315346\n",
      "Iteration 352, loss = 1.17263740\n",
      "Iteration 353, loss = 1.17212863\n",
      "Iteration 354, loss = 1.17162226\n",
      "Iteration 355, loss = 1.17112102\n",
      "Iteration 356, loss = 1.17061878\n",
      "Iteration 357, loss = 1.17011417\n",
      "Iteration 358, loss = 1.16962027\n",
      "Iteration 359, loss = 1.16911912\n",
      "Iteration 360, loss = 1.16863510\n",
      "Iteration 361, loss = 1.16814762\n",
      "Iteration 362, loss = 1.16765548\n",
      "Iteration 363, loss = 1.16715935\n",
      "Iteration 364, loss = 1.16668289\n",
      "Iteration 365, loss = 1.16620529\n",
      "Iteration 366, loss = 1.16571851\n",
      "Iteration 367, loss = 1.16522786\n",
      "Iteration 368, loss = 1.16475518\n",
      "Iteration 369, loss = 1.16427780\n",
      "Iteration 370, loss = 1.16380620\n",
      "Iteration 371, loss = 1.16333424\n",
      "Iteration 372, loss = 1.16286547\n",
      "Iteration 373, loss = 1.16239988\n",
      "Iteration 374, loss = 1.16192991\n",
      "Iteration 375, loss = 1.16146775\n",
      "Iteration 376, loss = 1.16100614\n",
      "Iteration 377, loss = 1.16054535\n",
      "Iteration 378, loss = 1.16008916\n",
      "Iteration 379, loss = 1.15963921\n",
      "Iteration 380, loss = 1.15918638\n",
      "Iteration 381, loss = 1.15873346\n",
      "Iteration 382, loss = 1.15828274\n",
      "Iteration 383, loss = 1.15784416\n",
      "Iteration 384, loss = 1.15740087\n",
      "Iteration 385, loss = 1.15695257\n",
      "Iteration 386, loss = 1.15651076\n",
      "Iteration 387, loss = 1.15607308\n",
      "Iteration 388, loss = 1.15563328\n",
      "Iteration 389, loss = 1.15519777\n",
      "Iteration 390, loss = 1.15476721\n",
      "Iteration 391, loss = 1.15433314\n",
      "Iteration 392, loss = 1.15390213\n",
      "Iteration 393, loss = 1.15347376\n",
      "Iteration 394, loss = 1.15305068\n",
      "Iteration 395, loss = 1.15262216\n",
      "Iteration 396, loss = 1.15219746\n",
      "Iteration 397, loss = 1.15177677\n",
      "Iteration 398, loss = 1.15135839\n",
      "Iteration 399, loss = 1.15093662\n",
      "Iteration 400, loss = 1.15051979\n",
      "Iteration 401, loss = 1.15010429\n",
      "Iteration 402, loss = 1.14969014\n",
      "Iteration 403, loss = 1.14927736\n",
      "Iteration 404, loss = 1.14886774\n",
      "Iteration 405, loss = 1.14845617\n",
      "Iteration 406, loss = 1.14805181\n",
      "Iteration 407, loss = 1.14764555\n",
      "Iteration 408, loss = 1.14723988\n",
      "Iteration 409, loss = 1.14683640\n",
      "Iteration 410, loss = 1.14644103\n",
      "Iteration 411, loss = 1.14604304\n",
      "Iteration 412, loss = 1.14563995\n",
      "Iteration 413, loss = 1.14524947\n",
      "Iteration 414, loss = 1.14485877\n",
      "Iteration 415, loss = 1.14446072\n",
      "Iteration 416, loss = 1.14407408\n",
      "Iteration 417, loss = 1.14369095\n",
      "Iteration 418, loss = 1.14330241\n",
      "Iteration 419, loss = 1.14291753\n",
      "Iteration 420, loss = 1.14253612\n",
      "Iteration 421, loss = 1.14215596\n",
      "Iteration 422, loss = 1.14178114\n",
      "Iteration 423, loss = 1.14139981\n",
      "Iteration 424, loss = 1.14102321\n",
      "Iteration 425, loss = 1.14064802\n",
      "Iteration 426, loss = 1.14027405\n",
      "Iteration 427, loss = 1.13990819\n",
      "Iteration 428, loss = 1.13953797\n",
      "Iteration 429, loss = 1.13916252\n",
      "Iteration 430, loss = 1.13879412\n",
      "Iteration 431, loss = 1.13842796\n",
      "Iteration 432, loss = 1.13806293\n",
      "Iteration 433, loss = 1.13770746\n",
      "Iteration 434, loss = 1.13734453\n",
      "Iteration 435, loss = 1.13697516\n",
      "Iteration 436, loss = 1.13661501\n",
      "Iteration 437, loss = 1.13625602\n",
      "Iteration 438, loss = 1.13589820\n",
      "Iteration 439, loss = 1.13554218\n",
      "Iteration 440, loss = 1.13518720\n",
      "Iteration 441, loss = 1.13483370\n",
      "Iteration 442, loss = 1.13448174\n",
      "Iteration 443, loss = 1.13413087\n",
      "Iteration 444, loss = 1.13377465\n",
      "Iteration 445, loss = 1.13340886\n",
      "Iteration 446, loss = 1.13304362\n",
      "Iteration 447, loss = 1.13267912\n",
      "Iteration 448, loss = 1.13231687\n",
      "Iteration 449, loss = 1.13194877\n",
      "Iteration 450, loss = 1.13158315\n",
      "Iteration 451, loss = 1.13121760\n",
      "Iteration 452, loss = 1.13085225\n",
      "Iteration 453, loss = 1.13048720\n",
      "Iteration 454, loss = 1.13012256\n",
      "Iteration 455, loss = 1.12976390\n",
      "Iteration 456, loss = 1.12940673\n",
      "Iteration 457, loss = 1.12905074\n",
      "Iteration 458, loss = 1.12869594\n",
      "Iteration 459, loss = 1.12834232\n",
      "Iteration 460, loss = 1.12798990\n",
      "Iteration 461, loss = 1.12763866\n",
      "Iteration 462, loss = 1.12728862\n",
      "Iteration 463, loss = 1.12693978\n",
      "Iteration 464, loss = 1.12659213\n",
      "Iteration 465, loss = 1.12624567\n",
      "Iteration 466, loss = 1.12590040\n",
      "Iteration 467, loss = 1.12555632\n",
      "Iteration 468, loss = 1.12521342\n",
      "Iteration 469, loss = 1.12487171\n",
      "Iteration 470, loss = 1.12453118\n",
      "Iteration 471, loss = 1.12419183\n",
      "Iteration 472, loss = 1.12385364\n",
      "Iteration 473, loss = 1.12351663\n",
      "Iteration 474, loss = 1.12318078\n",
      "Iteration 475, loss = 1.12284609\n",
      "Iteration 476, loss = 1.12251255\n",
      "Iteration 477, loss = 1.12218017\n",
      "Iteration 478, loss = 1.12184893\n",
      "Iteration 479, loss = 1.12151883\n",
      "Iteration 480, loss = 1.12118986\n",
      "Iteration 481, loss = 1.12086203\n",
      "Iteration 482, loss = 1.12053532\n",
      "Iteration 483, loss = 1.12020974\n",
      "Iteration 484, loss = 1.11988526\n",
      "Iteration 485, loss = 1.11956190\n",
      "Iteration 486, loss = 1.11923965\n",
      "Iteration 487, loss = 1.11891849\n",
      "Iteration 488, loss = 1.11859842\n",
      "Iteration 489, loss = 1.11827945\n",
      "Iteration 490, loss = 1.11796156\n",
      "Iteration 491, loss = 1.11764474\n",
      "Iteration 492, loss = 1.11732900\n",
      "Iteration 493, loss = 1.11701433\n",
      "Iteration 494, loss = 1.11670031\n",
      "Iteration 495, loss = 1.11638704\n",
      "Iteration 496, loss = 1.11607475\n",
      "Iteration 497, loss = 1.11576346\n",
      "Iteration 498, loss = 1.11545315\n",
      "Iteration 499, loss = 1.11514383\n",
      "Iteration 500, loss = 1.11483550\n",
      "Iteration 501, loss = 1.11452816\n",
      "Iteration 502, loss = 1.11422179\n",
      "Iteration 503, loss = 1.11391641\n",
      "Iteration 504, loss = 1.11361201\n",
      "Iteration 505, loss = 1.11330859\n",
      "Iteration 506, loss = 1.11300614\n",
      "Iteration 507, loss = 1.11270466\n",
      "Iteration 508, loss = 1.11240415\n",
      "Iteration 509, loss = 1.11210461\n",
      "Iteration 510, loss = 1.11180602\n",
      "Iteration 511, loss = 1.11150840\n",
      "Iteration 512, loss = 1.11121173\n",
      "Iteration 513, loss = 1.11091601\n",
      "Iteration 514, loss = 1.11062123\n",
      "Iteration 515, loss = 1.11032740\n",
      "Iteration 516, loss = 1.11003451\n",
      "Iteration 517, loss = 1.10974255\n",
      "Iteration 518, loss = 1.10945153\n",
      "Iteration 519, loss = 1.10916143\n",
      "Iteration 520, loss = 1.10887225\n",
      "Iteration 521, loss = 1.10858400\n",
      "Iteration 522, loss = 1.10829665\n",
      "Iteration 523, loss = 1.10801022\n",
      "Iteration 524, loss = 1.10772470\n",
      "Iteration 525, loss = 1.10744007\n",
      "Iteration 526, loss = 1.10715635\n",
      "Iteration 527, loss = 1.10687352\n",
      "Iteration 528, loss = 1.10659158\n",
      "Iteration 529, loss = 1.10631052\n",
      "Iteration 530, loss = 1.10603035\n",
      "Iteration 531, loss = 1.10575106\n",
      "Iteration 532, loss = 1.10547263\n",
      "Iteration 533, loss = 1.10519508\n",
      "Iteration 534, loss = 1.10491839\n",
      "Iteration 535, loss = 1.10464257\n",
      "Iteration 536, loss = 1.10436760\n",
      "Iteration 537, loss = 1.10409349\n",
      "Iteration 538, loss = 1.10382022\n",
      "Iteration 539, loss = 1.10354780\n",
      "Iteration 540, loss = 1.10327622\n",
      "Iteration 541, loss = 1.10300548\n",
      "Iteration 542, loss = 1.10273557\n",
      "Iteration 543, loss = 1.10246650\n",
      "Iteration 544, loss = 1.10219824\n",
      "Iteration 545, loss = 1.10193081\n",
      "Iteration 546, loss = 1.10166420\n",
      "Iteration 547, loss = 1.10139841\n",
      "Iteration 548, loss = 1.10113342\n",
      "Iteration 549, loss = 1.10086924\n",
      "Iteration 550, loss = 1.10060586\n",
      "Iteration 551, loss = 1.10034329\n",
      "Iteration 552, loss = 1.10008151\n",
      "Iteration 553, loss = 1.09982052\n",
      "Iteration 554, loss = 1.09956032\n",
      "Iteration 555, loss = 1.09930090\n",
      "Iteration 556, loss = 1.09904226\n",
      "Iteration 557, loss = 1.09878441\n",
      "Iteration 558, loss = 1.09852732\n",
      "Iteration 559, loss = 1.09827101\n",
      "Iteration 560, loss = 1.09801547\n",
      "Iteration 561, loss = 1.09776068\n",
      "Iteration 562, loss = 1.09750666\n",
      "Iteration 563, loss = 1.09725340\n",
      "Iteration 564, loss = 1.09700264\n",
      "Iteration 565, loss = 1.09675323\n",
      "Iteration 566, loss = 1.09650472\n",
      "Iteration 567, loss = 1.09625710\n",
      "Iteration 568, loss = 1.09601034\n",
      "Iteration 569, loss = 1.09576444\n",
      "Iteration 570, loss = 1.09551937\n",
      "Iteration 571, loss = 1.09527512\n",
      "Iteration 572, loss = 1.09503168\n",
      "Iteration 573, loss = 1.09478903\n",
      "Iteration 574, loss = 1.09454717\n",
      "Iteration 575, loss = 1.09431096\n",
      "Iteration 576, loss = 1.09408001\n",
      "Iteration 577, loss = 1.09385023\n",
      "Iteration 578, loss = 1.09362156\n",
      "Iteration 579, loss = 1.09339395\n",
      "Iteration 580, loss = 1.09316737\n",
      "Iteration 581, loss = 1.09294178\n",
      "Iteration 582, loss = 1.09271714\n",
      "Iteration 583, loss = 1.09249342\n",
      "Iteration 584, loss = 1.09227059\n",
      "Iteration 585, loss = 1.09204864\n",
      "Iteration 586, loss = 1.09182753\n",
      "Iteration 587, loss = 1.09160724\n",
      "Iteration 588, loss = 1.09138776\n",
      "Iteration 589, loss = 1.09116906\n",
      "Iteration 590, loss = 1.09095114\n",
      "Iteration 591, loss = 1.09073398\n",
      "Iteration 592, loss = 1.09051755\n",
      "Iteration 593, loss = 1.09030186\n",
      "Iteration 594, loss = 1.09008689\n",
      "Iteration 595, loss = 1.08987263\n",
      "Iteration 596, loss = 1.08965906\n",
      "Iteration 597, loss = 1.08944619\n",
      "Iteration 598, loss = 1.08923400\n",
      "Iteration 599, loss = 1.08902248\n",
      "Iteration 600, loss = 1.08881163\n",
      "Iteration 601, loss = 1.08860144\n",
      "Iteration 602, loss = 1.08839190\n",
      "Iteration 603, loss = 1.08818301\n",
      "Iteration 604, loss = 1.08797477\n",
      "Iteration 605, loss = 1.08776716\n",
      "Iteration 606, loss = 1.08756018\n",
      "Iteration 607, loss = 1.08735382\n",
      "Iteration 608, loss = 1.08714809\n",
      "Iteration 609, loss = 1.08694298\n",
      "Iteration 610, loss = 1.08673848\n",
      "Iteration 611, loss = 1.08653459\n",
      "Iteration 612, loss = 1.08633131\n",
      "Iteration 613, loss = 1.08612863\n",
      "Iteration 614, loss = 1.08592670\n",
      "Iteration 615, loss = 1.08572580\n",
      "Iteration 616, loss = 1.08552857\n",
      "Iteration 617, loss = 1.08532629\n",
      "Iteration 618, loss = 1.08512764\n",
      "Iteration 619, loss = 1.08492959\n",
      "Iteration 620, loss = 1.08473212\n",
      "Iteration 621, loss = 1.08453524\n",
      "Iteration 622, loss = 1.08433894\n",
      "Iteration 623, loss = 1.08414322\n",
      "Iteration 624, loss = 1.08395068\n",
      "Iteration 625, loss = 1.08375773\n",
      "Iteration 626, loss = 1.08356436\n",
      "Iteration 627, loss = 1.08337350\n",
      "Iteration 628, loss = 1.08318187\n",
      "Iteration 629, loss = 1.08298911\n",
      "Iteration 630, loss = 1.08279876\n",
      "Iteration 631, loss = 1.08260804\n",
      "Iteration 632, loss = 1.08241704\n",
      "Iteration 633, loss = 1.08222820\n",
      "Iteration 634, loss = 1.08203986\n",
      "Iteration 635, loss = 1.08185267\n",
      "Iteration 636, loss = 1.08166586\n",
      "Iteration 637, loss = 1.08147945\n",
      "Iteration 638, loss = 1.08129343\n",
      "Iteration 639, loss = 1.08110783\n",
      "Iteration 640, loss = 1.08092265\n",
      "Iteration 641, loss = 1.08073792\n",
      "Iteration 642, loss = 1.08055382\n",
      "Iteration 643, loss = 1.08037015\n",
      "Iteration 644, loss = 1.08018692\n",
      "Iteration 645, loss = 1.08000482\n",
      "Iteration 646, loss = 1.07982397\n",
      "Iteration 647, loss = 1.07964296\n",
      "Iteration 648, loss = 1.07946158\n",
      "Iteration 649, loss = 1.07928373\n",
      "Iteration 650, loss = 1.07910181\n",
      "Iteration 651, loss = 1.07892307\n",
      "Iteration 652, loss = 1.07874482\n",
      "Iteration 653, loss = 1.07856696\n",
      "Iteration 654, loss = 1.07838946\n",
      "Iteration 655, loss = 1.07821233\n",
      "Iteration 656, loss = 1.07803609\n",
      "Iteration 657, loss = 1.07786002\n",
      "Iteration 658, loss = 1.07768482\n",
      "Iteration 659, loss = 1.07750981\n",
      "Iteration 660, loss = 1.07733559\n",
      "Iteration 661, loss = 1.07716170\n",
      "Iteration 662, loss = 1.07698815\n",
      "Iteration 663, loss = 1.07681573\n",
      "Iteration 664, loss = 1.07664440\n",
      "Iteration 665, loss = 1.07647299\n",
      "Iteration 666, loss = 1.07630118\n",
      "Iteration 667, loss = 1.07612966\n",
      "Iteration 668, loss = 1.07595994\n",
      "Iteration 669, loss = 1.07578997\n",
      "Iteration 670, loss = 1.07562103\n",
      "Iteration 671, loss = 1.07545246\n",
      "Iteration 672, loss = 1.07528401\n",
      "Iteration 673, loss = 1.07511619\n",
      "Iteration 674, loss = 1.07494866\n",
      "Iteration 675, loss = 1.07478190\n",
      "Iteration 676, loss = 1.07461577\n",
      "Iteration 677, loss = 1.07445082\n",
      "Iteration 678, loss = 1.07428754\n",
      "Iteration 679, loss = 1.07412464\n",
      "Iteration 680, loss = 1.07396131\n",
      "Iteration 681, loss = 1.07379764\n",
      "Iteration 682, loss = 1.07363370\n",
      "Iteration 683, loss = 1.07347433\n",
      "Iteration 684, loss = 1.07331268\n",
      "Iteration 685, loss = 1.07315406\n",
      "Iteration 686, loss = 1.07299699\n",
      "Iteration 687, loss = 1.07283955\n",
      "Iteration 688, loss = 1.07268182\n",
      "Iteration 689, loss = 1.07252389\n",
      "Iteration 690, loss = 1.07236791\n",
      "Iteration 691, loss = 1.07221259\n",
      "Iteration 692, loss = 1.07205757\n",
      "Iteration 693, loss = 1.07190285\n",
      "Iteration 694, loss = 1.07174844\n",
      "Iteration 695, loss = 1.07159435\n",
      "Iteration 696, loss = 1.07144219\n",
      "Iteration 697, loss = 1.07128740\n",
      "Iteration 698, loss = 1.07113453\n",
      "Iteration 699, loss = 1.07098236\n",
      "Iteration 700, loss = 1.07083306\n",
      "Iteration 701, loss = 1.07068347\n",
      "Iteration 702, loss = 1.07053352\n",
      "Iteration 703, loss = 1.07038329\n",
      "Iteration 704, loss = 1.07023288\n",
      "Iteration 705, loss = 1.07008379\n",
      "Iteration 706, loss = 1.06993413\n",
      "Iteration 707, loss = 1.06978628\n",
      "Iteration 708, loss = 1.06963819\n",
      "Iteration 709, loss = 1.06949137\n",
      "Iteration 710, loss = 1.06934560\n",
      "Iteration 711, loss = 1.06920010\n",
      "Iteration 712, loss = 1.06905418\n",
      "Iteration 713, loss = 1.06890790\n",
      "Iteration 714, loss = 1.06876135\n",
      "Iteration 715, loss = 1.06861652\n",
      "Iteration 716, loss = 1.06847458\n",
      "Iteration 717, loss = 1.06832841\n",
      "Iteration 718, loss = 1.06818473\n",
      "Iteration 719, loss = 1.06804122\n",
      "Iteration 720, loss = 1.06789814\n",
      "Iteration 721, loss = 1.06775594\n",
      "Iteration 722, loss = 1.06761372\n",
      "Iteration 723, loss = 1.06747247\n",
      "Iteration 724, loss = 1.06733423\n",
      "Iteration 725, loss = 1.06719102\n",
      "Iteration 726, loss = 1.06705175\n",
      "Iteration 727, loss = 1.06691266\n",
      "Iteration 728, loss = 1.06677325\n",
      "Iteration 729, loss = 1.06663358\n",
      "Iteration 730, loss = 1.06649784\n",
      "Iteration 731, loss = 1.06635747\n",
      "Iteration 732, loss = 1.06621991\n",
      "Iteration 733, loss = 1.06608355\n",
      "Iteration 734, loss = 1.06594691\n",
      "Iteration 735, loss = 1.06580998\n",
      "Iteration 736, loss = 1.06567360\n",
      "Iteration 737, loss = 1.06554070\n",
      "Iteration 738, loss = 1.06540265\n",
      "Iteration 739, loss = 1.06526882\n",
      "Iteration 740, loss = 1.06513486\n",
      "Iteration 741, loss = 1.06500056\n",
      "Iteration 742, loss = 1.06486599\n",
      "Iteration 743, loss = 1.06473307\n",
      "Iteration 744, loss = 1.06459948\n",
      "Iteration 745, loss = 1.06446734\n",
      "Iteration 746, loss = 1.06433524\n",
      "Iteration 747, loss = 1.06420303\n",
      "Iteration 748, loss = 1.06407148\n",
      "Iteration 749, loss = 1.06394003\n",
      "Iteration 750, loss = 1.06380894\n",
      "Iteration 751, loss = 1.06367995\n",
      "Iteration 752, loss = 1.06354930\n",
      "Iteration 753, loss = 1.06342029\n",
      "Iteration 754, loss = 1.06329092\n",
      "Iteration 755, loss = 1.06316126\n",
      "Iteration 756, loss = 1.06303259\n",
      "Iteration 757, loss = 1.06290327\n",
      "Iteration 758, loss = 1.06277578\n",
      "Iteration 759, loss = 1.06264847\n",
      "Iteration 760, loss = 1.06252119\n",
      "Iteration 761, loss = 1.06239398\n",
      "Iteration 762, loss = 1.06226707\n",
      "Iteration 763, loss = 1.06214238\n",
      "Iteration 764, loss = 1.06201787\n",
      "Iteration 765, loss = 1.06189292\n",
      "Iteration 766, loss = 1.06176761\n",
      "Iteration 767, loss = 1.06164201\n",
      "Iteration 768, loss = 1.06151619\n",
      "Iteration 769, loss = 1.06139021\n",
      "Iteration 770, loss = 1.06127228\n",
      "Iteration 771, loss = 1.06114787\n",
      "Iteration 772, loss = 1.06101987\n",
      "Iteration 773, loss = 1.06089874\n",
      "Iteration 774, loss = 1.06077742\n",
      "Iteration 775, loss = 1.06065567\n",
      "Iteration 776, loss = 1.06053358\n",
      "Iteration 777, loss = 1.06041121\n",
      "Iteration 778, loss = 1.06028863\n",
      "Iteration 779, loss = 1.06016793\n",
      "Iteration 780, loss = 1.06004747\n",
      "Iteration 781, loss = 1.05992958\n",
      "Iteration 782, loss = 1.05980670\n",
      "Iteration 783, loss = 1.05968643\n",
      "Iteration 784, loss = 1.05956629\n",
      "Iteration 785, loss = 1.05944757\n",
      "Iteration 786, loss = 1.05932851\n",
      "Iteration 787, loss = 1.05920920\n",
      "Iteration 788, loss = 1.05909534\n",
      "Iteration 789, loss = 1.05897514\n",
      "Iteration 790, loss = 1.05885716\n",
      "Iteration 791, loss = 1.05874118\n",
      "Iteration 792, loss = 1.05862479\n",
      "Iteration 793, loss = 1.05850804\n",
      "Iteration 794, loss = 1.05839101\n",
      "Iteration 795, loss = 1.05827377\n",
      "Iteration 796, loss = 1.05815866\n",
      "Iteration 797, loss = 1.05804430\n",
      "Iteration 798, loss = 1.05792841\n",
      "Iteration 799, loss = 1.05781327\n",
      "Iteration 800, loss = 1.05769811\n",
      "Iteration 801, loss = 1.05758419\n",
      "Iteration 802, loss = 1.05747045\n",
      "Iteration 803, loss = 1.05735640\n",
      "Iteration 804, loss = 1.05724208\n",
      "Iteration 805, loss = 1.05713125\n",
      "Iteration 806, loss = 1.05701804\n",
      "Iteration 807, loss = 1.05690365\n",
      "Iteration 808, loss = 1.05679207\n",
      "Iteration 809, loss = 1.05668013\n",
      "Iteration 810, loss = 1.05656818\n",
      "Iteration 811, loss = 1.05645700\n",
      "Iteration 812, loss = 1.05634569\n",
      "Iteration 813, loss = 1.05623430\n",
      "Iteration 814, loss = 1.05612330\n",
      "Iteration 815, loss = 1.05601457\n",
      "Iteration 816, loss = 1.05590327\n",
      "Iteration 817, loss = 1.05579371\n",
      "Iteration 818, loss = 1.05568466\n",
      "Iteration 819, loss = 1.05557560\n",
      "Iteration 820, loss = 1.05546697\n",
      "Iteration 821, loss = 1.05535802\n",
      "Iteration 822, loss = 1.05524879\n",
      "Iteration 823, loss = 1.05514338\n",
      "Iteration 824, loss = 1.05503362\n",
      "Iteration 825, loss = 1.05492594\n",
      "Iteration 826, loss = 1.05481917\n",
      "Iteration 827, loss = 1.05471233\n",
      "Iteration 828, loss = 1.05460516\n",
      "Iteration 829, loss = 1.05449772\n",
      "Iteration 830, loss = 1.05439491\n",
      "Iteration 831, loss = 1.05428656\n",
      "Iteration 832, loss = 1.05418112\n",
      "Iteration 833, loss = 1.05407691\n",
      "Iteration 834, loss = 1.05397226\n",
      "Iteration 835, loss = 1.05386723\n",
      "Iteration 836, loss = 1.05376189\n",
      "Iteration 837, loss = 1.05365631\n",
      "Iteration 838, loss = 1.05355243\n",
      "Iteration 839, loss = 1.05345108\n",
      "Iteration 840, loss = 1.05334471\n",
      "Iteration 841, loss = 1.05324165\n",
      "Iteration 842, loss = 1.05314037\n",
      "Iteration 843, loss = 1.05303858\n",
      "Iteration 844, loss = 1.05293634\n",
      "Iteration 845, loss = 1.05283372\n",
      "Iteration 846, loss = 1.05273078\n",
      "Iteration 847, loss = 1.05262759\n",
      "Iteration 848, loss = 1.05252421\n",
      "Iteration 849, loss = 1.05242295\n",
      "Iteration 850, loss = 1.05232178\n",
      "Iteration 851, loss = 1.05222041\n",
      "Iteration 852, loss = 1.05211884\n",
      "Iteration 853, loss = 1.05201821\n",
      "Iteration 854, loss = 1.05191945\n",
      "Iteration 855, loss = 1.05182031\n",
      "Iteration 856, loss = 1.05172075\n",
      "Iteration 857, loss = 1.05162082\n",
      "Iteration 858, loss = 1.05152058\n",
      "Iteration 859, loss = 1.05142356\n",
      "Iteration 860, loss = 1.05132438\n",
      "Iteration 861, loss = 1.05122419\n",
      "Iteration 862, loss = 1.05112655\n",
      "Iteration 863, loss = 1.05102849\n",
      "Iteration 864, loss = 1.05093008\n",
      "Iteration 865, loss = 1.05083138\n",
      "Iteration 866, loss = 1.05073557\n",
      "Iteration 867, loss = 1.05063649\n",
      "Iteration 868, loss = 1.05054083\n",
      "Iteration 869, loss = 1.05044526\n",
      "Iteration 870, loss = 1.05034922\n",
      "Iteration 871, loss = 1.05025278\n",
      "Iteration 872, loss = 1.05015600\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.57285675\n",
      "Iteration 2, loss = 1.57196455\n",
      "Iteration 3, loss = 1.57107530\n",
      "Iteration 4, loss = 1.57018903\n",
      "Iteration 5, loss = 1.56930577\n",
      "Iteration 6, loss = 1.56842556\n",
      "Iteration 7, loss = 1.56754839\n",
      "Iteration 8, loss = 1.56667427\n",
      "Iteration 9, loss = 1.56580316\n",
      "Iteration 10, loss = 1.56493504\n",
      "Iteration 11, loss = 1.56406983\n",
      "Iteration 12, loss = 1.56320743\n",
      "Iteration 13, loss = 1.56234771\n",
      "Iteration 14, loss = 1.56149050\n",
      "Iteration 15, loss = 1.56063560\n",
      "Iteration 16, loss = 1.55978274\n",
      "Iteration 17, loss = 1.55893162\n",
      "Iteration 18, loss = 1.55808192\n",
      "Iteration 19, loss = 1.55723327\n",
      "Iteration 20, loss = 1.55638530\n",
      "Iteration 21, loss = 1.55553761\n",
      "Iteration 22, loss = 1.55468985\n",
      "Iteration 23, loss = 1.55384168\n",
      "Iteration 24, loss = 1.55299277\n",
      "Iteration 25, loss = 1.55214287\n",
      "Iteration 26, loss = 1.55129173\n",
      "Iteration 27, loss = 1.55043914\n",
      "Iteration 28, loss = 1.54958495\n",
      "Iteration 29, loss = 1.54872898\n",
      "Iteration 30, loss = 1.54787113\n",
      "Iteration 31, loss = 1.54701126\n",
      "Iteration 32, loss = 1.54614928\n",
      "Iteration 33, loss = 1.54528509\n",
      "Iteration 34, loss = 1.54441860\n",
      "Iteration 35, loss = 1.54354974\n",
      "Iteration 36, loss = 1.54267842\n",
      "Iteration 37, loss = 1.54180457\n",
      "Iteration 38, loss = 1.54092812\n",
      "Iteration 39, loss = 1.54004900\n",
      "Iteration 40, loss = 1.53916715\n",
      "Iteration 41, loss = 1.53828250\n",
      "Iteration 42, loss = 1.53739499\n",
      "Iteration 43, loss = 1.53650456\n",
      "Iteration 44, loss = 1.53561116\n",
      "Iteration 45, loss = 1.53471474\n",
      "Iteration 46, loss = 1.53381525\n",
      "Iteration 47, loss = 1.53291264\n",
      "Iteration 48, loss = 1.53200686\n",
      "Iteration 49, loss = 1.53109788\n",
      "Iteration 50, loss = 1.53018565\n",
      "Iteration 51, loss = 1.52927015\n",
      "Iteration 52, loss = 1.52835133\n",
      "Iteration 53, loss = 1.52742916\n",
      "Iteration 54, loss = 1.52650362\n",
      "Iteration 55, loss = 1.52557467\n",
      "Iteration 56, loss = 1.52464231\n",
      "Iteration 57, loss = 1.52370649\n",
      "Iteration 58, loss = 1.52276721\n",
      "Iteration 59, loss = 1.52182445\n",
      "Iteration 60, loss = 1.52087818\n",
      "Iteration 61, loss = 1.51992841\n",
      "Iteration 62, loss = 1.51897512\n",
      "Iteration 63, loss = 1.51801830\n",
      "Iteration 64, loss = 1.51705794\n",
      "Iteration 65, loss = 1.51609404\n",
      "Iteration 66, loss = 1.51512661\n",
      "Iteration 67, loss = 1.51415563\n",
      "Iteration 68, loss = 1.51318111\n",
      "Iteration 69, loss = 1.51220307\n",
      "Iteration 70, loss = 1.51122150\n",
      "Iteration 71, loss = 1.51023641\n",
      "Iteration 72, loss = 1.50924782\n",
      "Iteration 73, loss = 1.50825436\n",
      "Iteration 74, loss = 1.50725596\n",
      "Iteration 75, loss = 1.50625359\n",
      "Iteration 76, loss = 1.50524746\n",
      "Iteration 77, loss = 1.50423763\n",
      "Iteration 78, loss = 1.50322414\n",
      "Iteration 79, loss = 1.50220704\n",
      "Iteration 80, loss = 1.50118637\n",
      "Iteration 81, loss = 1.50016220\n",
      "Iteration 82, loss = 1.49913458\n",
      "Iteration 83, loss = 1.49810354\n",
      "Iteration 84, loss = 1.49706915\n",
      "Iteration 85, loss = 1.49603145\n",
      "Iteration 86, loss = 1.49499049\n",
      "Iteration 87, loss = 1.49394633\n",
      "Iteration 88, loss = 1.49289900\n",
      "Iteration 89, loss = 1.49184858\n",
      "Iteration 90, loss = 1.49079511\n",
      "Iteration 91, loss = 1.48973866\n",
      "Iteration 92, loss = 1.48867927\n",
      "Iteration 93, loss = 1.48761702\n",
      "Iteration 94, loss = 1.48655197\n",
      "Iteration 95, loss = 1.48548418\n",
      "Iteration 96, loss = 1.48441372\n",
      "Iteration 97, loss = 1.48334066\n",
      "Iteration 98, loss = 1.48226507\n",
      "Iteration 99, loss = 1.48118703\n",
      "Iteration 100, loss = 1.48010660\n",
      "Iteration 101, loss = 1.47902388\n",
      "Iteration 102, loss = 1.47793893\n",
      "Iteration 103, loss = 1.47685185\n",
      "Iteration 104, loss = 1.47576271\n",
      "Iteration 105, loss = 1.47467160\n",
      "Iteration 106, loss = 1.47357861\n",
      "Iteration 107, loss = 1.47248382\n",
      "Iteration 108, loss = 1.47138734\n",
      "Iteration 109, loss = 1.47028925\n",
      "Iteration 110, loss = 1.46918964\n",
      "Iteration 111, loss = 1.46808862\n",
      "Iteration 112, loss = 1.46698628\n",
      "Iteration 113, loss = 1.46588273\n",
      "Iteration 114, loss = 1.46477805\n",
      "Iteration 115, loss = 1.46367235\n",
      "Iteration 116, loss = 1.46256412\n",
      "Iteration 117, loss = 1.46145047\n",
      "Iteration 118, loss = 1.46033548\n",
      "Iteration 119, loss = 1.45921932\n",
      "Iteration 120, loss = 1.45810216\n",
      "Iteration 121, loss = 1.45698416\n",
      "Iteration 122, loss = 1.45586548\n",
      "Iteration 123, loss = 1.45474625\n",
      "Iteration 124, loss = 1.45362661\n",
      "Iteration 125, loss = 1.45250670\n",
      "Iteration 126, loss = 1.45138663\n",
      "Iteration 127, loss = 1.45026654\n",
      "Iteration 128, loss = 1.44914654\n",
      "Iteration 129, loss = 1.44802675\n",
      "Iteration 130, loss = 1.44690727\n",
      "Iteration 131, loss = 1.44578823\n",
      "Iteration 132, loss = 1.44466973\n",
      "Iteration 133, loss = 1.44353390\n",
      "Iteration 134, loss = 1.44238451\n",
      "Iteration 135, loss = 1.44122635\n",
      "Iteration 136, loss = 1.44006249\n",
      "Iteration 137, loss = 1.43889464\n",
      "Iteration 138, loss = 1.43772398\n",
      "Iteration 139, loss = 1.43655135\n",
      "Iteration 140, loss = 1.43537179\n",
      "Iteration 141, loss = 1.43418860\n",
      "Iteration 142, loss = 1.43300434\n",
      "Iteration 143, loss = 1.43181945\n",
      "Iteration 144, loss = 1.43063430\n",
      "Iteration 145, loss = 1.42944924\n",
      "Iteration 146, loss = 1.42826452\n",
      "Iteration 147, loss = 1.42708038\n",
      "Iteration 148, loss = 1.42589703\n",
      "Iteration 149, loss = 1.42471544\n",
      "Iteration 150, loss = 1.42352674\n",
      "Iteration 151, loss = 1.42233869\n",
      "Iteration 152, loss = 1.42115147\n",
      "Iteration 153, loss = 1.41992873\n",
      "Iteration 154, loss = 1.41865409\n",
      "Iteration 155, loss = 1.41741654\n",
      "Iteration 156, loss = 1.41618807\n",
      "Iteration 157, loss = 1.41495360\n",
      "Iteration 158, loss = 1.41371874\n",
      "Iteration 159, loss = 1.41248389\n",
      "Iteration 160, loss = 1.41124938\n",
      "Iteration 161, loss = 1.41001547\n",
      "Iteration 162, loss = 1.40878235\n",
      "Iteration 163, loss = 1.40755019\n",
      "Iteration 164, loss = 1.40631913\n",
      "Iteration 165, loss = 1.40508926\n",
      "Iteration 166, loss = 1.40386068\n",
      "Iteration 167, loss = 1.40262320\n",
      "Iteration 168, loss = 1.40138552\n",
      "Iteration 169, loss = 1.40014474\n",
      "Iteration 170, loss = 1.39895455\n",
      "Iteration 171, loss = 1.39778160\n",
      "Iteration 172, loss = 1.39660836\n",
      "Iteration 173, loss = 1.39543507\n",
      "Iteration 174, loss = 1.39425814\n",
      "Iteration 175, loss = 1.39307139\n",
      "Iteration 176, loss = 1.39188391\n",
      "Iteration 177, loss = 1.39069596\n",
      "Iteration 178, loss = 1.38950774\n",
      "Iteration 179, loss = 1.38830550\n",
      "Iteration 180, loss = 1.38709759\n",
      "Iteration 181, loss = 1.38588863\n",
      "Iteration 182, loss = 1.38467892\n",
      "Iteration 183, loss = 1.38346869\n",
      "Iteration 184, loss = 1.38225816\n",
      "Iteration 185, loss = 1.38104760\n",
      "Iteration 186, loss = 1.37984136\n",
      "Iteration 187, loss = 1.37863554\n",
      "Iteration 188, loss = 1.37743020\n",
      "Iteration 189, loss = 1.37622745\n",
      "Iteration 190, loss = 1.37502765\n",
      "Iteration 191, loss = 1.37383192\n",
      "Iteration 192, loss = 1.37263739\n",
      "Iteration 193, loss = 1.37144404\n",
      "Iteration 194, loss = 1.37025183\n",
      "Iteration 195, loss = 1.36906075\n",
      "Iteration 196, loss = 1.36787077\n",
      "Iteration 197, loss = 1.36668187\n",
      "Iteration 198, loss = 1.36549403\n",
      "Iteration 199, loss = 1.36430724\n",
      "Iteration 200, loss = 1.36312147\n",
      "Iteration 201, loss = 1.36193672\n",
      "Iteration 202, loss = 1.36075297\n",
      "Iteration 203, loss = 1.35957021\n",
      "Iteration 204, loss = 1.35838843\n",
      "Iteration 205, loss = 1.35720762\n",
      "Iteration 206, loss = 1.35602777\n",
      "Iteration 207, loss = 1.35484888\n",
      "Iteration 208, loss = 1.35367095\n",
      "Iteration 209, loss = 1.35249397\n",
      "Iteration 210, loss = 1.35131795\n",
      "Iteration 211, loss = 1.35014288\n",
      "Iteration 212, loss = 1.34896877\n",
      "Iteration 213, loss = 1.34779562\n",
      "Iteration 214, loss = 1.34662345\n",
      "Iteration 215, loss = 1.34545224\n",
      "Iteration 216, loss = 1.34428389\n",
      "Iteration 217, loss = 1.34311878\n",
      "Iteration 218, loss = 1.34195506\n",
      "Iteration 219, loss = 1.34079286\n",
      "Iteration 220, loss = 1.33963570\n",
      "Iteration 221, loss = 1.33845486\n",
      "Iteration 222, loss = 1.33727005\n",
      "Iteration 223, loss = 1.33605858\n",
      "Iteration 224, loss = 1.33484042\n",
      "Iteration 225, loss = 1.33361816\n",
      "Iteration 226, loss = 1.33239267\n",
      "Iteration 227, loss = 1.33117546\n",
      "Iteration 228, loss = 1.32995772\n",
      "Iteration 229, loss = 1.32873983\n",
      "Iteration 230, loss = 1.32752213\n",
      "Iteration 231, loss = 1.32630495\n",
      "Iteration 232, loss = 1.32508854\n",
      "Iteration 233, loss = 1.32387315\n",
      "Iteration 234, loss = 1.32265901\n",
      "Iteration 235, loss = 1.32144631\n",
      "Iteration 236, loss = 1.32023523\n",
      "Iteration 237, loss = 1.31902594\n",
      "Iteration 238, loss = 1.31781857\n",
      "Iteration 239, loss = 1.31661326\n",
      "Iteration 240, loss = 1.31541014\n",
      "Iteration 241, loss = 1.31419869\n",
      "Iteration 242, loss = 1.31298284\n",
      "Iteration 243, loss = 1.31176810\n",
      "Iteration 244, loss = 1.31055471\n",
      "Iteration 245, loss = 1.30934290\n",
      "Iteration 246, loss = 1.30813287\n",
      "Iteration 247, loss = 1.30692482\n",
      "Iteration 248, loss = 1.30571681\n",
      "Iteration 249, loss = 1.30449495\n",
      "Iteration 250, loss = 1.30327315\n",
      "Iteration 251, loss = 1.30202680\n",
      "Iteration 252, loss = 1.30076955\n",
      "Iteration 253, loss = 1.29951094\n",
      "Iteration 254, loss = 1.29825158\n",
      "Iteration 255, loss = 1.29699203\n",
      "Iteration 256, loss = 1.29573276\n",
      "Iteration 257, loss = 1.29447421\n",
      "Iteration 258, loss = 1.29321675\n",
      "Iteration 259, loss = 1.29196074\n",
      "Iteration 260, loss = 1.29068781\n",
      "Iteration 261, loss = 1.28940145\n",
      "Iteration 262, loss = 1.28812151\n",
      "Iteration 263, loss = 1.28684247\n",
      "Iteration 264, loss = 1.28556490\n",
      "Iteration 265, loss = 1.28428932\n",
      "Iteration 266, loss = 1.28301618\n",
      "Iteration 267, loss = 1.28174589\n",
      "Iteration 268, loss = 1.28047880\n",
      "Iteration 269, loss = 1.27921523\n",
      "Iteration 270, loss = 1.27794075\n",
      "Iteration 271, loss = 1.27666514\n",
      "Iteration 272, loss = 1.27539234\n",
      "Iteration 273, loss = 1.27412273\n",
      "Iteration 274, loss = 1.27285666\n",
      "Iteration 275, loss = 1.27159445\n",
      "Iteration 276, loss = 1.27033082\n",
      "Iteration 277, loss = 1.26903967\n",
      "Iteration 278, loss = 1.26775050\n",
      "Iteration 279, loss = 1.26646402\n",
      "Iteration 280, loss = 1.26518084\n",
      "Iteration 281, loss = 1.26390152\n",
      "Iteration 282, loss = 1.26262101\n",
      "Iteration 283, loss = 1.26132952\n",
      "Iteration 284, loss = 1.26004618\n",
      "Iteration 285, loss = 1.25877267\n",
      "Iteration 286, loss = 1.25747813\n",
      "Iteration 287, loss = 1.25618755\n",
      "Iteration 288, loss = 1.25490158\n",
      "Iteration 289, loss = 1.25362076\n",
      "Iteration 290, loss = 1.25234558\n",
      "Iteration 291, loss = 1.25106139\n",
      "Iteration 292, loss = 1.24976018\n",
      "Iteration 293, loss = 1.24847011\n",
      "Iteration 294, loss = 1.24720052\n",
      "Iteration 295, loss = 1.24593831\n",
      "Iteration 296, loss = 1.24468385\n",
      "Iteration 297, loss = 1.24342177\n",
      "Iteration 298, loss = 1.24214455\n",
      "Iteration 299, loss = 1.24086886\n",
      "Iteration 300, loss = 1.23956209\n",
      "Iteration 301, loss = 1.23822354\n",
      "Iteration 302, loss = 1.23685388\n",
      "Iteration 303, loss = 1.23547149\n",
      "Iteration 304, loss = 1.23408667\n",
      "Iteration 305, loss = 1.23270562\n",
      "Iteration 306, loss = 1.23132977\n",
      "Iteration 307, loss = 1.22996035\n",
      "Iteration 308, loss = 1.22859839\n",
      "Iteration 309, loss = 1.22724476\n",
      "Iteration 310, loss = 1.22590022\n",
      "Iteration 311, loss = 1.22461024\n",
      "Iteration 312, loss = 1.22333088\n",
      "Iteration 313, loss = 1.22205401\n",
      "Iteration 314, loss = 1.22079088\n",
      "Iteration 315, loss = 1.21955152\n",
      "Iteration 316, loss = 1.21832576\n",
      "Iteration 317, loss = 1.21711357\n",
      "Iteration 318, loss = 1.21591492\n",
      "Iteration 319, loss = 1.21472976\n",
      "Iteration 320, loss = 1.21355804\n",
      "Iteration 321, loss = 1.21239971\n",
      "Iteration 322, loss = 1.21125470\n",
      "Iteration 323, loss = 1.21012295\n",
      "Iteration 324, loss = 1.20900439\n",
      "Iteration 325, loss = 1.20789893\n",
      "Iteration 326, loss = 1.20680651\n",
      "Iteration 327, loss = 1.20572702\n",
      "Iteration 328, loss = 1.20466038\n",
      "Iteration 329, loss = 1.20360651\n",
      "Iteration 330, loss = 1.20256529\n",
      "Iteration 331, loss = 1.20153664\n",
      "Iteration 332, loss = 1.20052045\n",
      "Iteration 333, loss = 1.19951662\n",
      "Iteration 334, loss = 1.19853484\n",
      "Iteration 335, loss = 1.19759038\n",
      "Iteration 336, loss = 1.19665315\n",
      "Iteration 337, loss = 1.19571151\n",
      "Iteration 338, loss = 1.19476237\n",
      "Iteration 339, loss = 1.19381689\n",
      "Iteration 340, loss = 1.19288186\n",
      "Iteration 341, loss = 1.19195951\n",
      "Iteration 342, loss = 1.19105117\n",
      "Iteration 343, loss = 1.19015460\n",
      "Iteration 344, loss = 1.18927246\n",
      "Iteration 345, loss = 1.18840168\n",
      "Iteration 346, loss = 1.18754218\n",
      "Iteration 347, loss = 1.18669391\n",
      "Iteration 348, loss = 1.18585676\n",
      "Iteration 349, loss = 1.18503064\n",
      "Iteration 350, loss = 1.18421543\n",
      "Iteration 351, loss = 1.18341103\n",
      "Iteration 352, loss = 1.18261732\n",
      "Iteration 353, loss = 1.18183415\n",
      "Iteration 354, loss = 1.18106140\n",
      "Iteration 355, loss = 1.18029893\n",
      "Iteration 356, loss = 1.17954660\n",
      "Iteration 357, loss = 1.17880426\n",
      "Iteration 358, loss = 1.17807175\n",
      "Iteration 359, loss = 1.17734894\n",
      "Iteration 360, loss = 1.17663566\n",
      "Iteration 361, loss = 1.17593509\n",
      "Iteration 362, loss = 1.17524414\n",
      "Iteration 363, loss = 1.17454629\n",
      "Iteration 364, loss = 1.17385250\n",
      "Iteration 365, loss = 1.17316625\n",
      "Iteration 366, loss = 1.17248762\n",
      "Iteration 367, loss = 1.17179981\n",
      "Iteration 368, loss = 1.17111462\n",
      "Iteration 369, loss = 1.17043056\n",
      "Iteration 370, loss = 1.16973514\n",
      "Iteration 371, loss = 1.16904436\n",
      "Iteration 372, loss = 1.16835657\n",
      "Iteration 373, loss = 1.16765307\n",
      "Iteration 374, loss = 1.16695353\n",
      "Iteration 375, loss = 1.16625866\n",
      "Iteration 376, loss = 1.16556906\n",
      "Iteration 377, loss = 1.16488526\n",
      "Iteration 378, loss = 1.16420770\n",
      "Iteration 379, loss = 1.16353672\n",
      "Iteration 380, loss = 1.16287263\n",
      "Iteration 381, loss = 1.16221568\n",
      "Iteration 382, loss = 1.16156972\n",
      "Iteration 383, loss = 1.16096064\n",
      "Iteration 384, loss = 1.16036131\n",
      "Iteration 385, loss = 1.15977147\n",
      "Iteration 386, loss = 1.15919088\n",
      "Iteration 387, loss = 1.15861930\n",
      "Iteration 388, loss = 1.15805649\n",
      "Iteration 389, loss = 1.15750225\n",
      "Iteration 390, loss = 1.15695635\n",
      "Iteration 391, loss = 1.15641858\n",
      "Iteration 392, loss = 1.15588875\n",
      "Iteration 393, loss = 1.15536664\n",
      "Iteration 394, loss = 1.15486149\n",
      "Iteration 395, loss = 1.15436721\n",
      "Iteration 396, loss = 1.15388047\n",
      "Iteration 397, loss = 1.15339390\n",
      "Iteration 398, loss = 1.15290375\n",
      "Iteration 399, loss = 1.15241951\n",
      "Iteration 400, loss = 1.15194115\n",
      "Iteration 401, loss = 1.15146861\n",
      "Iteration 402, loss = 1.15100184\n",
      "Iteration 403, loss = 1.15054078\n",
      "Iteration 404, loss = 1.15008535\n",
      "Iteration 405, loss = 1.14963550\n",
      "Iteration 406, loss = 1.14919113\n",
      "Iteration 407, loss = 1.14875217\n",
      "Iteration 408, loss = 1.14831854\n",
      "Iteration 409, loss = 1.14789016\n",
      "Iteration 410, loss = 1.14746393\n",
      "Iteration 411, loss = 1.14702807\n",
      "Iteration 412, loss = 1.14659316\n",
      "Iteration 413, loss = 1.14616174\n",
      "Iteration 414, loss = 1.14573396\n",
      "Iteration 415, loss = 1.14530994\n",
      "Iteration 416, loss = 1.14488977\n",
      "Iteration 417, loss = 1.14447352\n",
      "Iteration 418, loss = 1.14406126\n",
      "Iteration 419, loss = 1.14365303\n",
      "Iteration 420, loss = 1.14324885\n",
      "Iteration 421, loss = 1.14284873\n",
      "Iteration 422, loss = 1.14245270\n",
      "Iteration 423, loss = 1.14206073\n",
      "Iteration 424, loss = 1.14167282\n",
      "Iteration 425, loss = 1.14128895\n",
      "Iteration 426, loss = 1.14093565\n",
      "Iteration 427, loss = 1.14058886\n",
      "Iteration 428, loss = 1.14024718\n",
      "Iteration 429, loss = 1.13991036\n",
      "Iteration 430, loss = 1.13957817\n",
      "Iteration 431, loss = 1.13925040\n",
      "Iteration 432, loss = 1.13892687\n",
      "Iteration 433, loss = 1.13860742\n",
      "Iteration 434, loss = 1.13829188\n",
      "Iteration 435, loss = 1.13798012\n",
      "Iteration 436, loss = 1.13767200\n",
      "Iteration 437, loss = 1.13736741\n",
      "Iteration 438, loss = 1.13706622\n",
      "Iteration 439, loss = 1.13676833\n",
      "Iteration 440, loss = 1.13647365\n",
      "Iteration 441, loss = 1.13618209\n",
      "Iteration 442, loss = 1.13589136\n",
      "Iteration 443, loss = 1.13559971\n",
      "Iteration 444, loss = 1.13531047\n",
      "Iteration 445, loss = 1.13502362\n",
      "Iteration 446, loss = 1.13473915\n",
      "Iteration 447, loss = 1.13445701\n",
      "Iteration 448, loss = 1.13417720\n",
      "Iteration 449, loss = 1.13389967\n",
      "Iteration 450, loss = 1.13362441\n",
      "Iteration 451, loss = 1.13335139\n",
      "Iteration 452, loss = 1.13307509\n",
      "Iteration 453, loss = 1.13280004\n",
      "Iteration 454, loss = 1.13252662\n",
      "Iteration 455, loss = 1.13225489\n",
      "Iteration 456, loss = 1.13198491\n",
      "Iteration 457, loss = 1.13171672\n",
      "Iteration 458, loss = 1.13145035\n",
      "Iteration 459, loss = 1.13118584\n",
      "Iteration 460, loss = 1.13092320\n",
      "Iteration 461, loss = 1.13066246\n",
      "Iteration 462, loss = 1.13040362\n",
      "Iteration 463, loss = 1.13014669\n",
      "Iteration 464, loss = 1.12989167\n",
      "Iteration 465, loss = 1.12963856\n",
      "Iteration 466, loss = 1.12938735\n",
      "Iteration 467, loss = 1.12913805\n",
      "Iteration 468, loss = 1.12889063\n",
      "Iteration 469, loss = 1.12864508\n",
      "Iteration 470, loss = 1.12840140\n",
      "Iteration 471, loss = 1.12815957\n",
      "Iteration 472, loss = 1.12791956\n",
      "Iteration 473, loss = 1.12768136\n",
      "Iteration 474, loss = 1.12744495\n",
      "Iteration 475, loss = 1.12721030\n",
      "Iteration 476, loss = 1.12697741\n",
      "Iteration 477, loss = 1.12674403\n",
      "Iteration 478, loss = 1.12650847\n",
      "Iteration 479, loss = 1.12627410\n",
      "Iteration 480, loss = 1.12604098\n",
      "Iteration 481, loss = 1.12580917\n",
      "Iteration 482, loss = 1.12557871\n",
      "Iteration 483, loss = 1.12534965\n",
      "Iteration 484, loss = 1.12512201\n",
      "Iteration 485, loss = 1.12489582\n",
      "Iteration 486, loss = 1.12467111\n",
      "Iteration 487, loss = 1.12444787\n",
      "Iteration 488, loss = 1.12422613\n",
      "Iteration 489, loss = 1.12400589\n",
      "Iteration 490, loss = 1.12378715\n",
      "Iteration 491, loss = 1.12356991\n",
      "Iteration 492, loss = 1.12335416\n",
      "Iteration 493, loss = 1.12313990\n",
      "Iteration 494, loss = 1.12292712\n",
      "Iteration 495, loss = 1.12271581\n",
      "Iteration 496, loss = 1.12250595\n",
      "Iteration 497, loss = 1.12229753\n",
      "Iteration 498, loss = 1.12209054\n",
      "Iteration 499, loss = 1.12188496\n",
      "Iteration 500, loss = 1.12168077\n",
      "Iteration 501, loss = 1.12147795\n",
      "Iteration 502, loss = 1.12127649\n",
      "Iteration 503, loss = 1.12107636\n",
      "Iteration 504, loss = 1.12087756\n",
      "Iteration 505, loss = 1.12068005\n",
      "Iteration 506, loss = 1.12048382\n",
      "Iteration 507, loss = 1.12028885\n",
      "Iteration 508, loss = 1.12009511\n",
      "Iteration 509, loss = 1.11990260\n",
      "Iteration 510, loss = 1.11971129\n",
      "Iteration 511, loss = 1.11952116\n",
      "Iteration 512, loss = 1.11933219\n",
      "Iteration 513, loss = 1.11914437\n",
      "Iteration 514, loss = 1.11895766\n",
      "Iteration 515, loss = 1.11877207\n",
      "Iteration 516, loss = 1.11858756\n",
      "Iteration 517, loss = 1.11840412\n",
      "Iteration 518, loss = 1.11822173\n",
      "Iteration 519, loss = 1.11804037\n",
      "Iteration 520, loss = 1.11786003\n",
      "Iteration 521, loss = 1.11768070\n",
      "Iteration 522, loss = 1.11750235\n",
      "Iteration 523, loss = 1.11732496\n",
      "Iteration 524, loss = 1.11714853\n",
      "Iteration 525, loss = 1.11697304\n",
      "Iteration 526, loss = 1.11679846\n",
      "Iteration 527, loss = 1.11662480\n",
      "Iteration 528, loss = 1.11645203\n",
      "Iteration 529, loss = 1.11628014\n",
      "Iteration 530, loss = 1.11610911\n",
      "Iteration 531, loss = 1.11593894\n",
      "Iteration 532, loss = 1.11576961\n",
      "Iteration 533, loss = 1.11560110\n",
      "Iteration 534, loss = 1.11543341\n",
      "Iteration 535, loss = 1.11526652\n",
      "Iteration 536, loss = 1.11510042\n",
      "Iteration 537, loss = 1.11493510\n",
      "Iteration 538, loss = 1.11477055\n",
      "Iteration 539, loss = 1.11460675\n",
      "Iteration 540, loss = 1.11444371\n",
      "Iteration 541, loss = 1.11428139\n",
      "Iteration 542, loss = 1.11411981\n",
      "Iteration 543, loss = 1.11395894\n",
      "Iteration 544, loss = 1.11379878\n",
      "Iteration 545, loss = 1.11363931\n",
      "Iteration 546, loss = 1.11348054\n",
      "Iteration 547, loss = 1.11332244\n",
      "Iteration 548, loss = 1.11316502\n",
      "Iteration 549, loss = 1.11300825\n",
      "Iteration 550, loss = 1.11285214\n",
      "Iteration 551, loss = 1.11269668\n",
      "Iteration 552, loss = 1.11254186\n",
      "Iteration 553, loss = 1.11238766\n",
      "Iteration 554, loss = 1.11223409\n",
      "Iteration 555, loss = 1.11208114\n",
      "Iteration 556, loss = 1.11192879\n",
      "Iteration 557, loss = 1.11177705\n",
      "Iteration 558, loss = 1.11162591\n",
      "Iteration 559, loss = 1.11147535\n",
      "Iteration 560, loss = 1.11132537\n",
      "Iteration 561, loss = 1.11117598\n",
      "Iteration 562, loss = 1.11102715\n",
      "Iteration 563, loss = 1.11087889\n",
      "Iteration 564, loss = 1.11073118\n",
      "Iteration 565, loss = 1.11058403\n",
      "Iteration 566, loss = 1.11043743\n",
      "Iteration 567, loss = 1.11029137\n",
      "Iteration 568, loss = 1.11014585\n",
      "Iteration 569, loss = 1.11000086\n",
      "Iteration 570, loss = 1.10985639\n",
      "Iteration 571, loss = 1.10971245\n",
      "Iteration 572, loss = 1.10956903\n",
      "Iteration 573, loss = 1.10942612\n",
      "Iteration 574, loss = 1.10928372\n",
      "Iteration 575, loss = 1.10914182\n",
      "Iteration 576, loss = 1.10900042\n",
      "Iteration 577, loss = 1.10885952\n",
      "Iteration 578, loss = 1.10871910\n",
      "Iteration 579, loss = 1.10857918\n",
      "Iteration 580, loss = 1.10843974\n",
      "Iteration 581, loss = 1.10830078\n",
      "Iteration 582, loss = 1.10816229\n",
      "Iteration 583, loss = 1.10802428\n",
      "Iteration 584, loss = 1.10788673\n",
      "Iteration 585, loss = 1.10774965\n",
      "Iteration 586, loss = 1.10761303\n",
      "Iteration 587, loss = 1.10747686\n",
      "Iteration 588, loss = 1.10734116\n",
      "Iteration 589, loss = 1.10720590\n",
      "Iteration 590, loss = 1.10707109\n",
      "Iteration 591, loss = 1.10693673\n",
      "Iteration 592, loss = 1.10680280\n",
      "Iteration 593, loss = 1.10666932\n",
      "Iteration 594, loss = 1.10653627\n",
      "Iteration 595, loss = 1.10640366\n",
      "Iteration 596, loss = 1.10627148\n",
      "Iteration 597, loss = 1.10613972\n",
      "Iteration 598, loss = 1.10600839\n",
      "Iteration 599, loss = 1.10587724\n",
      "Iteration 600, loss = 1.10574416\n",
      "Iteration 601, loss = 1.10561083\n",
      "Iteration 602, loss = 1.10547733\n",
      "Iteration 603, loss = 1.10534373\n",
      "Iteration 604, loss = 1.10521008\n",
      "Iteration 605, loss = 1.10507645\n",
      "Iteration 606, loss = 1.10494204\n",
      "Iteration 607, loss = 1.10480208\n",
      "Iteration 608, loss = 1.10464652\n",
      "Iteration 609, loss = 1.10447831\n",
      "Iteration 610, loss = 1.10430351\n",
      "Iteration 611, loss = 1.10412295\n",
      "Iteration 612, loss = 1.10393740\n",
      "Iteration 613, loss = 1.10374755\n",
      "Iteration 614, loss = 1.10355400\n",
      "Iteration 615, loss = 1.10335731\n",
      "Iteration 616, loss = 1.10315796\n",
      "Iteration 617, loss = 1.10295641\n",
      "Iteration 618, loss = 1.10275303\n",
      "Iteration 619, loss = 1.10254819\n",
      "Iteration 620, loss = 1.10234218\n",
      "Iteration 621, loss = 1.10213529\n",
      "Iteration 622, loss = 1.10192776\n",
      "Iteration 623, loss = 1.10171981\n",
      "Iteration 624, loss = 1.10151163\n",
      "Iteration 625, loss = 1.10130339\n",
      "Iteration 626, loss = 1.10109524\n",
      "Iteration 627, loss = 1.10088730\n",
      "Iteration 628, loss = 1.10065350\n",
      "Iteration 629, loss = 1.10040742\n",
      "Iteration 630, loss = 1.10015887\n",
      "Iteration 631, loss = 1.09990865\n",
      "Iteration 632, loss = 1.09965745\n",
      "Iteration 633, loss = 1.09940588\n",
      "Iteration 634, loss = 1.09915445\n",
      "Iteration 635, loss = 1.09890360\n",
      "Iteration 636, loss = 1.09865372\n",
      "Iteration 637, loss = 1.09840512\n",
      "Iteration 638, loss = 1.09815807\n",
      "Iteration 639, loss = 1.09791280\n",
      "Iteration 640, loss = 1.09766949\n",
      "Iteration 641, loss = 1.09742829\n",
      "Iteration 642, loss = 1.09718932\n",
      "Iteration 643, loss = 1.09695269\n",
      "Iteration 644, loss = 1.09671845\n",
      "Iteration 645, loss = 1.09648667\n",
      "Iteration 646, loss = 1.09625739\n",
      "Iteration 647, loss = 1.09603062\n",
      "Iteration 648, loss = 1.09580638\n",
      "Iteration 649, loss = 1.09558466\n",
      "Iteration 650, loss = 1.09536546\n",
      "Iteration 651, loss = 1.09515283\n",
      "Iteration 652, loss = 1.09494279\n",
      "Iteration 653, loss = 1.09473513\n",
      "Iteration 654, loss = 1.09452983\n",
      "Iteration 655, loss = 1.09432684\n",
      "Iteration 656, loss = 1.09412612\n",
      "Iteration 657, loss = 1.09392763\n",
      "Iteration 658, loss = 1.09373133\n",
      "Iteration 659, loss = 1.09353717\n",
      "Iteration 660, loss = 1.09334511\n",
      "Iteration 661, loss = 1.09315511\n",
      "Iteration 662, loss = 1.09296713\n",
      "Iteration 663, loss = 1.09278111\n",
      "Iteration 664, loss = 1.09259703\n",
      "Iteration 665, loss = 1.09241484\n",
      "Iteration 666, loss = 1.09223450\n",
      "Iteration 667, loss = 1.09205596\n",
      "Iteration 668, loss = 1.09187920\n",
      "Iteration 669, loss = 1.09170416\n",
      "Iteration 670, loss = 1.09153082\n",
      "Iteration 671, loss = 1.09135914\n",
      "Iteration 672, loss = 1.09117988\n",
      "Iteration 673, loss = 1.09099957\n",
      "Iteration 674, loss = 1.09081926\n",
      "Iteration 675, loss = 1.09063909\n",
      "Iteration 676, loss = 1.09045919\n",
      "Iteration 677, loss = 1.09027968\n",
      "Iteration 678, loss = 1.09010066\n",
      "Iteration 679, loss = 1.08992223\n",
      "Iteration 680, loss = 1.08974445\n",
      "Iteration 681, loss = 1.08956740\n",
      "Iteration 682, loss = 1.08939114\n",
      "Iteration 683, loss = 1.08921571\n",
      "Iteration 684, loss = 1.08904115\n",
      "Iteration 685, loss = 1.08886751\n",
      "Iteration 686, loss = 1.08869481\n",
      "Iteration 687, loss = 1.08852308\n",
      "Iteration 688, loss = 1.08835233\n",
      "Iteration 689, loss = 1.08818259\n",
      "Iteration 690, loss = 1.08801385\n",
      "Iteration 691, loss = 1.08784614\n",
      "Iteration 692, loss = 1.08767945\n",
      "Iteration 693, loss = 1.08751380\n",
      "Iteration 694, loss = 1.08734918\n",
      "Iteration 695, loss = 1.08718724\n",
      "Iteration 696, loss = 1.08702733\n",
      "Iteration 697, loss = 1.08686841\n",
      "Iteration 698, loss = 1.08671047\n",
      "Iteration 699, loss = 1.08655351\n",
      "Iteration 700, loss = 1.08639752\n",
      "Iteration 701, loss = 1.08624249\n",
      "Iteration 702, loss = 1.08608841\n",
      "Iteration 703, loss = 1.08593528\n",
      "Iteration 704, loss = 1.08578308\n",
      "Iteration 705, loss = 1.08563181\n",
      "Iteration 706, loss = 1.08548146\n",
      "Iteration 707, loss = 1.08533203\n",
      "Iteration 708, loss = 1.08518350\n",
      "Iteration 709, loss = 1.08503586\n",
      "Iteration 710, loss = 1.08488912\n",
      "Iteration 711, loss = 1.08474325\n",
      "Iteration 712, loss = 1.08459826\n",
      "Iteration 713, loss = 1.08445414\n",
      "Iteration 714, loss = 1.08431087\n",
      "Iteration 715, loss = 1.08416844\n",
      "Iteration 716, loss = 1.08402686\n",
      "Iteration 717, loss = 1.08388611\n",
      "Iteration 718, loss = 1.08374619\n",
      "Iteration 719, loss = 1.08360708\n",
      "Iteration 720, loss = 1.08346878\n",
      "Iteration 721, loss = 1.08333128\n",
      "Iteration 722, loss = 1.08319457\n",
      "Iteration 723, loss = 1.08305865\n",
      "Iteration 724, loss = 1.08292351\n",
      "Iteration 725, loss = 1.08279278\n",
      "Iteration 726, loss = 1.08267920\n",
      "Iteration 727, loss = 1.08256513\n",
      "Iteration 728, loss = 1.08247185\n",
      "Iteration 729, loss = 1.08237706\n",
      "Iteration 730, loss = 1.08227965\n",
      "Iteration 731, loss = 1.08217996\n",
      "Iteration 732, loss = 1.08208547\n",
      "Iteration 733, loss = 1.08199744\n",
      "Iteration 734, loss = 1.08190815\n",
      "Iteration 735, loss = 1.08181778\n",
      "Iteration 736, loss = 1.08172650\n",
      "Iteration 737, loss = 1.08163447\n",
      "Iteration 738, loss = 1.08154182\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.97403484\n",
      "Iteration 2, loss = 1.96923304\n",
      "Iteration 3, loss = 1.96445347\n",
      "Iteration 4, loss = 1.95969893\n",
      "Iteration 5, loss = 1.95497417\n",
      "Iteration 6, loss = 1.95027275\n",
      "Iteration 7, loss = 1.94559491\n",
      "Iteration 8, loss = 1.94094087\n",
      "Iteration 9, loss = 1.93631091\n",
      "Iteration 10, loss = 1.93170526\n",
      "Iteration 11, loss = 1.92712551\n",
      "Iteration 12, loss = 1.92257687\n",
      "Iteration 13, loss = 1.91805819\n",
      "Iteration 14, loss = 1.91356796\n",
      "Iteration 15, loss = 1.90910348\n",
      "Iteration 16, loss = 1.90466481\n",
      "Iteration 17, loss = 1.90025205\n",
      "Iteration 18, loss = 1.89586526\n",
      "Iteration 19, loss = 1.89150453\n",
      "Iteration 20, loss = 1.88716993\n",
      "Iteration 21, loss = 1.88286154\n",
      "Iteration 22, loss = 1.87857940\n",
      "Iteration 23, loss = 1.87432357\n",
      "Iteration 24, loss = 1.87009409\n",
      "Iteration 25, loss = 1.86589099\n",
      "Iteration 26, loss = 1.86171429\n",
      "Iteration 27, loss = 1.85756401\n",
      "Iteration 28, loss = 1.85344013\n",
      "Iteration 29, loss = 1.84934265\n",
      "Iteration 30, loss = 1.84527155\n",
      "Iteration 31, loss = 1.84122679\n",
      "Iteration 32, loss = 1.83720833\n",
      "Iteration 33, loss = 1.83321612\n",
      "Iteration 34, loss = 1.82925009\n",
      "Iteration 35, loss = 1.82531018\n",
      "Iteration 36, loss = 1.82139631\n",
      "Iteration 37, loss = 1.81750838\n",
      "Iteration 38, loss = 1.81364631\n",
      "Iteration 39, loss = 1.80980999\n",
      "Iteration 40, loss = 1.80599930\n",
      "Iteration 41, loss = 1.80221414\n",
      "Iteration 42, loss = 1.79845438\n",
      "Iteration 43, loss = 1.79471989\n",
      "Iteration 44, loss = 1.79101053\n",
      "Iteration 45, loss = 1.78732618\n",
      "Iteration 46, loss = 1.78366667\n",
      "Iteration 47, loss = 1.78003186\n",
      "Iteration 48, loss = 1.77642161\n",
      "Iteration 49, loss = 1.77283369\n",
      "Iteration 50, loss = 1.76926745\n",
      "Iteration 51, loss = 1.76572492\n",
      "Iteration 52, loss = 1.76220601\n",
      "Iteration 53, loss = 1.75871062\n",
      "Iteration 54, loss = 1.75523865\n",
      "Iteration 55, loss = 1.75178998\n",
      "Iteration 56, loss = 1.74836447\n",
      "Iteration 57, loss = 1.74496201\n",
      "Iteration 58, loss = 1.74158244\n",
      "Iteration 59, loss = 1.73822564\n",
      "Iteration 60, loss = 1.73489144\n",
      "Iteration 61, loss = 1.73157971\n",
      "Iteration 62, loss = 1.72829028\n",
      "Iteration 63, loss = 1.72502299\n",
      "Iteration 64, loss = 1.72177769\n",
      "Iteration 65, loss = 1.71854157\n",
      "Iteration 66, loss = 1.71527814\n",
      "Iteration 67, loss = 1.71202270\n",
      "Iteration 68, loss = 1.70877873\n",
      "Iteration 69, loss = 1.70554858\n",
      "Iteration 70, loss = 1.70233392\n",
      "Iteration 71, loss = 1.69913597\n",
      "Iteration 72, loss = 1.69595566\n",
      "Iteration 73, loss = 1.69279369\n",
      "Iteration 74, loss = 1.68965061\n",
      "Iteration 75, loss = 1.68652683\n",
      "Iteration 76, loss = 1.68342265\n",
      "Iteration 77, loss = 1.68033830\n",
      "Iteration 78, loss = 1.67727394\n",
      "Iteration 79, loss = 1.67422966\n",
      "Iteration 80, loss = 1.67120553\n",
      "Iteration 81, loss = 1.66820552\n",
      "Iteration 82, loss = 1.66522809\n",
      "Iteration 83, loss = 1.66227113\n",
      "Iteration 84, loss = 1.65933454\n",
      "Iteration 85, loss = 1.65641818\n",
      "Iteration 86, loss = 1.65352191\n",
      "Iteration 87, loss = 1.65064558\n",
      "Iteration 88, loss = 1.64778903\n",
      "Iteration 89, loss = 1.64495211\n",
      "Iteration 90, loss = 1.64213465\n",
      "Iteration 91, loss = 1.63933649\n",
      "Iteration 92, loss = 1.63655744\n",
      "Iteration 93, loss = 1.63379734\n",
      "Iteration 94, loss = 1.63105602\n",
      "Iteration 95, loss = 1.62833331\n",
      "Iteration 96, loss = 1.62562903\n",
      "Iteration 97, loss = 1.62294302\n",
      "Iteration 98, loss = 1.62026643\n",
      "Iteration 99, loss = 1.61760242\n",
      "Iteration 100, loss = 1.61495468\n",
      "Iteration 101, loss = 1.61232326\n",
      "Iteration 102, loss = 1.60970819\n",
      "Iteration 103, loss = 1.60710949\n",
      "Iteration 104, loss = 1.60452278\n",
      "Iteration 105, loss = 1.60189320\n",
      "Iteration 106, loss = 1.59927555\n",
      "Iteration 107, loss = 1.59667072\n",
      "Iteration 108, loss = 1.59393490\n",
      "Iteration 109, loss = 1.59117934\n",
      "Iteration 110, loss = 1.58842285\n",
      "Iteration 111, loss = 1.58566975\n",
      "Iteration 112, loss = 1.58292342\n",
      "Iteration 113, loss = 1.58018656\n",
      "Iteration 114, loss = 1.57746133\n",
      "Iteration 115, loss = 1.57474945\n",
      "Iteration 116, loss = 1.57205233\n",
      "Iteration 117, loss = 1.56937108\n",
      "Iteration 118, loss = 1.56670661\n",
      "Iteration 119, loss = 1.56405961\n",
      "Iteration 120, loss = 1.56143066\n",
      "Iteration 121, loss = 1.55882019\n",
      "Iteration 122, loss = 1.55622852\n",
      "Iteration 123, loss = 1.55365591\n",
      "Iteration 124, loss = 1.55110250\n",
      "Iteration 125, loss = 1.54856842\n",
      "Iteration 126, loss = 1.54605372\n",
      "Iteration 127, loss = 1.54355840\n",
      "Iteration 128, loss = 1.54108244\n",
      "Iteration 129, loss = 1.53862578\n",
      "Iteration 130, loss = 1.53618834\n",
      "Iteration 131, loss = 1.53377002\n",
      "Iteration 132, loss = 1.53137070\n",
      "Iteration 133, loss = 1.52899025\n",
      "Iteration 134, loss = 1.52662851\n",
      "Iteration 135, loss = 1.52428534\n",
      "Iteration 136, loss = 1.52196447\n",
      "Iteration 137, loss = 1.51966242\n",
      "Iteration 138, loss = 1.51737871\n",
      "Iteration 139, loss = 1.51511311\n",
      "Iteration 140, loss = 1.51286543\n",
      "Iteration 141, loss = 1.51063544\n",
      "Iteration 142, loss = 1.50842294\n",
      "Iteration 143, loss = 1.50622772\n",
      "Iteration 144, loss = 1.50404958\n",
      "Iteration 145, loss = 1.50188833\n",
      "Iteration 146, loss = 1.49974375\n",
      "Iteration 147, loss = 1.49761568\n",
      "Iteration 148, loss = 1.49550390\n",
      "Iteration 149, loss = 1.49339983\n",
      "Iteration 150, loss = 1.49128412\n",
      "Iteration 151, loss = 1.48918234\n",
      "Iteration 152, loss = 1.48709469\n",
      "Iteration 153, loss = 1.48502134\n",
      "Iteration 154, loss = 1.48295190\n",
      "Iteration 155, loss = 1.48089527\n",
      "Iteration 156, loss = 1.47885227\n",
      "Iteration 157, loss = 1.47682307\n",
      "Iteration 158, loss = 1.47479270\n",
      "Iteration 159, loss = 1.47267066\n",
      "Iteration 160, loss = 1.47055311\n",
      "Iteration 161, loss = 1.46844184\n",
      "Iteration 162, loss = 1.46633837\n",
      "Iteration 163, loss = 1.46424402\n",
      "Iteration 164, loss = 1.46215987\n",
      "Iteration 165, loss = 1.46008686\n",
      "Iteration 166, loss = 1.45803589\n",
      "Iteration 167, loss = 1.45602271\n",
      "Iteration 168, loss = 1.45402396\n",
      "Iteration 169, loss = 1.45203979\n",
      "Iteration 170, loss = 1.45007031\n",
      "Iteration 171, loss = 1.44811563\n",
      "Iteration 172, loss = 1.44617578\n",
      "Iteration 173, loss = 1.44425080\n",
      "Iteration 174, loss = 1.44234069\n",
      "Iteration 175, loss = 1.44044542\n",
      "Iteration 176, loss = 1.43856497\n",
      "Iteration 177, loss = 1.43669928\n",
      "Iteration 178, loss = 1.43484828\n",
      "Iteration 179, loss = 1.43301356\n",
      "Iteration 180, loss = 1.43119527\n",
      "Iteration 181, loss = 1.42939308\n",
      "Iteration 182, loss = 1.42763451\n",
      "Iteration 183, loss = 1.42589789\n",
      "Iteration 184, loss = 1.42418865\n",
      "Iteration 185, loss = 1.42250456\n",
      "Iteration 186, loss = 1.42083654\n",
      "Iteration 187, loss = 1.41918414\n",
      "Iteration 188, loss = 1.41754692\n",
      "Iteration 189, loss = 1.41592449\n",
      "Iteration 190, loss = 1.41431649\n",
      "Iteration 191, loss = 1.41272256\n",
      "Iteration 192, loss = 1.41114241\n",
      "Iteration 193, loss = 1.40957573\n",
      "Iteration 194, loss = 1.40802225\n",
      "Iteration 195, loss = 1.40648172\n",
      "Iteration 196, loss = 1.40495390\n",
      "Iteration 197, loss = 1.40343856\n",
      "Iteration 198, loss = 1.40193549\n",
      "Iteration 199, loss = 1.40044449\n",
      "Iteration 200, loss = 1.39896538\n",
      "Iteration 201, loss = 1.39749797\n",
      "Iteration 202, loss = 1.39604209\n",
      "Iteration 203, loss = 1.39459759\n",
      "Iteration 204, loss = 1.39316430\n",
      "Iteration 205, loss = 1.39174209\n",
      "Iteration 206, loss = 1.39033082\n",
      "Iteration 207, loss = 1.38893034\n",
      "Iteration 208, loss = 1.38753420\n",
      "Iteration 209, loss = 1.38614019\n",
      "Iteration 210, loss = 1.38478684\n",
      "Iteration 211, loss = 1.38344077\n",
      "Iteration 212, loss = 1.38210562\n",
      "Iteration 213, loss = 1.38078115\n",
      "Iteration 214, loss = 1.37946714\n",
      "Iteration 215, loss = 1.37813305\n",
      "Iteration 216, loss = 1.37678352\n",
      "Iteration 217, loss = 1.37543700\n",
      "Iteration 218, loss = 1.37409551\n",
      "Iteration 219, loss = 1.37275966\n",
      "Iteration 220, loss = 1.37143002\n",
      "Iteration 221, loss = 1.37010705\n",
      "Iteration 222, loss = 1.36879115\n",
      "Iteration 223, loss = 1.36748268\n",
      "Iteration 224, loss = 1.36618194\n",
      "Iteration 225, loss = 1.36488917\n",
      "Iteration 226, loss = 1.36360461\n",
      "Iteration 227, loss = 1.36232842\n",
      "Iteration 228, loss = 1.36106076\n",
      "Iteration 229, loss = 1.35980105\n",
      "Iteration 230, loss = 1.35854456\n",
      "Iteration 231, loss = 1.35729659\n",
      "Iteration 232, loss = 1.35605728\n",
      "Iteration 233, loss = 1.35482674\n",
      "Iteration 234, loss = 1.35360504\n",
      "Iteration 235, loss = 1.35239225\n",
      "Iteration 236, loss = 1.35118842\n",
      "Iteration 237, loss = 1.34999358\n",
      "Iteration 238, loss = 1.34880775\n",
      "Iteration 239, loss = 1.34763091\n",
      "Iteration 240, loss = 1.34646308\n",
      "Iteration 241, loss = 1.34530422\n",
      "Iteration 242, loss = 1.34415433\n",
      "Iteration 243, loss = 1.34301335\n",
      "Iteration 244, loss = 1.34188456\n",
      "Iteration 245, loss = 1.34076340\n",
      "Iteration 246, loss = 1.33964069\n",
      "Iteration 247, loss = 1.33850299\n",
      "Iteration 248, loss = 1.33736967\n",
      "Iteration 249, loss = 1.33629729\n",
      "Iteration 250, loss = 1.33523370\n",
      "Iteration 251, loss = 1.33417884\n",
      "Iteration 252, loss = 1.33313264\n",
      "Iteration 253, loss = 1.33209504\n",
      "Iteration 254, loss = 1.33106596\n",
      "Iteration 255, loss = 1.33004532\n",
      "Iteration 256, loss = 1.32903304\n",
      "Iteration 257, loss = 1.32802904\n",
      "Iteration 258, loss = 1.32703323\n",
      "Iteration 259, loss = 1.32604551\n",
      "Iteration 260, loss = 1.32506581\n",
      "Iteration 261, loss = 1.32409404\n",
      "Iteration 262, loss = 1.32313009\n",
      "Iteration 263, loss = 1.32217390\n",
      "Iteration 264, loss = 1.32122536\n",
      "Iteration 265, loss = 1.32028440\n",
      "Iteration 266, loss = 1.31935233\n",
      "Iteration 267, loss = 1.31842788\n",
      "Iteration 268, loss = 1.31751083\n",
      "Iteration 269, loss = 1.31660108\n",
      "Iteration 270, loss = 1.31569853\n",
      "Iteration 271, loss = 1.31480310\n",
      "Iteration 272, loss = 1.31391470\n",
      "Iteration 273, loss = 1.31303324\n",
      "Iteration 274, loss = 1.31215864\n",
      "Iteration 275, loss = 1.31129082\n",
      "Iteration 276, loss = 1.31042970\n",
      "Iteration 277, loss = 1.30957520\n",
      "Iteration 278, loss = 1.30872724\n",
      "Iteration 279, loss = 1.30788576\n",
      "Iteration 280, loss = 1.30705068\n",
      "Iteration 281, loss = 1.30622192\n",
      "Iteration 282, loss = 1.30539943\n",
      "Iteration 283, loss = 1.30458312\n",
      "Iteration 284, loss = 1.30378481\n",
      "Iteration 285, loss = 1.30299338\n",
      "Iteration 286, loss = 1.30221809\n",
      "Iteration 287, loss = 1.30143632\n",
      "Iteration 288, loss = 1.30064388\n",
      "Iteration 289, loss = 1.29985611\n",
      "Iteration 290, loss = 1.29907314\n",
      "Iteration 291, loss = 1.29829504\n",
      "Iteration 292, loss = 1.29752049\n",
      "Iteration 293, loss = 1.29674546\n",
      "Iteration 294, loss = 1.29597503\n",
      "Iteration 295, loss = 1.29520947\n",
      "Iteration 296, loss = 1.29444970\n",
      "Iteration 297, loss = 1.29369487\n",
      "Iteration 298, loss = 1.29294502\n",
      "Iteration 299, loss = 1.29220022\n",
      "Iteration 300, loss = 1.29146048\n",
      "Iteration 301, loss = 1.29072583\n",
      "Iteration 302, loss = 1.28999628\n",
      "Iteration 303, loss = 1.28927185\n",
      "Iteration 304, loss = 1.28855252\n",
      "Iteration 305, loss = 1.28783830\n",
      "Iteration 306, loss = 1.28712916\n",
      "Iteration 307, loss = 1.28642510\n",
      "Iteration 308, loss = 1.28572609\n",
      "Iteration 309, loss = 1.28503210\n",
      "Iteration 310, loss = 1.28435446\n",
      "Iteration 311, loss = 1.28368464\n",
      "Iteration 312, loss = 1.28302061\n",
      "Iteration 313, loss = 1.28236220\n",
      "Iteration 314, loss = 1.28170927\n",
      "Iteration 315, loss = 1.28106168\n",
      "Iteration 316, loss = 1.28041931\n",
      "Iteration 317, loss = 1.27978205\n",
      "Iteration 318, loss = 1.27914978\n",
      "Iteration 319, loss = 1.27852240\n",
      "Iteration 320, loss = 1.27789983\n",
      "Iteration 321, loss = 1.27729027\n",
      "Iteration 322, loss = 1.27671190\n",
      "Iteration 323, loss = 1.27613960\n",
      "Iteration 324, loss = 1.27557309\n",
      "Iteration 325, loss = 1.27501211\n",
      "Iteration 326, loss = 1.27445643\n",
      "Iteration 327, loss = 1.27390583\n",
      "Iteration 328, loss = 1.27336012\n",
      "Iteration 329, loss = 1.27279784\n",
      "Iteration 330, loss = 1.27223534\n",
      "Iteration 331, loss = 1.27167536\n",
      "Iteration 332, loss = 1.27111801\n",
      "Iteration 333, loss = 1.27056339\n",
      "Iteration 334, loss = 1.27001159\n",
      "Iteration 335, loss = 1.26946267\n",
      "Iteration 336, loss = 1.26891670\n",
      "Iteration 337, loss = 1.26837373\n",
      "Iteration 338, loss = 1.26783381\n",
      "Iteration 339, loss = 1.26729698\n",
      "Iteration 340, loss = 1.26676325\n",
      "Iteration 341, loss = 1.26623266\n",
      "Iteration 342, loss = 1.26570522\n",
      "Iteration 343, loss = 1.26518095\n",
      "Iteration 344, loss = 1.26466120\n",
      "Iteration 345, loss = 1.26414502\n",
      "Iteration 346, loss = 1.26363224\n",
      "Iteration 347, loss = 1.26312285\n",
      "Iteration 348, loss = 1.26261685\n",
      "Iteration 349, loss = 1.26211421\n",
      "Iteration 350, loss = 1.26161491\n",
      "Iteration 351, loss = 1.26111895\n",
      "Iteration 352, loss = 1.26062630\n",
      "Iteration 353, loss = 1.26013695\n",
      "Iteration 354, loss = 1.25965087\n",
      "Iteration 355, loss = 1.25916717\n",
      "Iteration 356, loss = 1.25868377\n",
      "Iteration 357, loss = 1.25820335\n",
      "Iteration 358, loss = 1.25772591\n",
      "Iteration 359, loss = 1.25725147\n",
      "Iteration 360, loss = 1.25678003\n",
      "Iteration 361, loss = 1.25631161\n",
      "Iteration 362, loss = 1.25584314\n",
      "Iteration 363, loss = 1.25535026\n",
      "Iteration 364, loss = 1.25485727\n",
      "Iteration 365, loss = 1.25436450\n",
      "Iteration 366, loss = 1.25387227\n",
      "Iteration 367, loss = 1.25338085\n",
      "Iteration 368, loss = 1.25289051\n",
      "Iteration 369, loss = 1.25240145\n",
      "Iteration 370, loss = 1.25191386\n",
      "Iteration 371, loss = 1.25142792\n",
      "Iteration 372, loss = 1.25094379\n",
      "Iteration 373, loss = 1.25046158\n",
      "Iteration 374, loss = 1.24998142\n",
      "Iteration 375, loss = 1.24950506\n",
      "Iteration 376, loss = 1.24903247\n",
      "Iteration 377, loss = 1.24856253\n",
      "Iteration 378, loss = 1.24809529\n",
      "Iteration 379, loss = 1.24762808\n",
      "Iteration 380, loss = 1.24715656\n",
      "Iteration 381, loss = 1.24665885\n",
      "Iteration 382, loss = 1.24616098\n",
      "Iteration 383, loss = 1.24566604\n",
      "Iteration 384, loss = 1.24517168\n",
      "Iteration 385, loss = 1.24467823\n",
      "Iteration 386, loss = 1.24418598\n",
      "Iteration 387, loss = 1.24369516\n",
      "Iteration 388, loss = 1.24320602\n",
      "Iteration 389, loss = 1.24271873\n",
      "Iteration 390, loss = 1.24223348\n",
      "Iteration 391, loss = 1.24175053\n",
      "Iteration 392, loss = 1.24127980\n",
      "Iteration 393, loss = 1.24081222\n",
      "Iteration 394, loss = 1.24035430\n",
      "Iteration 395, loss = 1.23990281\n",
      "Iteration 396, loss = 1.23946284\n",
      "Iteration 397, loss = 1.23902841\n",
      "Iteration 398, loss = 1.23857959\n",
      "Iteration 399, loss = 1.23813312\n",
      "Iteration 400, loss = 1.23768904\n",
      "Iteration 401, loss = 1.23724737\n",
      "Iteration 402, loss = 1.23680812\n",
      "Iteration 403, loss = 1.23637132\n",
      "Iteration 404, loss = 1.23593698\n",
      "Iteration 405, loss = 1.23550509\n",
      "Iteration 406, loss = 1.23508341\n",
      "Iteration 407, loss = 1.23468298\n",
      "Iteration 408, loss = 1.23428669\n",
      "Iteration 409, loss = 1.23389433\n",
      "Iteration 410, loss = 1.23350875\n",
      "Iteration 411, loss = 1.23313251\n",
      "Iteration 412, loss = 1.23276029\n",
      "Iteration 413, loss = 1.23239175\n",
      "Iteration 414, loss = 1.23202669\n",
      "Iteration 415, loss = 1.23166702\n",
      "Iteration 416, loss = 1.23131306\n",
      "Iteration 417, loss = 1.23096246\n",
      "Iteration 418, loss = 1.23061509\n",
      "Iteration 419, loss = 1.23027079\n",
      "Iteration 420, loss = 1.22992943\n",
      "Iteration 421, loss = 1.22959091\n",
      "Iteration 422, loss = 1.22925510\n",
      "Iteration 423, loss = 1.22893852\n",
      "Iteration 424, loss = 1.22862679\n",
      "Iteration 425, loss = 1.22832586\n",
      "Iteration 426, loss = 1.22803994\n",
      "Iteration 427, loss = 1.22775756\n",
      "Iteration 428, loss = 1.22747851\n",
      "Iteration 429, loss = 1.22720259\n",
      "Iteration 430, loss = 1.22692963\n",
      "Iteration 431, loss = 1.22665948\n",
      "Iteration 432, loss = 1.22639198\n",
      "Iteration 433, loss = 1.22612701\n",
      "Iteration 434, loss = 1.22586446\n",
      "Iteration 435, loss = 1.22560421\n",
      "Iteration 436, loss = 1.22534617\n",
      "Iteration 437, loss = 1.22509026\n",
      "Iteration 438, loss = 1.22483639\n",
      "Iteration 439, loss = 1.22458449\n",
      "Iteration 440, loss = 1.22433449\n",
      "Iteration 441, loss = 1.22408634\n",
      "Iteration 442, loss = 1.22383998\n",
      "Iteration 443, loss = 1.22359536\n",
      "Iteration 444, loss = 1.22335242\n",
      "Iteration 445, loss = 1.22311114\n",
      "Iteration 446, loss = 1.22287147\n",
      "Iteration 447, loss = 1.22263337\n",
      "Iteration 448, loss = 1.22239682\n",
      "Iteration 449, loss = 1.22216177\n",
      "Iteration 450, loss = 1.22192821\n",
      "Iteration 451, loss = 1.22169611\n",
      "Iteration 452, loss = 1.22146543\n",
      "Iteration 453, loss = 1.22123616\n",
      "Iteration 454, loss = 1.22100828\n",
      "Iteration 455, loss = 1.22078177\n",
      "Iteration 456, loss = 1.22055661\n",
      "Iteration 457, loss = 1.22033277\n",
      "Iteration 458, loss = 1.22011025\n",
      "Iteration 459, loss = 1.21988903\n",
      "Iteration 460, loss = 1.21966909\n",
      "Iteration 461, loss = 1.21945042\n",
      "Iteration 462, loss = 1.21923301\n",
      "Iteration 463, loss = 1.21901683\n",
      "Iteration 464, loss = 1.21880189\n",
      "Iteration 465, loss = 1.21858817\n",
      "Iteration 466, loss = 1.21837565\n",
      "Iteration 467, loss = 1.21816433\n",
      "Iteration 468, loss = 1.21795419\n",
      "Iteration 469, loss = 1.21774523\n",
      "Iteration 470, loss = 1.21753743\n",
      "Iteration 471, loss = 1.21733079\n",
      "Iteration 472, loss = 1.21712530\n",
      "Iteration 473, loss = 1.21692094\n",
      "Iteration 474, loss = 1.21671771\n",
      "Iteration 475, loss = 1.21651560\n",
      "Iteration 476, loss = 1.21631460\n",
      "Iteration 477, loss = 1.21611470\n",
      "Iteration 478, loss = 1.21591590\n",
      "Iteration 479, loss = 1.21571818\n",
      "Iteration 480, loss = 1.21552155\n",
      "Iteration 481, loss = 1.21532515\n",
      "Iteration 482, loss = 1.21511544\n",
      "Iteration 483, loss = 1.21490428\n",
      "Iteration 484, loss = 1.21469241\n",
      "Iteration 485, loss = 1.21447999\n",
      "Iteration 486, loss = 1.21426721\n",
      "Iteration 487, loss = 1.21405513\n",
      "Iteration 488, loss = 1.21384467\n",
      "Iteration 489, loss = 1.21363451\n",
      "Iteration 490, loss = 1.21342471\n",
      "Iteration 491, loss = 1.21321533\n",
      "Iteration 492, loss = 1.21300668\n",
      "Iteration 493, loss = 1.21279896\n",
      "Iteration 494, loss = 1.21258440\n",
      "Iteration 495, loss = 1.21236784\n",
      "Iteration 496, loss = 1.21215063\n",
      "Iteration 497, loss = 1.21193296\n",
      "Iteration 498, loss = 1.21170534\n",
      "Iteration 499, loss = 1.21146735\n",
      "Iteration 500, loss = 1.21122678\n",
      "Iteration 501, loss = 1.21098403\n",
      "Iteration 502, loss = 1.21073524\n",
      "Iteration 503, loss = 1.21049029\n",
      "Iteration 504, loss = 1.21024419\n",
      "Iteration 505, loss = 1.20999720\n",
      "Iteration 506, loss = 1.20975178\n",
      "Iteration 507, loss = 1.20950755\n",
      "Iteration 508, loss = 1.20926336\n",
      "Iteration 509, loss = 1.20901933\n",
      "Iteration 510, loss = 1.20877555\n",
      "Iteration 511, loss = 1.20853211\n",
      "Iteration 512, loss = 1.20828908\n",
      "Iteration 513, loss = 1.20805857\n",
      "Iteration 514, loss = 1.20783529\n",
      "Iteration 515, loss = 1.20761347\n",
      "Iteration 516, loss = 1.20739303\n",
      "Iteration 517, loss = 1.20717391\n",
      "Iteration 518, loss = 1.20695606\n",
      "Iteration 519, loss = 1.20673943\n",
      "Iteration 520, loss = 1.20652397\n",
      "Iteration 521, loss = 1.20630964\n",
      "Iteration 522, loss = 1.20609641\n",
      "Iteration 523, loss = 1.20588422\n",
      "Iteration 524, loss = 1.20567307\n",
      "Iteration 525, loss = 1.20546270\n",
      "Iteration 526, loss = 1.20525310\n",
      "Iteration 527, loss = 1.20504442\n",
      "Iteration 528, loss = 1.20483665\n",
      "Iteration 529, loss = 1.20462963\n",
      "Iteration 530, loss = 1.20442296\n",
      "Iteration 531, loss = 1.20422028\n",
      "Iteration 532, loss = 1.20402034\n",
      "Iteration 533, loss = 1.20382153\n",
      "Iteration 534, loss = 1.20362379\n",
      "Iteration 535, loss = 1.20342709\n",
      "Iteration 536, loss = 1.20323138\n",
      "Iteration 537, loss = 1.20303662\n",
      "Iteration 538, loss = 1.20284422\n",
      "Iteration 539, loss = 1.20265316\n",
      "Iteration 540, loss = 1.20247632\n",
      "Iteration 541, loss = 1.20230588\n",
      "Iteration 542, loss = 1.20213771\n",
      "Iteration 543, loss = 1.20197131\n",
      "Iteration 544, loss = 1.20180658\n",
      "Iteration 545, loss = 1.20164339\n",
      "Iteration 546, loss = 1.20148164\n",
      "Iteration 547, loss = 1.20132125\n",
      "Iteration 548, loss = 1.20116213\n",
      "Iteration 549, loss = 1.20100421\n",
      "Iteration 550, loss = 1.20084743\n",
      "Iteration 551, loss = 1.20069172\n",
      "Iteration 552, loss = 1.20053702\n",
      "Iteration 553, loss = 1.20038329\n",
      "Iteration 554, loss = 1.20023049\n",
      "Iteration 555, loss = 1.20007857\n",
      "Iteration 556, loss = 1.19992749\n",
      "Iteration 557, loss = 1.19977722\n",
      "Iteration 558, loss = 1.19962774\n",
      "Iteration 559, loss = 1.19947900\n",
      "Iteration 560, loss = 1.19933099\n",
      "Iteration 561, loss = 1.19918368\n",
      "Iteration 562, loss = 1.19903705\n",
      "Iteration 563, loss = 1.19889108\n",
      "Iteration 564, loss = 1.19874576\n",
      "Iteration 565, loss = 1.19860106\n",
      "Iteration 566, loss = 1.19845696\n",
      "Iteration 567, loss = 1.19831347\n",
      "Iteration 568, loss = 1.19817083\n",
      "Iteration 569, loss = 1.19803039\n",
      "Iteration 570, loss = 1.19789062\n",
      "Iteration 571, loss = 1.19775150\n",
      "Iteration 572, loss = 1.19761300\n",
      "Iteration 573, loss = 1.19747511\n",
      "Iteration 574, loss = 1.19733780\n",
      "Iteration 575, loss = 1.19720107\n",
      "Iteration 576, loss = 1.19706491\n",
      "Iteration 577, loss = 1.19692929\n",
      "Iteration 578, loss = 1.19679421\n",
      "Iteration 579, loss = 1.19665966\n",
      "Iteration 580, loss = 1.19652561\n",
      "Iteration 581, loss = 1.19639207\n",
      "Iteration 582, loss = 1.19625902\n",
      "Iteration 583, loss = 1.19612646\n",
      "Iteration 584, loss = 1.19599436\n",
      "Iteration 585, loss = 1.19586274\n",
      "Iteration 586, loss = 1.19573157\n",
      "Iteration 587, loss = 1.19560085\n",
      "Iteration 588, loss = 1.19547058\n",
      "Iteration 589, loss = 1.19534075\n",
      "Iteration 590, loss = 1.19521134\n",
      "Iteration 591, loss = 1.19508237\n",
      "Iteration 592, loss = 1.19495389\n",
      "Iteration 593, loss = 1.19482588\n",
      "Iteration 594, loss = 1.19469831\n",
      "Iteration 595, loss = 1.19457118\n",
      "Iteration 596, loss = 1.19444450\n",
      "Iteration 597, loss = 1.19431835\n",
      "Iteration 598, loss = 1.19419263\n",
      "Iteration 599, loss = 1.19406731\n",
      "Iteration 600, loss = 1.19394332\n",
      "Iteration 601, loss = 1.19382063\n",
      "Iteration 602, loss = 1.19370559\n",
      "Iteration 603, loss = 1.19359532\n",
      "Iteration 604, loss = 1.19348599\n",
      "Iteration 605, loss = 1.19337756\n",
      "Iteration 606, loss = 1.19326997\n",
      "Iteration 607, loss = 1.19316317\n",
      "Iteration 608, loss = 1.19305712\n",
      "Iteration 609, loss = 1.19295177\n",
      "Iteration 610, loss = 1.19284710\n",
      "Iteration 611, loss = 1.19274308\n",
      "Iteration 612, loss = 1.19263967\n",
      "Iteration 613, loss = 1.19253684\n",
      "Iteration 614, loss = 1.19243458\n",
      "Iteration 615, loss = 1.19233287\n",
      "Iteration 616, loss = 1.19223168\n",
      "Iteration 617, loss = 1.19213169\n",
      "Iteration 618, loss = 1.19203226\n",
      "Iteration 619, loss = 1.19193335\n",
      "Iteration 620, loss = 1.19183494\n",
      "Iteration 621, loss = 1.19173723\n",
      "Iteration 622, loss = 1.19164051\n",
      "Iteration 623, loss = 1.19154428\n",
      "Iteration 624, loss = 1.19144853\n",
      "Iteration 625, loss = 1.19135324\n",
      "Iteration 626, loss = 1.19125841\n",
      "Iteration 627, loss = 1.19116402\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.68456568\n",
      "Iteration 2, loss = 1.68337157\n",
      "Iteration 3, loss = 1.68217704\n",
      "Iteration 4, loss = 1.68098210\n",
      "Iteration 5, loss = 1.67978674\n",
      "Iteration 6, loss = 1.67859098\n",
      "Iteration 7, loss = 1.67739482\n",
      "Iteration 8, loss = 1.67619825\n",
      "Iteration 9, loss = 1.67500128\n",
      "Iteration 10, loss = 1.67380390\n",
      "Iteration 11, loss = 1.67260612\n",
      "Iteration 12, loss = 1.67140794\n",
      "Iteration 13, loss = 1.67020936\n",
      "Iteration 14, loss = 1.66901038\n",
      "Iteration 15, loss = 1.66781100\n",
      "Iteration 16, loss = 1.66661122\n",
      "Iteration 17, loss = 1.66541105\n",
      "Iteration 18, loss = 1.66421047\n",
      "Iteration 19, loss = 1.66300951\n",
      "Iteration 20, loss = 1.66180815\n",
      "Iteration 21, loss = 1.66060640\n",
      "Iteration 22, loss = 1.65940427\n",
      "Iteration 23, loss = 1.65820176\n",
      "Iteration 24, loss = 1.65699887\n",
      "Iteration 25, loss = 1.65579562\n",
      "Iteration 26, loss = 1.65459201\n",
      "Iteration 27, loss = 1.65338804\n",
      "Iteration 28, loss = 1.65218373\n",
      "Iteration 29, loss = 1.65097908\n",
      "Iteration 30, loss = 1.64977410\n",
      "Iteration 31, loss = 1.64856882\n",
      "Iteration 32, loss = 1.64736323\n",
      "Iteration 33, loss = 1.64615735\n",
      "Iteration 34, loss = 1.64495120\n",
      "Iteration 35, loss = 1.64374479\n",
      "Iteration 36, loss = 1.64253814\n",
      "Iteration 37, loss = 1.64133127\n",
      "Iteration 38, loss = 1.64012419\n",
      "Iteration 39, loss = 1.63891692\n",
      "Iteration 40, loss = 1.63770949\n",
      "Iteration 41, loss = 1.63650192\n",
      "Iteration 42, loss = 1.63529423\n",
      "Iteration 43, loss = 1.63408644\n",
      "Iteration 44, loss = 1.63287858\n",
      "Iteration 45, loss = 1.63167068\n",
      "Iteration 46, loss = 1.63046276\n",
      "Iteration 47, loss = 1.62925485\n",
      "Iteration 48, loss = 1.62804698\n",
      "Iteration 49, loss = 1.62683919\n",
      "Iteration 50, loss = 1.62563150\n",
      "Iteration 51, loss = 1.62442394\n",
      "Iteration 52, loss = 1.62321656\n",
      "Iteration 53, loss = 1.62200938\n",
      "Iteration 54, loss = 1.62080244\n",
      "Iteration 55, loss = 1.61959578\n",
      "Iteration 56, loss = 1.61838944\n",
      "Iteration 57, loss = 1.61718346\n",
      "Iteration 58, loss = 1.61597788\n",
      "Iteration 59, loss = 1.61477273\n",
      "Iteration 60, loss = 1.61356807\n",
      "Iteration 61, loss = 1.61236393\n",
      "Iteration 62, loss = 1.61116036\n",
      "Iteration 63, loss = 1.60995740\n",
      "Iteration 64, loss = 1.60875511\n",
      "Iteration 65, loss = 1.60755352\n",
      "Iteration 66, loss = 1.60635269\n",
      "Iteration 67, loss = 1.60515267\n",
      "Iteration 68, loss = 1.60395350\n",
      "Iteration 69, loss = 1.60275524\n",
      "Iteration 70, loss = 1.60155794\n",
      "Iteration 71, loss = 1.60036166\n",
      "Iteration 72, loss = 1.59916643\n",
      "Iteration 73, loss = 1.59797233\n",
      "Iteration 74, loss = 1.59677940\n",
      "Iteration 75, loss = 1.59558770\n",
      "Iteration 76, loss = 1.59439729\n",
      "Iteration 77, loss = 1.59320821\n",
      "Iteration 78, loss = 1.59202054\n",
      "Iteration 79, loss = 1.59083433\n",
      "Iteration 80, loss = 1.58964964\n",
      "Iteration 81, loss = 1.58846652\n",
      "Iteration 82, loss = 1.58727381\n",
      "Iteration 83, loss = 1.58602614\n",
      "Iteration 84, loss = 1.58476762\n",
      "Iteration 85, loss = 1.58350156\n",
      "Iteration 86, loss = 1.58222996\n",
      "Iteration 87, loss = 1.58095424\n",
      "Iteration 88, loss = 1.57967550\n",
      "Iteration 89, loss = 1.57839461\n",
      "Iteration 90, loss = 1.57711228\n",
      "Iteration 91, loss = 1.57582913\n",
      "Iteration 92, loss = 1.57454565\n",
      "Iteration 93, loss = 1.57326228\n",
      "Iteration 94, loss = 1.57197939\n",
      "Iteration 95, loss = 1.57069731\n",
      "Iteration 96, loss = 1.56941634\n",
      "Iteration 97, loss = 1.56813672\n",
      "Iteration 98, loss = 1.56685869\n",
      "Iteration 99, loss = 1.56558246\n",
      "Iteration 100, loss = 1.56430820\n",
      "Iteration 101, loss = 1.56303608\n",
      "Iteration 102, loss = 1.56176628\n",
      "Iteration 103, loss = 1.56049891\n",
      "Iteration 104, loss = 1.55923413\n",
      "Iteration 105, loss = 1.55797205\n",
      "Iteration 106, loss = 1.55671279\n",
      "Iteration 107, loss = 1.55545645\n",
      "Iteration 108, loss = 1.55420315\n",
      "Iteration 109, loss = 1.55295297\n",
      "Iteration 110, loss = 1.55170602\n",
      "Iteration 111, loss = 1.55046238\n",
      "Iteration 112, loss = 1.54922214\n",
      "Iteration 113, loss = 1.54798537\n",
      "Iteration 114, loss = 1.54675217\n",
      "Iteration 115, loss = 1.54552260\n",
      "Iteration 116, loss = 1.54429673\n",
      "Iteration 117, loss = 1.54307465\n",
      "Iteration 118, loss = 1.54185641\n",
      "Iteration 119, loss = 1.54063136\n",
      "Iteration 120, loss = 1.53939917\n",
      "Iteration 121, loss = 1.53816846\n",
      "Iteration 122, loss = 1.53693967\n",
      "Iteration 123, loss = 1.53571319\n",
      "Iteration 124, loss = 1.53448934\n",
      "Iteration 125, loss = 1.53326841\n",
      "Iteration 126, loss = 1.53205064\n",
      "Iteration 127, loss = 1.53083627\n",
      "Iteration 128, loss = 1.52962548\n",
      "Iteration 129, loss = 1.52841845\n",
      "Iteration 130, loss = 1.52721534\n",
      "Iteration 131, loss = 1.52601628\n",
      "Iteration 132, loss = 1.52482139\n",
      "Iteration 133, loss = 1.52363080\n",
      "Iteration 134, loss = 1.52244458\n",
      "Iteration 135, loss = 1.52126285\n",
      "Iteration 136, loss = 1.52008567\n",
      "Iteration 137, loss = 1.51891311\n",
      "Iteration 138, loss = 1.51774525\n",
      "Iteration 139, loss = 1.51658213\n",
      "Iteration 140, loss = 1.51542382\n",
      "Iteration 141, loss = 1.51427035\n",
      "Iteration 142, loss = 1.51312177\n",
      "Iteration 143, loss = 1.51197811\n",
      "Iteration 144, loss = 1.51083942\n",
      "Iteration 145, loss = 1.50970570\n",
      "Iteration 146, loss = 1.50857700\n",
      "Iteration 147, loss = 1.50745333\n",
      "Iteration 148, loss = 1.50633471\n",
      "Iteration 149, loss = 1.50522116\n",
      "Iteration 150, loss = 1.50411268\n",
      "Iteration 151, loss = 1.50300929\n",
      "Iteration 152, loss = 1.50191100\n",
      "Iteration 153, loss = 1.50081780\n",
      "Iteration 154, loss = 1.49972971\n",
      "Iteration 155, loss = 1.49864671\n",
      "Iteration 156, loss = 1.49756882\n",
      "Iteration 157, loss = 1.49649603\n",
      "Iteration 158, loss = 1.49542832\n",
      "Iteration 159, loss = 1.49436570\n",
      "Iteration 160, loss = 1.49330816\n",
      "Iteration 161, loss = 1.49225568\n",
      "Iteration 162, loss = 1.49120825\n",
      "Iteration 163, loss = 1.49016586\n",
      "Iteration 164, loss = 1.48912849\n",
      "Iteration 165, loss = 1.48809613\n",
      "Iteration 166, loss = 1.48706876\n",
      "Iteration 167, loss = 1.48604635\n",
      "Iteration 168, loss = 1.48502890\n",
      "Iteration 169, loss = 1.48401638\n",
      "Iteration 170, loss = 1.48300876\n",
      "Iteration 171, loss = 1.48200603\n",
      "Iteration 172, loss = 1.48100815\n",
      "Iteration 173, loss = 1.48001511\n",
      "Iteration 174, loss = 1.47902688\n",
      "Iteration 175, loss = 1.47804342\n",
      "Iteration 176, loss = 1.47706472\n",
      "Iteration 177, loss = 1.47609075\n",
      "Iteration 178, loss = 1.47512147\n",
      "Iteration 179, loss = 1.47415686\n",
      "Iteration 180, loss = 1.47319689\n",
      "Iteration 181, loss = 1.47224153\n",
      "Iteration 182, loss = 1.47129074\n",
      "Iteration 183, loss = 1.47034449\n",
      "Iteration 184, loss = 1.46940276\n",
      "Iteration 185, loss = 1.46846550\n",
      "Iteration 186, loss = 1.46753270\n",
      "Iteration 187, loss = 1.46660430\n",
      "Iteration 188, loss = 1.46568029\n",
      "Iteration 189, loss = 1.46476063\n",
      "Iteration 190, loss = 1.46384528\n",
      "Iteration 191, loss = 1.46293421\n",
      "Iteration 192, loss = 1.46202739\n",
      "Iteration 193, loss = 1.46112478\n",
      "Iteration 194, loss = 1.46022635\n",
      "Iteration 195, loss = 1.45933206\n",
      "Iteration 196, loss = 1.45844188\n",
      "Iteration 197, loss = 1.45755577\n",
      "Iteration 198, loss = 1.45667371\n",
      "Iteration 199, loss = 1.45579565\n",
      "Iteration 200, loss = 1.45492156\n",
      "Iteration 201, loss = 1.45405141\n",
      "Iteration 202, loss = 1.45318516\n",
      "Iteration 203, loss = 1.45232278\n",
      "Iteration 204, loss = 1.45146424\n",
      "Iteration 205, loss = 1.45060950\n",
      "Iteration 206, loss = 1.44975852\n",
      "Iteration 207, loss = 1.44891128\n",
      "Iteration 208, loss = 1.44806773\n",
      "Iteration 209, loss = 1.44722785\n",
      "Iteration 210, loss = 1.44639161\n",
      "Iteration 211, loss = 1.44555896\n",
      "Iteration 212, loss = 1.44471586\n",
      "Iteration 213, loss = 1.44384731\n",
      "Iteration 214, loss = 1.44297776\n",
      "Iteration 215, loss = 1.44210772\n",
      "Iteration 216, loss = 1.44123769\n",
      "Iteration 217, loss = 1.44036809\n",
      "Iteration 218, loss = 1.43949927\n",
      "Iteration 219, loss = 1.43863158\n",
      "Iteration 220, loss = 1.43776530\n",
      "Iteration 221, loss = 1.43690067\n",
      "Iteration 222, loss = 1.43603793\n",
      "Iteration 223, loss = 1.43517726\n",
      "Iteration 224, loss = 1.43431882\n",
      "Iteration 225, loss = 1.43346277\n",
      "Iteration 226, loss = 1.43260923\n",
      "Iteration 227, loss = 1.43175830\n",
      "Iteration 228, loss = 1.43091008\n",
      "Iteration 229, loss = 1.43006464\n",
      "Iteration 230, loss = 1.42922206\n",
      "Iteration 231, loss = 1.42838238\n",
      "Iteration 232, loss = 1.42754566\n",
      "Iteration 233, loss = 1.42671192\n",
      "Iteration 234, loss = 1.42588120\n",
      "Iteration 235, loss = 1.42505351\n",
      "Iteration 236, loss = 1.42422888\n",
      "Iteration 237, loss = 1.42340732\n",
      "Iteration 238, loss = 1.42258882\n",
      "Iteration 239, loss = 1.42177340\n",
      "Iteration 240, loss = 1.42096105\n",
      "Iteration 241, loss = 1.42015631\n",
      "Iteration 242, loss = 1.41937912\n",
      "Iteration 243, loss = 1.41860661\n",
      "Iteration 244, loss = 1.41777398\n",
      "Iteration 245, loss = 1.41693097\n",
      "Iteration 246, loss = 1.41608045\n",
      "Iteration 247, loss = 1.41522402\n",
      "Iteration 248, loss = 1.41436303\n",
      "Iteration 249, loss = 1.41349869\n",
      "Iteration 250, loss = 1.41263204\n",
      "Iteration 251, loss = 1.41176398\n",
      "Iteration 252, loss = 1.41092643\n",
      "Iteration 253, loss = 1.41013896\n",
      "Iteration 254, loss = 1.40935762\n",
      "Iteration 255, loss = 1.40858211\n",
      "Iteration 256, loss = 1.40781216\n",
      "Iteration 257, loss = 1.40704752\n",
      "Iteration 258, loss = 1.40628797\n",
      "Iteration 259, loss = 1.40553331\n",
      "Iteration 260, loss = 1.40478335\n",
      "Iteration 261, loss = 1.40403791\n",
      "Iteration 262, loss = 1.40329684\n",
      "Iteration 263, loss = 1.40256000\n",
      "Iteration 264, loss = 1.40182155\n",
      "Iteration 265, loss = 1.40105394\n",
      "Iteration 266, loss = 1.40028499\n",
      "Iteration 267, loss = 1.39951551\n",
      "Iteration 268, loss = 1.39874679\n",
      "Iteration 269, loss = 1.39797917\n",
      "Iteration 270, loss = 1.39721335\n",
      "Iteration 271, loss = 1.39645281\n",
      "Iteration 272, loss = 1.39569422\n",
      "Iteration 273, loss = 1.39493778\n",
      "Iteration 274, loss = 1.39418365\n",
      "Iteration 275, loss = 1.39343198\n",
      "Iteration 276, loss = 1.39268288\n",
      "Iteration 277, loss = 1.39193646\n",
      "Iteration 278, loss = 1.39119281\n",
      "Iteration 279, loss = 1.39045200\n",
      "Iteration 280, loss = 1.38971409\n",
      "Iteration 281, loss = 1.38897915\n",
      "Iteration 282, loss = 1.38824719\n",
      "Iteration 283, loss = 1.38751827\n",
      "Iteration 284, loss = 1.38679677\n",
      "Iteration 285, loss = 1.38612364\n",
      "Iteration 286, loss = 1.38545520\n",
      "Iteration 287, loss = 1.38483153\n",
      "Iteration 288, loss = 1.38421702\n",
      "Iteration 289, loss = 1.38360204\n",
      "Iteration 290, loss = 1.38298668\n",
      "Iteration 291, loss = 1.38237103\n",
      "Iteration 292, loss = 1.38175521\n",
      "Iteration 293, loss = 1.38114277\n",
      "Iteration 294, loss = 1.38053042\n",
      "Iteration 295, loss = 1.37991751\n",
      "Iteration 296, loss = 1.37930424\n",
      "Iteration 297, loss = 1.37869080\n",
      "Iteration 298, loss = 1.37807738\n",
      "Iteration 299, loss = 1.37746413\n",
      "Iteration 300, loss = 1.37685121\n",
      "Iteration 301, loss = 1.37623876\n",
      "Iteration 302, loss = 1.37562692\n",
      "Iteration 303, loss = 1.37501580\n",
      "Iteration 304, loss = 1.37440550\n",
      "Iteration 305, loss = 1.37379612\n",
      "Iteration 306, loss = 1.37318774\n",
      "Iteration 307, loss = 1.37258044\n",
      "Iteration 308, loss = 1.37197429\n",
      "Iteration 309, loss = 1.37136935\n",
      "Iteration 310, loss = 1.37076566\n",
      "Iteration 311, loss = 1.37016465\n",
      "Iteration 312, loss = 1.36956597\n",
      "Iteration 313, loss = 1.36896872\n",
      "Iteration 314, loss = 1.36837289\n",
      "Iteration 315, loss = 1.36778100\n",
      "Iteration 316, loss = 1.36720462\n",
      "Iteration 317, loss = 1.36662744\n",
      "Iteration 318, loss = 1.36604968\n",
      "Iteration 319, loss = 1.36547153\n",
      "Iteration 320, loss = 1.36489318\n",
      "Iteration 321, loss = 1.36431481\n",
      "Iteration 322, loss = 1.36373658\n",
      "Iteration 323, loss = 1.36315863\n",
      "Iteration 324, loss = 1.36258324\n",
      "Iteration 325, loss = 1.36200931\n",
      "Iteration 326, loss = 1.36143641\n",
      "Iteration 327, loss = 1.36086866\n",
      "Iteration 328, loss = 1.36030660\n",
      "Iteration 329, loss = 1.35974410\n",
      "Iteration 330, loss = 1.35918134\n",
      "Iteration 331, loss = 1.35861847\n",
      "Iteration 332, loss = 1.35805564\n",
      "Iteration 333, loss = 1.35749299\n",
      "Iteration 334, loss = 1.35693650\n",
      "Iteration 335, loss = 1.35638276\n",
      "Iteration 336, loss = 1.35582942\n",
      "Iteration 337, loss = 1.35527658\n",
      "Iteration 338, loss = 1.35472434\n",
      "Iteration 339, loss = 1.35417279\n",
      "Iteration 340, loss = 1.35362201\n",
      "Iteration 341, loss = 1.35307208\n",
      "Iteration 342, loss = 1.35252648\n",
      "Iteration 343, loss = 1.35198354\n",
      "Iteration 344, loss = 1.35144015\n",
      "Iteration 345, loss = 1.35089649\n",
      "Iteration 346, loss = 1.35035297\n",
      "Iteration 347, loss = 1.34981455\n",
      "Iteration 348, loss = 1.34927669\n",
      "Iteration 349, loss = 1.34873948\n",
      "Iteration 350, loss = 1.34822681\n",
      "Iteration 351, loss = 1.34769339\n",
      "Iteration 352, loss = 1.34716127\n",
      "Iteration 353, loss = 1.34662997\n",
      "Iteration 354, loss = 1.34610188\n",
      "Iteration 355, loss = 1.34557414\n",
      "Iteration 356, loss = 1.34504673\n",
      "Iteration 357, loss = 1.34452098\n",
      "Iteration 358, loss = 1.34396899\n",
      "Iteration 359, loss = 1.34340746\n",
      "Iteration 360, loss = 1.34283791\n",
      "Iteration 361, loss = 1.34226155\n",
      "Iteration 362, loss = 1.34167946\n",
      "Iteration 363, loss = 1.34109261\n",
      "Iteration 364, loss = 1.34050190\n",
      "Iteration 365, loss = 1.33991005\n",
      "Iteration 366, loss = 1.33931658\n",
      "Iteration 367, loss = 1.33872159\n",
      "Iteration 368, loss = 1.33812559\n",
      "Iteration 369, loss = 1.33748177\n",
      "Iteration 370, loss = 1.33680839\n",
      "Iteration 371, loss = 1.33612546\n",
      "Iteration 372, loss = 1.33543290\n",
      "Iteration 373, loss = 1.33473818\n",
      "Iteration 374, loss = 1.33396627\n",
      "Iteration 375, loss = 1.33318154\n",
      "Iteration 376, loss = 1.33238727\n",
      "Iteration 377, loss = 1.33158789\n",
      "Iteration 378, loss = 1.33078564\n",
      "Iteration 379, loss = 1.32998209\n",
      "Iteration 380, loss = 1.32917746\n",
      "Iteration 381, loss = 1.32837315\n",
      "Iteration 382, loss = 1.32757119\n",
      "Iteration 383, loss = 1.32677212\n",
      "Iteration 384, loss = 1.32597643\n",
      "Iteration 385, loss = 1.32518021\n",
      "Iteration 386, loss = 1.32436062\n",
      "Iteration 387, loss = 1.32360580\n",
      "Iteration 388, loss = 1.32286125\n",
      "Iteration 389, loss = 1.32214638\n",
      "Iteration 390, loss = 1.32148644\n",
      "Iteration 391, loss = 1.32083781\n",
      "Iteration 392, loss = 1.32019882\n",
      "Iteration 393, loss = 1.31956885\n",
      "Iteration 394, loss = 1.31894732\n",
      "Iteration 395, loss = 1.31832710\n",
      "Iteration 396, loss = 1.31764252\n",
      "Iteration 397, loss = 1.31691500\n",
      "Iteration 398, loss = 1.31606319\n",
      "Iteration 399, loss = 1.31519399\n",
      "Iteration 400, loss = 1.31431159\n",
      "Iteration 401, loss = 1.31342371\n",
      "Iteration 402, loss = 1.31260028\n",
      "Iteration 403, loss = 1.31178255\n",
      "Iteration 404, loss = 1.31096968\n",
      "Iteration 405, loss = 1.31008919\n",
      "Iteration 406, loss = 1.30920221\n",
      "Iteration 407, loss = 1.30832243\n",
      "Iteration 408, loss = 1.30750246\n",
      "Iteration 409, loss = 1.30669024\n",
      "Iteration 410, loss = 1.30588606\n",
      "Iteration 411, loss = 1.30509009\n",
      "Iteration 412, loss = 1.30430244\n",
      "Iteration 413, loss = 1.30352316\n",
      "Iteration 414, loss = 1.30276041\n",
      "Iteration 415, loss = 1.30202868\n",
      "Iteration 416, loss = 1.30130572\n",
      "Iteration 417, loss = 1.30059118\n",
      "Iteration 418, loss = 1.29983720\n",
      "Iteration 419, loss = 1.29909894\n",
      "Iteration 420, loss = 1.29838011\n",
      "Iteration 421, loss = 1.29769611\n",
      "Iteration 422, loss = 1.29704697\n",
      "Iteration 423, loss = 1.29640346\n",
      "Iteration 424, loss = 1.29580392\n",
      "Iteration 425, loss = 1.29521725\n",
      "Iteration 426, loss = 1.29463825\n",
      "Iteration 427, loss = 1.29406632\n",
      "Iteration 428, loss = 1.29350093\n",
      "Iteration 429, loss = 1.29294166\n",
      "Iteration 430, loss = 1.29238812\n",
      "Iteration 431, loss = 1.29183999\n",
      "Iteration 432, loss = 1.29129699\n",
      "Iteration 433, loss = 1.29076568\n",
      "Iteration 434, loss = 1.29024000\n",
      "Iteration 435, loss = 1.28971756\n",
      "Iteration 436, loss = 1.28919826\n",
      "Iteration 437, loss = 1.28868216\n",
      "Iteration 438, loss = 1.28817122\n",
      "Iteration 439, loss = 1.28766324\n",
      "Iteration 440, loss = 1.28715818\n",
      "Iteration 441, loss = 1.28665599\n",
      "Iteration 442, loss = 1.28615663\n",
      "Iteration 443, loss = 1.28566007\n",
      "Iteration 444, loss = 1.28516629\n",
      "Iteration 445, loss = 1.28467526\n",
      "Iteration 446, loss = 1.28418696\n",
      "Iteration 447, loss = 1.28370136\n",
      "Iteration 448, loss = 1.28321844\n",
      "Iteration 449, loss = 1.28273818\n",
      "Iteration 450, loss = 1.28226371\n",
      "Iteration 451, loss = 1.28179502\n",
      "Iteration 452, loss = 1.28132929\n",
      "Iteration 453, loss = 1.28086651\n",
      "Iteration 454, loss = 1.28040665\n",
      "Iteration 455, loss = 1.27994970\n",
      "Iteration 456, loss = 1.27949563\n",
      "Iteration 457, loss = 1.27904441\n",
      "Iteration 458, loss = 1.27859602\n",
      "Iteration 459, loss = 1.27815043\n",
      "Iteration 460, loss = 1.27770762\n",
      "Iteration 461, loss = 1.27726756\n",
      "Iteration 462, loss = 1.27683022\n",
      "Iteration 463, loss = 1.27639558\n",
      "Iteration 464, loss = 1.27596361\n",
      "Iteration 465, loss = 1.27553428\n",
      "Iteration 466, loss = 1.27510756\n",
      "Iteration 467, loss = 1.27468343\n",
      "Iteration 468, loss = 1.27426186\n",
      "Iteration 469, loss = 1.27384283\n",
      "Iteration 470, loss = 1.27342630\n",
      "Iteration 471, loss = 1.27301224\n",
      "Iteration 472, loss = 1.27260064\n",
      "Iteration 473, loss = 1.27219147\n",
      "Iteration 474, loss = 1.27178470\n",
      "Iteration 475, loss = 1.27138029\n",
      "Iteration 476, loss = 1.27097824\n",
      "Iteration 477, loss = 1.27057851\n",
      "Iteration 478, loss = 1.27018107\n",
      "Iteration 479, loss = 1.26978591\n",
      "Iteration 480, loss = 1.26939299\n",
      "Iteration 481, loss = 1.26900230\n",
      "Iteration 482, loss = 1.26861380\n",
      "Iteration 483, loss = 1.26822747\n",
      "Iteration 484, loss = 1.26783331\n",
      "Iteration 485, loss = 1.26743717\n",
      "Iteration 486, loss = 1.26703974\n",
      "Iteration 487, loss = 1.26662517\n",
      "Iteration 488, loss = 1.26619555\n",
      "Iteration 489, loss = 1.26575649\n",
      "Iteration 490, loss = 1.26530959\n",
      "Iteration 491, loss = 1.26486982\n",
      "Iteration 492, loss = 1.26442811\n",
      "Iteration 493, loss = 1.26398509\n",
      "Iteration 494, loss = 1.26354139\n",
      "Iteration 495, loss = 1.26309756\n",
      "Iteration 496, loss = 1.26265410\n",
      "Iteration 497, loss = 1.26221146\n",
      "Iteration 498, loss = 1.26175167\n",
      "Iteration 499, loss = 1.26124773\n",
      "Iteration 500, loss = 1.26071407\n",
      "Iteration 501, loss = 1.26014384\n",
      "Iteration 502, loss = 1.25956485\n",
      "Iteration 503, loss = 1.25899174\n",
      "Iteration 504, loss = 1.25842525\n",
      "Iteration 505, loss = 1.25782649\n",
      "Iteration 506, loss = 1.25717637\n",
      "Iteration 507, loss = 1.25651901\n",
      "Iteration 508, loss = 1.25585722\n",
      "Iteration 509, loss = 1.25514448\n",
      "Iteration 510, loss = 1.25440828\n",
      "Iteration 511, loss = 1.25366721\n",
      "Iteration 512, loss = 1.25292442\n",
      "Iteration 513, loss = 1.25210878\n",
      "Iteration 514, loss = 1.25126162\n",
      "Iteration 515, loss = 1.25040988\n",
      "Iteration 516, loss = 1.24955732\n",
      "Iteration 517, loss = 1.24876148\n",
      "Iteration 518, loss = 1.24802302\n",
      "Iteration 519, loss = 1.24729833\n",
      "Iteration 520, loss = 1.24659635\n",
      "Iteration 521, loss = 1.24592749\n",
      "Iteration 522, loss = 1.24527311\n",
      "Iteration 523, loss = 1.24456462\n",
      "Iteration 524, loss = 1.24382768\n",
      "Iteration 525, loss = 1.24309826\n",
      "Iteration 526, loss = 1.24230314\n",
      "Iteration 527, loss = 1.24147915\n",
      "Iteration 528, loss = 1.24057400\n",
      "Iteration 529, loss = 1.23956721\n",
      "Iteration 530, loss = 1.23848680\n",
      "Iteration 531, loss = 1.23740165\n",
      "Iteration 532, loss = 1.23654903\n",
      "Iteration 533, loss = 1.23561371\n",
      "Iteration 534, loss = 1.23465590\n",
      "Iteration 535, loss = 1.23371142\n",
      "Iteration 536, loss = 1.23278233\n",
      "Iteration 537, loss = 1.23187028\n",
      "Iteration 538, loss = 1.23091449\n",
      "Iteration 539, loss = 1.22990573\n",
      "Iteration 540, loss = 1.22897340\n",
      "Iteration 541, loss = 1.22813953\n",
      "Iteration 542, loss = 1.22733002\n",
      "Iteration 543, loss = 1.22654435\n",
      "Iteration 544, loss = 1.22578191\n",
      "Iteration 545, loss = 1.22504202\n",
      "Iteration 546, loss = 1.22432394\n",
      "Iteration 547, loss = 1.22362690\n",
      "Iteration 548, loss = 1.22295009\n",
      "Iteration 549, loss = 1.22213425\n",
      "Iteration 550, loss = 1.22122631\n",
      "Iteration 551, loss = 1.22033346\n",
      "Iteration 552, loss = 1.21944791\n",
      "Iteration 553, loss = 1.21857054\n",
      "Iteration 554, loss = 1.21756251\n",
      "Iteration 555, loss = 1.21654868\n",
      "Iteration 556, loss = 1.21553619\n",
      "Iteration 557, loss = 1.21452919\n",
      "Iteration 558, loss = 1.21353110\n",
      "Iteration 559, loss = 1.21254469\n",
      "Iteration 560, loss = 1.21157221\n",
      "Iteration 561, loss = 1.21061540\n",
      "Iteration 562, loss = 1.20967560\n",
      "Iteration 563, loss = 1.20875380\n",
      "Iteration 564, loss = 1.20785068\n",
      "Iteration 565, loss = 1.20696600\n",
      "Iteration 566, loss = 1.20605427\n",
      "Iteration 567, loss = 1.20501726\n",
      "Iteration 568, loss = 1.20398779\n",
      "Iteration 569, loss = 1.20296865\n",
      "Iteration 570, loss = 1.20196210\n",
      "Iteration 571, loss = 1.20096988\n",
      "Iteration 572, loss = 1.19999334\n",
      "Iteration 573, loss = 1.19903348\n",
      "Iteration 574, loss = 1.19809098\n",
      "Iteration 575, loss = 1.19716630\n",
      "Iteration 576, loss = 1.19625970\n",
      "Iteration 577, loss = 1.19537123\n",
      "Iteration 578, loss = 1.19450085\n",
      "Iteration 579, loss = 1.19364838\n",
      "Iteration 580, loss = 1.19281355\n",
      "Iteration 581, loss = 1.19200195\n",
      "Iteration 582, loss = 1.19120851\n",
      "Iteration 583, loss = 1.19043192\n",
      "Iteration 584, loss = 1.18967168\n",
      "Iteration 585, loss = 1.18892728\n",
      "Iteration 586, loss = 1.18819821\n",
      "Iteration 587, loss = 1.18748395\n",
      "Iteration 588, loss = 1.18678401\n",
      "Iteration 589, loss = 1.18609787\n",
      "Iteration 590, loss = 1.18542505\n",
      "Iteration 591, loss = 1.18476510\n",
      "Iteration 592, loss = 1.18411755\n",
      "Iteration 593, loss = 1.18348197\n",
      "Iteration 594, loss = 1.18285796\n",
      "Iteration 595, loss = 1.18224511\n",
      "Iteration 596, loss = 1.18164305\n",
      "Iteration 597, loss = 1.18105144\n",
      "Iteration 598, loss = 1.18046992\n",
      "Iteration 599, loss = 1.17989819\n",
      "Iteration 600, loss = 1.17933595\n",
      "Iteration 601, loss = 1.17878292\n",
      "Iteration 602, loss = 1.17823883\n",
      "Iteration 603, loss = 1.17770344\n",
      "Iteration 604, loss = 1.17717650\n",
      "Iteration 605, loss = 1.17665779\n",
      "Iteration 606, loss = 1.17614712\n",
      "Iteration 607, loss = 1.17564427\n",
      "Iteration 608, loss = 1.17514908\n",
      "Iteration 609, loss = 1.17466135\n",
      "Iteration 610, loss = 1.17418094\n",
      "Iteration 611, loss = 1.17370767\n",
      "Iteration 612, loss = 1.17324140\n",
      "Iteration 613, loss = 1.17278200\n",
      "Iteration 614, loss = 1.17232933\n",
      "Iteration 615, loss = 1.17188325\n",
      "Iteration 616, loss = 1.17144366\n",
      "Iteration 617, loss = 1.17101042\n",
      "Iteration 618, loss = 1.17058344\n",
      "Iteration 619, loss = 1.17016261\n",
      "Iteration 620, loss = 1.16974781\n",
      "Iteration 621, loss = 1.16933897\n",
      "Iteration 622, loss = 1.16893597\n",
      "Iteration 623, loss = 1.16853872\n",
      "Iteration 624, loss = 1.16814715\n",
      "Iteration 625, loss = 1.16776115\n",
      "Iteration 626, loss = 1.16738066\n",
      "Iteration 627, loss = 1.16700558\n",
      "Iteration 628, loss = 1.16663583\n",
      "Iteration 629, loss = 1.16627135\n",
      "Iteration 630, loss = 1.16591205\n",
      "Iteration 631, loss = 1.16555787\n",
      "Iteration 632, loss = 1.16520872\n",
      "Iteration 633, loss = 1.16486454\n",
      "Iteration 634, loss = 1.16452527\n",
      "Iteration 635, loss = 1.16419082\n",
      "Iteration 636, loss = 1.16386114\n",
      "Iteration 637, loss = 1.16353616\n",
      "Iteration 638, loss = 1.16321582\n",
      "Iteration 639, loss = 1.16290005\n",
      "Iteration 640, loss = 1.16258879\n",
      "Iteration 641, loss = 1.16228198\n",
      "Iteration 642, loss = 1.16197957\n",
      "Iteration 643, loss = 1.16168148\n",
      "Iteration 644, loss = 1.16138767\n",
      "Iteration 645, loss = 1.16109807\n",
      "Iteration 646, loss = 1.16081263\n",
      "Iteration 647, loss = 1.16053129\n",
      "Iteration 648, loss = 1.16025400\n",
      "Iteration 649, loss = 1.15998070\n",
      "Iteration 650, loss = 1.15971134\n",
      "Iteration 651, loss = 1.15944586\n",
      "Iteration 652, loss = 1.15918422\n",
      "Iteration 653, loss = 1.15892635\n",
      "Iteration 654, loss = 1.15867222\n",
      "Iteration 655, loss = 1.15842176\n",
      "Iteration 656, loss = 1.15817492\n",
      "Iteration 657, loss = 1.15793167\n",
      "Iteration 658, loss = 1.15769195\n",
      "Iteration 659, loss = 1.15745570\n",
      "Iteration 660, loss = 1.15722680\n",
      "Iteration 661, loss = 1.15700259\n",
      "Iteration 662, loss = 1.15678190\n",
      "Iteration 663, loss = 1.15656465\n",
      "Iteration 664, loss = 1.15635080\n",
      "Iteration 665, loss = 1.15614026\n",
      "Iteration 666, loss = 1.15593298\n",
      "Iteration 667, loss = 1.15572891\n",
      "Iteration 668, loss = 1.15552797\n",
      "Iteration 669, loss = 1.15533013\n",
      "Iteration 670, loss = 1.15513531\n",
      "Iteration 671, loss = 1.15494348\n",
      "Iteration 672, loss = 1.15475459\n",
      "Iteration 673, loss = 1.15456857\n",
      "Iteration 674, loss = 1.15438539\n",
      "Iteration 675, loss = 1.15420499\n",
      "Iteration 676, loss = 1.15402734\n",
      "Iteration 677, loss = 1.15385239\n",
      "Iteration 678, loss = 1.15368009\n",
      "Iteration 679, loss = 1.15351040\n",
      "Iteration 680, loss = 1.15334328\n",
      "Iteration 681, loss = 1.15317869\n",
      "Iteration 682, loss = 1.15301659\n",
      "Iteration 683, loss = 1.15285694\n",
      "Iteration 684, loss = 1.15269969\n",
      "Iteration 685, loss = 1.15254482\n",
      "Iteration 686, loss = 1.15239229\n",
      "Iteration 687, loss = 1.15224205\n",
      "Iteration 688, loss = 1.15209408\n",
      "Iteration 689, loss = 1.15194833\n",
      "Iteration 690, loss = 1.15180478\n",
      "Iteration 691, loss = 1.15166338\n",
      "Iteration 692, loss = 1.15152410\n",
      "Iteration 693, loss = 1.15138692\n",
      "Iteration 694, loss = 1.15125179\n",
      "Iteration 695, loss = 1.15111869\n",
      "Iteration 696, loss = 1.15098759\n",
      "Iteration 697, loss = 1.15085845\n",
      "Iteration 698, loss = 1.15073124\n",
      "Iteration 699, loss = 1.15060593\n",
      "Iteration 700, loss = 1.15048249\n",
      "Iteration 701, loss = 1.15036090\n",
      "Iteration 702, loss = 1.15024113\n",
      "Iteration 703, loss = 1.15012314\n",
      "Iteration 704, loss = 1.15000690\n",
      "Iteration 705, loss = 1.14989241\n",
      "Iteration 706, loss = 1.14977961\n",
      "Iteration 707, loss = 1.14966849\n",
      "Iteration 708, loss = 1.14955903\n",
      "Iteration 709, loss = 1.14945119\n",
      "Iteration 710, loss = 1.14934496\n",
      "Iteration 711, loss = 1.14924030\n",
      "Iteration 712, loss = 1.14913719\n",
      "Iteration 713, loss = 1.14903561\n",
      "Iteration 714, loss = 1.14893554\n",
      "Iteration 715, loss = 1.14883694\n",
      "Iteration 716, loss = 1.14873981\n",
      "Iteration 717, loss = 1.14864411\n",
      "Iteration 718, loss = 1.14854983\n",
      "Iteration 719, loss = 1.14845693\n",
      "Iteration 720, loss = 1.14836541\n",
      "Iteration 721, loss = 1.14827524\n",
      "Iteration 722, loss = 1.14818640\n",
      "Iteration 723, loss = 1.14809887\n",
      "Iteration 724, loss = 1.14801263\n",
      "Iteration 725, loss = 1.14792765\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.35016769\n",
      "Iteration 2, loss = 1.34897240\n",
      "Iteration 3, loss = 1.34778203\n",
      "Iteration 4, loss = 1.34662509\n",
      "Iteration 5, loss = 1.34553300\n",
      "Iteration 6, loss = 1.34445429\n",
      "Iteration 7, loss = 1.34342108\n",
      "Iteration 8, loss = 1.34239462\n",
      "Iteration 9, loss = 1.34137411\n",
      "Iteration 10, loss = 1.34035904\n",
      "Iteration 11, loss = 1.33934903\n",
      "Iteration 12, loss = 1.33834384\n",
      "Iteration 13, loss = 1.33734327\n",
      "Iteration 14, loss = 1.33634718\n",
      "Iteration 15, loss = 1.33535546\n",
      "Iteration 16, loss = 1.33436803\n",
      "Iteration 17, loss = 1.33338481\n",
      "Iteration 18, loss = 1.33240573\n",
      "Iteration 19, loss = 1.33143074\n",
      "Iteration 20, loss = 1.33045978\n",
      "Iteration 21, loss = 1.32949280\n",
      "Iteration 22, loss = 1.32852977\n",
      "Iteration 23, loss = 1.32756544\n",
      "Iteration 24, loss = 1.32660271\n",
      "Iteration 25, loss = 1.32564299\n",
      "Iteration 26, loss = 1.32468649\n",
      "Iteration 27, loss = 1.32373335\n",
      "Iteration 28, loss = 1.32279477\n",
      "Iteration 29, loss = 1.32188302\n",
      "Iteration 30, loss = 1.32097575\n",
      "Iteration 31, loss = 1.32007283\n",
      "Iteration 32, loss = 1.31917411\n",
      "Iteration 33, loss = 1.31827948\n",
      "Iteration 34, loss = 1.31738880\n",
      "Iteration 35, loss = 1.31650197\n",
      "Iteration 36, loss = 1.31561889\n",
      "Iteration 37, loss = 1.31473947\n",
      "Iteration 38, loss = 1.31386361\n",
      "Iteration 39, loss = 1.31299125\n",
      "Iteration 40, loss = 1.31212229\n",
      "Iteration 41, loss = 1.31125668\n",
      "Iteration 42, loss = 1.31039436\n",
      "Iteration 43, loss = 1.30953525\n",
      "Iteration 44, loss = 1.30867932\n",
      "Iteration 45, loss = 1.30782649\n",
      "Iteration 46, loss = 1.30697673\n",
      "Iteration 47, loss = 1.30612999\n",
      "Iteration 48, loss = 1.30528623\n",
      "Iteration 49, loss = 1.30444539\n",
      "Iteration 50, loss = 1.30360745\n",
      "Iteration 51, loss = 1.30277237\n",
      "Iteration 52, loss = 1.30194010\n",
      "Iteration 53, loss = 1.30111062\n",
      "Iteration 54, loss = 1.30028388\n",
      "Iteration 55, loss = 1.29945986\n",
      "Iteration 56, loss = 1.29863852\n",
      "Iteration 57, loss = 1.29781983\n",
      "Iteration 58, loss = 1.29700376\n",
      "Iteration 59, loss = 1.29619028\n",
      "Iteration 60, loss = 1.29537935\n",
      "Iteration 61, loss = 1.29457095\n",
      "Iteration 62, loss = 1.29376504\n",
      "Iteration 63, loss = 1.29296160\n",
      "Iteration 64, loss = 1.29216060\n",
      "Iteration 65, loss = 1.29136200\n",
      "Iteration 66, loss = 1.29056578\n",
      "Iteration 67, loss = 1.28977190\n",
      "Iteration 68, loss = 1.28898034\n",
      "Iteration 69, loss = 1.28819105\n",
      "Iteration 70, loss = 1.28740402\n",
      "Iteration 71, loss = 1.28661922\n",
      "Iteration 72, loss = 1.28583660\n",
      "Iteration 73, loss = 1.28505614\n",
      "Iteration 74, loss = 1.28427780\n",
      "Iteration 75, loss = 1.28350156\n",
      "Iteration 76, loss = 1.28272739\n",
      "Iteration 77, loss = 1.28195524\n",
      "Iteration 78, loss = 1.28117664\n",
      "Iteration 79, loss = 1.28039684\n",
      "Iteration 80, loss = 1.27962931\n",
      "Iteration 81, loss = 1.27885875\n",
      "Iteration 82, loss = 1.27808311\n",
      "Iteration 83, loss = 1.27730825\n",
      "Iteration 84, loss = 1.27653422\n",
      "Iteration 85, loss = 1.27576112\n",
      "Iteration 86, loss = 1.27498897\n",
      "Iteration 87, loss = 1.27421784\n",
      "Iteration 88, loss = 1.27344775\n",
      "Iteration 89, loss = 1.27267873\n",
      "Iteration 90, loss = 1.27191080\n",
      "Iteration 91, loss = 1.27114396\n",
      "Iteration 92, loss = 1.27037823\n",
      "Iteration 93, loss = 1.26961361\n",
      "Iteration 94, loss = 1.26885010\n",
      "Iteration 95, loss = 1.26808769\n",
      "Iteration 96, loss = 1.26732637\n",
      "Iteration 97, loss = 1.26656614\n",
      "Iteration 98, loss = 1.26580698\n",
      "Iteration 99, loss = 1.26504887\n",
      "Iteration 100, loss = 1.26429298\n",
      "Iteration 101, loss = 1.26354408\n",
      "Iteration 102, loss = 1.26278571\n",
      "Iteration 103, loss = 1.26201824\n",
      "Iteration 104, loss = 1.26124927\n",
      "Iteration 105, loss = 1.26047992\n",
      "Iteration 106, loss = 1.25971031\n",
      "Iteration 107, loss = 1.25894055\n",
      "Iteration 108, loss = 1.25817073\n",
      "Iteration 109, loss = 1.25740094\n",
      "Iteration 110, loss = 1.25663123\n",
      "Iteration 111, loss = 1.25586167\n",
      "Iteration 112, loss = 1.25509232\n",
      "Iteration 113, loss = 1.25432321\n",
      "Iteration 114, loss = 1.25355438\n",
      "Iteration 115, loss = 1.25278587\n",
      "Iteration 116, loss = 1.25201769\n",
      "Iteration 117, loss = 1.25124987\n",
      "Iteration 118, loss = 1.25048243\n",
      "Iteration 119, loss = 1.24971538\n",
      "Iteration 120, loss = 1.24894873\n",
      "Iteration 121, loss = 1.24818611\n",
      "Iteration 122, loss = 1.24743801\n",
      "Iteration 123, loss = 1.24669161\n",
      "Iteration 124, loss = 1.24594680\n",
      "Iteration 125, loss = 1.24520349\n",
      "Iteration 126, loss = 1.24446158\n",
      "Iteration 127, loss = 1.24372349\n",
      "Iteration 128, loss = 1.24299842\n",
      "Iteration 129, loss = 1.24227543\n",
      "Iteration 130, loss = 1.24155438\n",
      "Iteration 131, loss = 1.24083515\n",
      "Iteration 132, loss = 1.24011764\n",
      "Iteration 133, loss = 1.23940174\n",
      "Iteration 134, loss = 1.23868458\n",
      "Iteration 135, loss = 1.23796524\n",
      "Iteration 136, loss = 1.23724672\n",
      "Iteration 137, loss = 1.23652899\n",
      "Iteration 138, loss = 1.23581204\n",
      "Iteration 139, loss = 1.23509534\n",
      "Iteration 140, loss = 1.23436785\n",
      "Iteration 141, loss = 1.23364005\n",
      "Iteration 142, loss = 1.23291203\n",
      "Iteration 143, loss = 1.23218387\n",
      "Iteration 144, loss = 1.23145564\n",
      "Iteration 145, loss = 1.23071720\n",
      "Iteration 146, loss = 1.22997273\n",
      "Iteration 147, loss = 1.22922629\n",
      "Iteration 148, loss = 1.22847811\n",
      "Iteration 149, loss = 1.22772838\n",
      "Iteration 150, loss = 1.22696470\n",
      "Iteration 151, loss = 1.22619719\n",
      "Iteration 152, loss = 1.22542715\n",
      "Iteration 153, loss = 1.22465487\n",
      "Iteration 154, loss = 1.22388061\n",
      "Iteration 155, loss = 1.22310461\n",
      "Iteration 156, loss = 1.22232710\n",
      "Iteration 157, loss = 1.22154672\n",
      "Iteration 158, loss = 1.22075971\n",
      "Iteration 159, loss = 1.21997282\n",
      "Iteration 160, loss = 1.21917300\n",
      "Iteration 161, loss = 1.21837078\n",
      "Iteration 162, loss = 1.21756646\n",
      "Iteration 163, loss = 1.21676034\n",
      "Iteration 164, loss = 1.21595267\n",
      "Iteration 165, loss = 1.21514371\n",
      "Iteration 166, loss = 1.21433637\n",
      "Iteration 167, loss = 1.21354360\n",
      "Iteration 168, loss = 1.21276368\n",
      "Iteration 169, loss = 1.21198833\n",
      "Iteration 170, loss = 1.21121290\n",
      "Iteration 171, loss = 1.21043331\n",
      "Iteration 172, loss = 1.20965475\n",
      "Iteration 173, loss = 1.20887721\n",
      "Iteration 174, loss = 1.20810068\n",
      "Iteration 175, loss = 1.20732516\n",
      "Iteration 176, loss = 1.20655067\n",
      "Iteration 177, loss = 1.20577719\n",
      "Iteration 178, loss = 1.20499677\n",
      "Iteration 179, loss = 1.20419121\n",
      "Iteration 180, loss = 1.20337700\n",
      "Iteration 181, loss = 1.20256029\n",
      "Iteration 182, loss = 1.20174350\n",
      "Iteration 183, loss = 1.20094199\n",
      "Iteration 184, loss = 1.20014073\n",
      "Iteration 185, loss = 1.19933986\n",
      "Iteration 186, loss = 1.19853953\n",
      "Iteration 187, loss = 1.19773985\n",
      "Iteration 188, loss = 1.19694096\n",
      "Iteration 189, loss = 1.19614296\n",
      "Iteration 190, loss = 1.19533639\n",
      "Iteration 191, loss = 1.19452552\n",
      "Iteration 192, loss = 1.19371463\n",
      "Iteration 193, loss = 1.19290394\n",
      "Iteration 194, loss = 1.19209368\n",
      "Iteration 195, loss = 1.19127303\n",
      "Iteration 196, loss = 1.19044979\n",
      "Iteration 197, loss = 1.18961882\n",
      "Iteration 198, loss = 1.18877152\n",
      "Iteration 199, loss = 1.18792218\n",
      "Iteration 200, loss = 1.18707132\n",
      "Iteration 201, loss = 1.18621939\n",
      "Iteration 202, loss = 1.18536680\n",
      "Iteration 203, loss = 1.18451395\n",
      "Iteration 204, loss = 1.18365392\n",
      "Iteration 205, loss = 1.18277267\n",
      "Iteration 206, loss = 1.18186437\n",
      "Iteration 207, loss = 1.18094910\n",
      "Iteration 208, loss = 1.18002957\n",
      "Iteration 209, loss = 1.17910667\n",
      "Iteration 210, loss = 1.17818120\n",
      "Iteration 211, loss = 1.17725389\n",
      "Iteration 212, loss = 1.17632537\n",
      "Iteration 213, loss = 1.17540820\n",
      "Iteration 214, loss = 1.17450199\n",
      "Iteration 215, loss = 1.17359689\n",
      "Iteration 216, loss = 1.17269311\n",
      "Iteration 217, loss = 1.17179085\n",
      "Iteration 218, loss = 1.17089029\n",
      "Iteration 219, loss = 1.16999160\n",
      "Iteration 220, loss = 1.16909086\n",
      "Iteration 221, loss = 1.16817919\n",
      "Iteration 222, loss = 1.16726148\n",
      "Iteration 223, loss = 1.16633962\n",
      "Iteration 224, loss = 1.16544125\n",
      "Iteration 225, loss = 1.16455446\n",
      "Iteration 226, loss = 1.16367147\n",
      "Iteration 227, loss = 1.16279229\n",
      "Iteration 228, loss = 1.16191697\n",
      "Iteration 229, loss = 1.16104551\n",
      "Iteration 230, loss = 1.16016418\n",
      "Iteration 231, loss = 1.15928123\n",
      "Iteration 232, loss = 1.15840061\n",
      "Iteration 233, loss = 1.15752254\n",
      "Iteration 234, loss = 1.15662616\n",
      "Iteration 235, loss = 1.15569728\n",
      "Iteration 236, loss = 1.15475648\n",
      "Iteration 237, loss = 1.15380686\n",
      "Iteration 238, loss = 1.15283406\n",
      "Iteration 239, loss = 1.15184830\n",
      "Iteration 240, loss = 1.15085816\n",
      "Iteration 241, loss = 1.14986535\n",
      "Iteration 242, loss = 1.14887131\n",
      "Iteration 243, loss = 1.14786824\n",
      "Iteration 244, loss = 1.14685900\n",
      "Iteration 245, loss = 1.14585042\n",
      "Iteration 246, loss = 1.14484346\n",
      "Iteration 247, loss = 1.14383895\n",
      "Iteration 248, loss = 1.14283760\n",
      "Iteration 249, loss = 1.14184003\n",
      "Iteration 250, loss = 1.14084676\n",
      "Iteration 251, loss = 1.13985825\n",
      "Iteration 252, loss = 1.13887488\n",
      "Iteration 253, loss = 1.13786577\n",
      "Iteration 254, loss = 1.13687808\n",
      "Iteration 255, loss = 1.13587959\n",
      "Iteration 256, loss = 1.13489294\n",
      "Iteration 257, loss = 1.13397011\n",
      "Iteration 258, loss = 1.13302931\n",
      "Iteration 259, loss = 1.13208384\n",
      "Iteration 260, loss = 1.13114078\n",
      "Iteration 261, loss = 1.13020032\n",
      "Iteration 262, loss = 1.12926467\n",
      "Iteration 263, loss = 1.12835317\n",
      "Iteration 264, loss = 1.12744651\n",
      "Iteration 265, loss = 1.12654475\n",
      "Iteration 266, loss = 1.12563247\n",
      "Iteration 267, loss = 1.12470644\n",
      "Iteration 268, loss = 1.12373647\n",
      "Iteration 269, loss = 1.12276070\n",
      "Iteration 270, loss = 1.12178079\n",
      "Iteration 271, loss = 1.12079824\n",
      "Iteration 272, loss = 1.11981440\n",
      "Iteration 273, loss = 1.11883518\n",
      "Iteration 274, loss = 1.11787459\n",
      "Iteration 275, loss = 1.11691694\n",
      "Iteration 276, loss = 1.11596298\n",
      "Iteration 277, loss = 1.11500702\n",
      "Iteration 278, loss = 1.11405290\n",
      "Iteration 279, loss = 1.11309852\n",
      "Iteration 280, loss = 1.11212170\n",
      "Iteration 281, loss = 1.11116602\n",
      "Iteration 282, loss = 1.11025040\n",
      "Iteration 283, loss = 1.10934230\n",
      "Iteration 284, loss = 1.10844188\n",
      "Iteration 285, loss = 1.10754927\n",
      "Iteration 286, loss = 1.10666455\n",
      "Iteration 287, loss = 1.10578778\n",
      "Iteration 288, loss = 1.10491899\n",
      "Iteration 289, loss = 1.10405818\n",
      "Iteration 290, loss = 1.10320537\n",
      "Iteration 291, loss = 1.10236052\n",
      "Iteration 292, loss = 1.10152361\n",
      "Iteration 293, loss = 1.10069459\n",
      "Iteration 294, loss = 1.09987342\n",
      "Iteration 295, loss = 1.09906005\n",
      "Iteration 296, loss = 1.09825442\n",
      "Iteration 297, loss = 1.09744140\n",
      "Iteration 298, loss = 1.09662317\n",
      "Iteration 299, loss = 1.09581049\n",
      "Iteration 300, loss = 1.09500361\n",
      "Iteration 301, loss = 1.09420274\n",
      "Iteration 302, loss = 1.09340805\n",
      "Iteration 303, loss = 1.09261969\n",
      "Iteration 304, loss = 1.09183779\n",
      "Iteration 305, loss = 1.09106244\n",
      "Iteration 306, loss = 1.09029374\n",
      "Iteration 307, loss = 1.08953174\n",
      "Iteration 308, loss = 1.08877650\n",
      "Iteration 309, loss = 1.08802806\n",
      "Iteration 310, loss = 1.08731435\n",
      "Iteration 311, loss = 1.08662208\n",
      "Iteration 312, loss = 1.08593935\n",
      "Iteration 313, loss = 1.08526585\n",
      "Iteration 314, loss = 1.08460127\n",
      "Iteration 315, loss = 1.08394534\n",
      "Iteration 316, loss = 1.08329782\n",
      "Iteration 317, loss = 1.08265846\n",
      "Iteration 318, loss = 1.08202704\n",
      "Iteration 319, loss = 1.08140336\n",
      "Iteration 320, loss = 1.08078723\n",
      "Iteration 321, loss = 1.08017846\n",
      "Iteration 322, loss = 1.07957961\n",
      "Iteration 323, loss = 1.07899710\n",
      "Iteration 324, loss = 1.07842186\n",
      "Iteration 325, loss = 1.07785373\n",
      "Iteration 326, loss = 1.07729255\n",
      "Iteration 327, loss = 1.07673816\n",
      "Iteration 328, loss = 1.07619044\n",
      "Iteration 329, loss = 1.07564924\n",
      "Iteration 330, loss = 1.07511445\n",
      "Iteration 331, loss = 1.07458595\n",
      "Iteration 332, loss = 1.07406361\n",
      "Iteration 333, loss = 1.07354733\n",
      "Iteration 334, loss = 1.07304364\n",
      "Iteration 335, loss = 1.07254756\n",
      "Iteration 336, loss = 1.07204460\n",
      "Iteration 337, loss = 1.07154035\n",
      "Iteration 338, loss = 1.07104025\n",
      "Iteration 339, loss = 1.07054443\n",
      "Iteration 340, loss = 1.07005301\n",
      "Iteration 341, loss = 1.06956608\n",
      "Iteration 342, loss = 1.06908370\n",
      "Iteration 343, loss = 1.06860595\n",
      "Iteration 344, loss = 1.06813285\n",
      "Iteration 345, loss = 1.06766443\n",
      "Iteration 346, loss = 1.06720071\n",
      "Iteration 347, loss = 1.06674168\n",
      "Iteration 348, loss = 1.06628761\n",
      "Iteration 349, loss = 1.06584273\n",
      "Iteration 350, loss = 1.06540249\n",
      "Iteration 351, loss = 1.06496687\n",
      "Iteration 352, loss = 1.06453586\n",
      "Iteration 353, loss = 1.06410944\n",
      "Iteration 354, loss = 1.06368759\n",
      "Iteration 355, loss = 1.06327026\n",
      "Iteration 356, loss = 1.06285742\n",
      "Iteration 357, loss = 1.06244903\n",
      "Iteration 358, loss = 1.06204502\n",
      "Iteration 359, loss = 1.06164536\n",
      "Iteration 360, loss = 1.06124998\n",
      "Iteration 361, loss = 1.06085675\n",
      "Iteration 362, loss = 1.06046479\n",
      "Iteration 363, loss = 1.06007652\n",
      "Iteration 364, loss = 1.05969201\n",
      "Iteration 365, loss = 1.05931127\n",
      "Iteration 366, loss = 1.05893433\n",
      "Iteration 367, loss = 1.05856120\n",
      "Iteration 368, loss = 1.05819189\n",
      "Iteration 369, loss = 1.05782639\n",
      "Iteration 370, loss = 1.05746469\n",
      "Iteration 371, loss = 1.05710679\n",
      "Iteration 372, loss = 1.05675264\n",
      "Iteration 373, loss = 1.05640224\n",
      "Iteration 374, loss = 1.05605554\n",
      "Iteration 375, loss = 1.05571251\n",
      "Iteration 376, loss = 1.05537312\n",
      "Iteration 377, loss = 1.05503733\n",
      "Iteration 378, loss = 1.05470509\n",
      "Iteration 379, loss = 1.05437636\n",
      "Iteration 380, loss = 1.05405108\n",
      "Iteration 381, loss = 1.05372922\n",
      "Iteration 382, loss = 1.05341002\n",
      "Iteration 383, loss = 1.05308577\n",
      "Iteration 384, loss = 1.05276389\n",
      "Iteration 385, loss = 1.05244448\n",
      "Iteration 386, loss = 1.05212763\n",
      "Iteration 387, loss = 1.05181343\n",
      "Iteration 388, loss = 1.05150352\n",
      "Iteration 389, loss = 1.05120295\n",
      "Iteration 390, loss = 1.05088782\n",
      "Iteration 391, loss = 1.05057014\n",
      "Iteration 392, loss = 1.05025320\n",
      "Iteration 393, loss = 1.04993721\n",
      "Iteration 394, loss = 1.04962236\n",
      "Iteration 395, loss = 1.04930880\n",
      "Iteration 396, loss = 1.04899667\n",
      "Iteration 397, loss = 1.04868610\n",
      "Iteration 398, loss = 1.04837717\n",
      "Iteration 399, loss = 1.04806999\n",
      "Iteration 400, loss = 1.04776462\n",
      "Iteration 401, loss = 1.04746112\n",
      "Iteration 402, loss = 1.04715955\n",
      "Iteration 403, loss = 1.04685995\n",
      "Iteration 404, loss = 1.04655901\n",
      "Iteration 405, loss = 1.04625692\n",
      "Iteration 406, loss = 1.04595572\n",
      "Iteration 407, loss = 1.04565552\n",
      "Iteration 408, loss = 1.04535644\n",
      "Iteration 409, loss = 1.04505856\n",
      "Iteration 410, loss = 1.04476196\n",
      "Iteration 411, loss = 1.04446671\n",
      "Iteration 412, loss = 1.04417286\n",
      "Iteration 413, loss = 1.04388046\n",
      "Iteration 414, loss = 1.04358955\n",
      "Iteration 415, loss = 1.04330017\n",
      "Iteration 416, loss = 1.04301233\n",
      "Iteration 417, loss = 1.04272606\n",
      "Iteration 418, loss = 1.04244138\n",
      "Iteration 419, loss = 1.04215829\n",
      "Iteration 420, loss = 1.04187680\n",
      "Iteration 421, loss = 1.04159691\n",
      "Iteration 422, loss = 1.04131862\n",
      "Iteration 423, loss = 1.04104193\n",
      "Iteration 424, loss = 1.04076683\n",
      "Iteration 425, loss = 1.04049027\n",
      "Iteration 426, loss = 1.04021250\n",
      "Iteration 427, loss = 1.03993556\n",
      "Iteration 428, loss = 1.03965951\n",
      "Iteration 429, loss = 1.03938439\n",
      "Iteration 430, loss = 1.03911027\n",
      "Iteration 431, loss = 1.03883717\n",
      "Iteration 432, loss = 1.03856513\n",
      "Iteration 433, loss = 1.03829416\n",
      "Iteration 434, loss = 1.03802430\n",
      "Iteration 435, loss = 1.03775556\n",
      "Iteration 436, loss = 1.03748214\n",
      "Iteration 437, loss = 1.03720718\n",
      "Iteration 438, loss = 1.03693234\n",
      "Iteration 439, loss = 1.03665774\n",
      "Iteration 440, loss = 1.03638348\n",
      "Iteration 441, loss = 1.03610967\n",
      "Iteration 442, loss = 1.03583638\n",
      "Iteration 443, loss = 1.03556368\n",
      "Iteration 444, loss = 1.03529164\n",
      "Iteration 445, loss = 1.03502031\n",
      "Iteration 446, loss = 1.03474973\n",
      "Iteration 447, loss = 1.03447349\n",
      "Iteration 448, loss = 1.03419392\n",
      "Iteration 449, loss = 1.03391413\n",
      "Iteration 450, loss = 1.03363429\n",
      "Iteration 451, loss = 1.03335451\n",
      "Iteration 452, loss = 1.03307494\n",
      "Iteration 453, loss = 1.03279566\n",
      "Iteration 454, loss = 1.03251677\n",
      "Iteration 455, loss = 1.03225681\n",
      "Iteration 456, loss = 1.03201171\n",
      "Iteration 457, loss = 1.03176839\n",
      "Iteration 458, loss = 1.03152569\n",
      "Iteration 459, loss = 1.03127526\n",
      "Iteration 460, loss = 1.03102769\n",
      "Iteration 461, loss = 1.03078097\n",
      "Iteration 462, loss = 1.03053493\n",
      "Iteration 463, loss = 1.03028964\n",
      "Iteration 464, loss = 1.03004514\n",
      "Iteration 465, loss = 1.02980501\n",
      "Iteration 466, loss = 1.02957109\n",
      "Iteration 467, loss = 1.02933868\n",
      "Iteration 468, loss = 1.02910774\n",
      "Iteration 469, loss = 1.02887822\n",
      "Iteration 470, loss = 1.02865008\n",
      "Iteration 471, loss = 1.02842327\n",
      "Iteration 472, loss = 1.02819775\n",
      "Iteration 473, loss = 1.02797349\n",
      "Iteration 474, loss = 1.02775044\n",
      "Iteration 475, loss = 1.02752858\n",
      "Iteration 476, loss = 1.02730787\n",
      "Iteration 477, loss = 1.02708827\n",
      "Iteration 478, loss = 1.02686977\n",
      "Iteration 479, loss = 1.02665233\n",
      "Iteration 480, loss = 1.02643594\n",
      "Iteration 481, loss = 1.02622057\n",
      "Iteration 482, loss = 1.02600620\n",
      "Iteration 483, loss = 1.02579281\n",
      "Iteration 484, loss = 1.02558040\n",
      "Iteration 485, loss = 1.02536894\n",
      "Iteration 486, loss = 1.02515842\n",
      "Iteration 487, loss = 1.02494883\n",
      "Iteration 488, loss = 1.02474015\n",
      "Iteration 489, loss = 1.02452761\n",
      "Iteration 490, loss = 1.02430849\n",
      "Iteration 491, loss = 1.02408848\n",
      "Iteration 492, loss = 1.02386778\n",
      "Iteration 493, loss = 1.02364657\n",
      "Iteration 494, loss = 1.02342821\n",
      "Iteration 495, loss = 1.02320979\n",
      "Iteration 496, loss = 1.02299101\n",
      "Iteration 497, loss = 1.02277174\n",
      "Iteration 498, loss = 1.02255214\n",
      "Iteration 499, loss = 1.02233234\n",
      "Iteration 500, loss = 1.02211246\n",
      "Iteration 501, loss = 1.02189260\n",
      "Iteration 502, loss = 1.02167287\n",
      "Iteration 503, loss = 1.02145404\n",
      "Iteration 504, loss = 1.02123691\n",
      "Iteration 505, loss = 1.02101968\n",
      "Iteration 506, loss = 1.02080309\n",
      "Iteration 507, loss = 1.02058744\n",
      "Iteration 508, loss = 1.02037218\n",
      "Iteration 509, loss = 1.02015888\n",
      "Iteration 510, loss = 1.01994555\n",
      "Iteration 511, loss = 1.01973214\n",
      "Iteration 512, loss = 1.01951876\n",
      "Iteration 513, loss = 1.01930747\n",
      "Iteration 514, loss = 1.01909680\n",
      "Iteration 515, loss = 1.01888655\n",
      "Iteration 516, loss = 1.01867675\n",
      "Iteration 517, loss = 1.01846742\n",
      "Iteration 518, loss = 1.01825897\n",
      "Iteration 519, loss = 1.01805064\n",
      "Iteration 520, loss = 1.01785882\n",
      "Iteration 521, loss = 1.01766926\n",
      "Iteration 522, loss = 1.01747808\n",
      "Iteration 523, loss = 1.01728619\n",
      "Iteration 524, loss = 1.01709489\n",
      "Iteration 525, loss = 1.01690356\n",
      "Iteration 526, loss = 1.01671221\n",
      "Iteration 527, loss = 1.01652087\n",
      "Iteration 528, loss = 1.01632960\n",
      "Iteration 529, loss = 1.01615008\n",
      "Iteration 530, loss = 1.01598128\n",
      "Iteration 531, loss = 1.01581384\n",
      "Iteration 532, loss = 1.01564768\n",
      "Iteration 533, loss = 1.01548272\n",
      "Iteration 534, loss = 1.01531891\n",
      "Iteration 535, loss = 1.01515617\n",
      "Iteration 536, loss = 1.01499480\n",
      "Iteration 537, loss = 1.01483098\n",
      "Iteration 538, loss = 1.01466644\n",
      "Iteration 539, loss = 1.01450244\n",
      "Iteration 540, loss = 1.01433898\n",
      "Iteration 541, loss = 1.01418397\n",
      "Iteration 542, loss = 1.01402901\n",
      "Iteration 543, loss = 1.01387280\n",
      "Iteration 544, loss = 1.01371550\n",
      "Iteration 545, loss = 1.01355723\n",
      "Iteration 546, loss = 1.01339345\n",
      "Iteration 547, loss = 1.01322859\n",
      "Iteration 548, loss = 1.01306204\n",
      "Iteration 549, loss = 1.01290096\n",
      "Iteration 550, loss = 1.01274138\n",
      "Iteration 551, loss = 1.01258118\n",
      "Iteration 552, loss = 1.01242045\n",
      "Iteration 553, loss = 1.01225927\n",
      "Iteration 554, loss = 1.01209770\n",
      "Iteration 555, loss = 1.01193583\n",
      "Iteration 556, loss = 1.01177370\n",
      "Iteration 557, loss = 1.01161137\n",
      "Iteration 558, loss = 1.01144891\n",
      "Iteration 559, loss = 1.01128681\n",
      "Iteration 560, loss = 1.01112804\n",
      "Iteration 561, loss = 1.01096769\n",
      "Iteration 562, loss = 1.01080593\n",
      "Iteration 563, loss = 1.01064834\n",
      "Iteration 564, loss = 1.01049029\n",
      "Iteration 565, loss = 1.01033055\n",
      "Iteration 566, loss = 1.01017138\n",
      "Iteration 567, loss = 1.01001219\n",
      "Iteration 568, loss = 1.00985279\n",
      "Iteration 569, loss = 1.00969674\n",
      "Iteration 570, loss = 1.00953978\n",
      "Iteration 571, loss = 1.00938119\n",
      "Iteration 572, loss = 1.00922126\n",
      "Iteration 573, loss = 1.00906457\n",
      "Iteration 574, loss = 1.00890753\n",
      "Iteration 575, loss = 1.00875019\n",
      "Iteration 576, loss = 1.00859290\n",
      "Iteration 577, loss = 1.00842985\n",
      "Iteration 578, loss = 1.00826296\n",
      "Iteration 579, loss = 1.00809555\n",
      "Iteration 580, loss = 1.00792724\n",
      "Iteration 581, loss = 1.00775783\n",
      "Iteration 582, loss = 1.00758958\n",
      "Iteration 583, loss = 1.00741963\n",
      "Iteration 584, loss = 1.00724871\n",
      "Iteration 585, loss = 1.00707835\n",
      "Iteration 586, loss = 1.00690739\n",
      "Iteration 587, loss = 1.00673685\n",
      "Iteration 588, loss = 1.00656519\n",
      "Iteration 589, loss = 1.00639443\n",
      "Iteration 590, loss = 1.00622336\n",
      "Iteration 591, loss = 1.00605158\n",
      "Iteration 592, loss = 1.00587916\n",
      "Iteration 593, loss = 1.00570617\n",
      "Iteration 594, loss = 1.00553606\n",
      "Iteration 595, loss = 1.00536424\n",
      "Iteration 596, loss = 1.00519039\n",
      "Iteration 597, loss = 1.00501794\n",
      "Iteration 598, loss = 1.00487592\n",
      "Iteration 599, loss = 1.00474447\n",
      "Iteration 600, loss = 1.00461227\n",
      "Iteration 601, loss = 1.00447944\n",
      "Iteration 602, loss = 1.00434611\n",
      "Iteration 603, loss = 1.00421483\n",
      "Iteration 604, loss = 1.00408334\n",
      "Iteration 605, loss = 1.00394993\n",
      "Iteration 606, loss = 1.00381833\n",
      "Iteration 607, loss = 1.00368766\n",
      "Iteration 608, loss = 1.00355664\n",
      "Iteration 609, loss = 1.00342535\n",
      "Iteration 610, loss = 1.00329370\n",
      "Iteration 611, loss = 1.00315681\n",
      "Iteration 612, loss = 1.00302328\n",
      "Iteration 613, loss = 1.00288837\n",
      "Iteration 614, loss = 1.00275140\n",
      "Iteration 615, loss = 1.00261445\n",
      "Iteration 616, loss = 1.00248009\n",
      "Iteration 617, loss = 1.00234532\n",
      "Iteration 618, loss = 1.00221024\n",
      "Iteration 619, loss = 1.00206937\n",
      "Iteration 620, loss = 1.00192729\n",
      "Iteration 621, loss = 1.00178472\n",
      "Iteration 622, loss = 1.00164084\n",
      "Iteration 623, loss = 1.00149794\n",
      "Iteration 624, loss = 1.00135432\n",
      "Iteration 625, loss = 1.00121126\n",
      "Iteration 626, loss = 1.00106723\n",
      "Iteration 627, loss = 1.00092540\n",
      "Iteration 628, loss = 1.00078306\n",
      "Iteration 629, loss = 1.00064025\n",
      "Iteration 630, loss = 1.00049708\n",
      "Iteration 631, loss = 1.00035364\n",
      "Iteration 632, loss = 1.00021311\n",
      "Iteration 633, loss = 1.00007200\n",
      "Iteration 634, loss = 0.99992932\n",
      "Iteration 635, loss = 0.99979795\n",
      "Iteration 636, loss = 0.99967519\n",
      "Iteration 637, loss = 0.99955270\n",
      "Iteration 638, loss = 0.99943215\n",
      "Iteration 639, loss = 0.99932193\n",
      "Iteration 640, loss = 0.99921067\n",
      "Iteration 641, loss = 0.99910375\n",
      "Iteration 642, loss = 0.99899424\n",
      "Iteration 643, loss = 0.99888203\n",
      "Iteration 644, loss = 0.99876785\n",
      "Iteration 645, loss = 0.99865755\n",
      "Iteration 646, loss = 0.99854639\n",
      "Iteration 647, loss = 0.99843454\n",
      "Iteration 648, loss = 0.99832214\n",
      "Iteration 649, loss = 0.99821377\n",
      "Iteration 650, loss = 0.99810302\n",
      "Iteration 651, loss = 0.99798999\n",
      "Iteration 652, loss = 0.99787941\n",
      "Iteration 653, loss = 0.99777018\n",
      "Iteration 654, loss = 0.99766035\n",
      "Iteration 655, loss = 0.99755016\n",
      "Iteration 656, loss = 0.99743961\n",
      "Iteration 657, loss = 0.99732880\n",
      "Iteration 658, loss = 0.99721954\n",
      "Iteration 659, loss = 0.99711139\n",
      "Iteration 660, loss = 0.99700127\n",
      "Iteration 661, loss = 0.99689348\n",
      "Iteration 662, loss = 0.99678597\n",
      "Iteration 663, loss = 0.99667884\n",
      "Iteration 664, loss = 0.99657194\n",
      "Iteration 665, loss = 0.99646515\n",
      "Iteration 666, loss = 0.99635938\n",
      "Iteration 667, loss = 0.99625253\n",
      "Iteration 668, loss = 0.99614845\n",
      "Iteration 669, loss = 0.99604430\n",
      "Iteration 670, loss = 0.99593975\n",
      "Iteration 671, loss = 0.99583487\n",
      "Iteration 672, loss = 0.99572971\n",
      "Iteration 673, loss = 0.99562565\n",
      "Iteration 674, loss = 0.99552267\n",
      "Iteration 675, loss = 0.99541995\n",
      "Iteration 676, loss = 0.99531777\n",
      "Iteration 677, loss = 0.99521498\n",
      "Iteration 678, loss = 0.99511188\n",
      "Iteration 679, loss = 0.99501558\n",
      "Iteration 680, loss = 0.99492127\n",
      "Iteration 681, loss = 0.99482781\n",
      "Iteration 682, loss = 0.99473526\n",
      "Iteration 683, loss = 0.99464238\n",
      "Iteration 684, loss = 0.99454895\n",
      "Iteration 685, loss = 0.99445562\n",
      "Iteration 686, loss = 0.99436466\n",
      "Iteration 687, loss = 0.99427733\n",
      "Iteration 688, loss = 0.99418963\n",
      "Iteration 689, loss = 0.99410404\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Iteration 1, loss = 1.42403521\n",
      "Iteration 2, loss = 1.42321388\n",
      "Iteration 3, loss = 1.42239246\n",
      "Iteration 4, loss = 1.42157418\n",
      "Iteration 5, loss = 1.42075783\n",
      "Iteration 6, loss = 1.41994163\n",
      "Iteration 7, loss = 1.41911496\n",
      "Iteration 8, loss = 1.41828474\n",
      "Iteration 9, loss = 1.41745381\n",
      "Iteration 10, loss = 1.41662227\n",
      "Iteration 11, loss = 1.41579016\n",
      "Iteration 12, loss = 1.41495756\n",
      "Iteration 13, loss = 1.41412448\n",
      "Iteration 14, loss = 1.41329096\n",
      "Iteration 15, loss = 1.41245700\n",
      "Iteration 16, loss = 1.41162263\n",
      "Iteration 17, loss = 1.41078781\n",
      "Iteration 18, loss = 1.40994116\n",
      "Iteration 19, loss = 1.40908869\n",
      "Iteration 20, loss = 1.40823595\n",
      "Iteration 21, loss = 1.40738037\n",
      "Iteration 22, loss = 1.40651185\n",
      "Iteration 23, loss = 1.40564114\n",
      "Iteration 24, loss = 1.40476839\n",
      "Iteration 25, loss = 1.40389372\n",
      "Iteration 26, loss = 1.40301721\n",
      "Iteration 27, loss = 1.40213894\n",
      "Iteration 28, loss = 1.40125898\n",
      "Iteration 29, loss = 1.40037735\n",
      "Iteration 30, loss = 1.39949411\n",
      "Iteration 31, loss = 1.39860926\n",
      "Iteration 32, loss = 1.39772283\n",
      "Iteration 33, loss = 1.39683482\n",
      "Iteration 34, loss = 1.39594539\n",
      "Iteration 35, loss = 1.39505465\n",
      "Iteration 36, loss = 1.39416256\n",
      "Iteration 37, loss = 1.39326910\n",
      "Iteration 38, loss = 1.39237428\n",
      "Iteration 39, loss = 1.39148047\n",
      "Iteration 40, loss = 1.39058996\n",
      "Iteration 41, loss = 1.38969846\n",
      "Iteration 42, loss = 1.38880593\n",
      "Iteration 43, loss = 1.38791232\n",
      "Iteration 44, loss = 1.38701758\n",
      "Iteration 45, loss = 1.38612168\n",
      "Iteration 46, loss = 1.38522457\n",
      "Iteration 47, loss = 1.38437175\n",
      "Iteration 48, loss = 1.38354543\n",
      "Iteration 49, loss = 1.38271817\n",
      "Iteration 50, loss = 1.38189015\n",
      "Iteration 51, loss = 1.38106146\n",
      "Iteration 52, loss = 1.38026614\n",
      "Iteration 53, loss = 1.37948409\n",
      "Iteration 54, loss = 1.37869604\n",
      "Iteration 55, loss = 1.37790300\n",
      "Iteration 56, loss = 1.37710579\n",
      "Iteration 57, loss = 1.37630512\n",
      "Iteration 58, loss = 1.37550157\n",
      "Iteration 59, loss = 1.37469566\n",
      "Iteration 60, loss = 1.37388784\n",
      "Iteration 61, loss = 1.37307850\n",
      "Iteration 62, loss = 1.37226796\n",
      "Iteration 63, loss = 1.37145652\n",
      "Iteration 64, loss = 1.37064443\n",
      "Iteration 65, loss = 1.36983192\n",
      "Iteration 66, loss = 1.36901917\n",
      "Iteration 67, loss = 1.36820634\n",
      "Iteration 68, loss = 1.36739358\n",
      "Iteration 69, loss = 1.36658102\n",
      "Iteration 70, loss = 1.36576874\n",
      "Iteration 71, loss = 1.36495685\n",
      "Iteration 72, loss = 1.36414541\n",
      "Iteration 73, loss = 1.36333746\n",
      "Iteration 74, loss = 1.36253596\n",
      "Iteration 75, loss = 1.36173578\n",
      "Iteration 76, loss = 1.36093688\n",
      "Iteration 77, loss = 1.36013924\n",
      "Iteration 78, loss = 1.35934282\n",
      "Iteration 79, loss = 1.35854759\n",
      "Iteration 80, loss = 1.35775351\n",
      "Iteration 81, loss = 1.35696056\n",
      "Iteration 82, loss = 1.35616872\n",
      "Iteration 83, loss = 1.35537795\n",
      "Iteration 84, loss = 1.35458823\n",
      "Iteration 85, loss = 1.35379953\n",
      "Iteration 86, loss = 1.35301184\n",
      "Iteration 87, loss = 1.35222514\n",
      "Iteration 88, loss = 1.35143940\n",
      "Iteration 89, loss = 1.35065460\n",
      "Iteration 90, loss = 1.34987074\n",
      "Iteration 91, loss = 1.34908779\n",
      "Iteration 92, loss = 1.34830576\n",
      "Iteration 93, loss = 1.34752461\n",
      "Iteration 94, loss = 1.34674436\n",
      "Iteration 95, loss = 1.34596498\n",
      "Iteration 96, loss = 1.34518749\n",
      "Iteration 97, loss = 1.34441786\n",
      "Iteration 98, loss = 1.34364933\n",
      "Iteration 99, loss = 1.34288186\n",
      "Iteration 100, loss = 1.34211541\n",
      "Iteration 101, loss = 1.34134797\n",
      "Iteration 102, loss = 1.34058124\n",
      "Iteration 103, loss = 1.33981540\n",
      "Iteration 104, loss = 1.33905045\n",
      "Iteration 105, loss = 1.33828639\n",
      "Iteration 106, loss = 1.33752321\n",
      "Iteration 107, loss = 1.33676091\n",
      "Iteration 108, loss = 1.33599950\n",
      "Iteration 109, loss = 1.33523899\n",
      "Iteration 110, loss = 1.33447937\n",
      "Iteration 111, loss = 1.33372066\n",
      "Iteration 112, loss = 1.33296287\n",
      "Iteration 113, loss = 1.33220600\n",
      "Iteration 114, loss = 1.33145008\n",
      "Iteration 115, loss = 1.33069511\n",
      "Iteration 116, loss = 1.32994110\n",
      "Iteration 117, loss = 1.32918809\n",
      "Iteration 118, loss = 1.32843607\n",
      "Iteration 119, loss = 1.32768507\n",
      "Iteration 120, loss = 1.32693510\n",
      "Iteration 121, loss = 1.32618619\n",
      "Iteration 122, loss = 1.32543836\n",
      "Iteration 123, loss = 1.32469162\n",
      "Iteration 124, loss = 1.32394600\n",
      "Iteration 125, loss = 1.32320151\n",
      "Iteration 126, loss = 1.32245818\n",
      "Iteration 127, loss = 1.32171603\n",
      "Iteration 128, loss = 1.32097509\n",
      "Iteration 129, loss = 1.32023537\n",
      "Iteration 130, loss = 1.31949690\n",
      "Iteration 131, loss = 1.31876354\n",
      "Iteration 132, loss = 1.31803156\n",
      "Iteration 133, loss = 1.31730046\n",
      "Iteration 134, loss = 1.31657031\n",
      "Iteration 135, loss = 1.31586800\n",
      "Iteration 136, loss = 1.31516817\n",
      "Iteration 137, loss = 1.31446763\n",
      "Iteration 138, loss = 1.31376674\n",
      "Iteration 139, loss = 1.31306584\n",
      "Iteration 140, loss = 1.31236497\n",
      "Iteration 141, loss = 1.31166571\n",
      "Iteration 142, loss = 1.31096855\n",
      "Iteration 143, loss = 1.31027351\n",
      "Iteration 144, loss = 1.30958057\n",
      "Iteration 145, loss = 1.30888973\n",
      "Iteration 146, loss = 1.30820097\n",
      "Iteration 147, loss = 1.30751429\n",
      "Iteration 148, loss = 1.30682969\n",
      "Iteration 149, loss = 1.30614715\n",
      "Iteration 150, loss = 1.30546666\n",
      "Iteration 151, loss = 1.30478823\n",
      "Iteration 152, loss = 1.30411184\n",
      "Iteration 153, loss = 1.30343748\n",
      "Iteration 154, loss = 1.30276515\n",
      "Iteration 155, loss = 1.30209484\n",
      "Iteration 156, loss = 1.30142655\n",
      "Iteration 157, loss = 1.30076027\n",
      "Iteration 158, loss = 1.30009599\n",
      "Iteration 159, loss = 1.29943370\n",
      "Iteration 160, loss = 1.29877342\n",
      "Iteration 161, loss = 1.29811512\n",
      "Iteration 162, loss = 1.29745881\n",
      "Iteration 163, loss = 1.29680448\n",
      "Iteration 164, loss = 1.29615213\n",
      "Iteration 165, loss = 1.29550176\n",
      "Iteration 166, loss = 1.29485337\n",
      "Iteration 167, loss = 1.29420695\n",
      "Iteration 168, loss = 1.29356250\n",
      "Iteration 169, loss = 1.29292002\n",
      "Iteration 170, loss = 1.29227951\n",
      "Iteration 171, loss = 1.29164098\n",
      "Iteration 172, loss = 1.29100441\n",
      "Iteration 173, loss = 1.29036981\n",
      "Iteration 174, loss = 1.28973718\n",
      "Iteration 175, loss = 1.28910652\n",
      "Iteration 176, loss = 1.28847783\n",
      "Iteration 177, loss = 1.28785112\n",
      "Iteration 178, loss = 1.28722637\n",
      "Iteration 179, loss = 1.28660360\n",
      "Iteration 180, loss = 1.28598394\n",
      "Iteration 181, loss = 1.28536585\n",
      "Iteration 182, loss = 1.28474861\n",
      "Iteration 183, loss = 1.28413322\n",
      "Iteration 184, loss = 1.28352479\n",
      "Iteration 185, loss = 1.28291924\n",
      "Iteration 186, loss = 1.28231581\n",
      "Iteration 187, loss = 1.28171450\n",
      "Iteration 188, loss = 1.28111529\n",
      "Iteration 189, loss = 1.28051818\n",
      "Iteration 190, loss = 1.27992401\n",
      "Iteration 191, loss = 1.27933233\n",
      "Iteration 192, loss = 1.27874228\n",
      "Iteration 193, loss = 1.27815391\n",
      "Iteration 194, loss = 1.27756911\n",
      "Iteration 195, loss = 1.27698629\n",
      "Iteration 196, loss = 1.27640535\n",
      "Iteration 197, loss = 1.27582631\n",
      "Iteration 198, loss = 1.27524606\n",
      "Iteration 199, loss = 1.27466164\n",
      "Iteration 200, loss = 1.27407698\n",
      "Iteration 201, loss = 1.27349235\n",
      "Iteration 202, loss = 1.27290802\n",
      "Iteration 203, loss = 1.27232420\n",
      "Iteration 204, loss = 1.27174114\n",
      "Iteration 205, loss = 1.27115902\n",
      "Iteration 206, loss = 1.27057803\n",
      "Iteration 207, loss = 1.26999835\n",
      "Iteration 208, loss = 1.26942012\n",
      "Iteration 209, loss = 1.26884122\n",
      "Iteration 210, loss = 1.26823907\n",
      "Iteration 211, loss = 1.26763352\n",
      "Iteration 212, loss = 1.26702546\n",
      "Iteration 213, loss = 1.26641569\n",
      "Iteration 214, loss = 1.26580494\n",
      "Iteration 215, loss = 1.26519386\n",
      "Iteration 216, loss = 1.26458304\n",
      "Iteration 217, loss = 1.26397299\n",
      "Iteration 218, loss = 1.26336418\n",
      "Iteration 219, loss = 1.26275699\n",
      "Iteration 220, loss = 1.26215177\n",
      "Iteration 221, loss = 1.26154883\n",
      "Iteration 222, loss = 1.26094840\n",
      "Iteration 223, loss = 1.26035070\n",
      "Iteration 224, loss = 1.25975590\n",
      "Iteration 225, loss = 1.25916416\n",
      "Iteration 226, loss = 1.25857557\n",
      "Iteration 227, loss = 1.25799772\n",
      "Iteration 228, loss = 1.25742750\n",
      "Iteration 229, loss = 1.25685984\n",
      "Iteration 230, loss = 1.25629466\n",
      "Iteration 231, loss = 1.25573194\n",
      "Iteration 232, loss = 1.25517160\n",
      "Iteration 233, loss = 1.25461363\n",
      "Iteration 234, loss = 1.25405797\n",
      "Iteration 235, loss = 1.25350460\n",
      "Iteration 236, loss = 1.25295348\n",
      "Iteration 237, loss = 1.25237180\n",
      "Iteration 238, loss = 1.25174815\n",
      "Iteration 239, loss = 1.25111409\n",
      "Iteration 240, loss = 1.25047168\n",
      "Iteration 241, loss = 1.24982273\n",
      "Iteration 242, loss = 1.24916886\n",
      "Iteration 243, loss = 1.24851151\n",
      "Iteration 244, loss = 1.24785194\n",
      "Iteration 245, loss = 1.24719123\n",
      "Iteration 246, loss = 1.24653033\n",
      "Iteration 247, loss = 1.24587006\n",
      "Iteration 248, loss = 1.24521859\n",
      "Iteration 249, loss = 1.24467522\n",
      "Iteration 250, loss = 1.24413628\n",
      "Iteration 251, loss = 1.24360119\n",
      "Iteration 252, loss = 1.24306948\n",
      "Iteration 253, loss = 1.24254077\n",
      "Iteration 254, loss = 1.24201475\n",
      "Iteration 255, loss = 1.24149117\n",
      "Iteration 256, loss = 1.24096983\n",
      "Iteration 257, loss = 1.24043054\n",
      "Iteration 258, loss = 1.23988230\n",
      "Iteration 259, loss = 1.23933466\n",
      "Iteration 260, loss = 1.23878780\n",
      "Iteration 261, loss = 1.23822048\n",
      "Iteration 262, loss = 1.23763612\n",
      "Iteration 263, loss = 1.23704661\n",
      "Iteration 264, loss = 1.23645300\n",
      "Iteration 265, loss = 1.23585630\n",
      "Iteration 266, loss = 1.23525738\n",
      "Iteration 267, loss = 1.23465705\n",
      "Iteration 268, loss = 1.23405602\n",
      "Iteration 269, loss = 1.23345492\n",
      "Iteration 270, loss = 1.23285430\n",
      "Iteration 271, loss = 1.23221493\n",
      "Iteration 272, loss = 1.23154876\n",
      "Iteration 273, loss = 1.23087608\n",
      "Iteration 274, loss = 1.23019865\n",
      "Iteration 275, loss = 1.22951022\n",
      "Iteration 276, loss = 1.22881029\n",
      "Iteration 277, loss = 1.22810614\n",
      "Iteration 278, loss = 1.22740960\n",
      "Iteration 279, loss = 1.22672064\n",
      "Iteration 280, loss = 1.22603380\n",
      "Iteration 281, loss = 1.22534965\n",
      "Iteration 282, loss = 1.22467206\n",
      "Iteration 283, loss = 1.22401847\n",
      "Iteration 284, loss = 1.22336946\n",
      "Iteration 285, loss = 1.22270898\n",
      "Iteration 286, loss = 1.22198488\n",
      "Iteration 287, loss = 1.22126012\n",
      "Iteration 288, loss = 1.22053607\n",
      "Iteration 289, loss = 1.21981388\n",
      "Iteration 290, loss = 1.21909452\n",
      "Iteration 291, loss = 1.21831188\n",
      "Iteration 292, loss = 1.21745336\n",
      "Iteration 293, loss = 1.21655187\n",
      "Iteration 294, loss = 1.21563947\n",
      "Iteration 295, loss = 1.21472886\n",
      "Iteration 296, loss = 1.21384881\n",
      "Iteration 297, loss = 1.21297307\n",
      "Iteration 298, loss = 1.21211441\n",
      "Iteration 299, loss = 1.21126873\n",
      "Iteration 300, loss = 1.21043181\n",
      "Iteration 301, loss = 1.20960431\n",
      "Iteration 302, loss = 1.20878675\n",
      "Iteration 303, loss = 1.20797951\n",
      "Iteration 304, loss = 1.20718287\n",
      "Iteration 305, loss = 1.20639703\n",
      "Iteration 306, loss = 1.20562254\n",
      "Iteration 307, loss = 1.20487201\n",
      "Iteration 308, loss = 1.20413382\n",
      "Iteration 309, loss = 1.20340830\n",
      "Iteration 310, loss = 1.20269566\n",
      "Iteration 311, loss = 1.20199601\n",
      "Iteration 312, loss = 1.20130939\n",
      "Iteration 313, loss = 1.20063573\n",
      "Iteration 314, loss = 1.19997495\n",
      "Iteration 315, loss = 1.19933921\n",
      "Iteration 316, loss = 1.19872112\n",
      "Iteration 317, loss = 1.19811752\n",
      "Iteration 318, loss = 1.19752798\n",
      "Iteration 319, loss = 1.19695206\n",
      "Iteration 320, loss = 1.19638930\n",
      "Iteration 321, loss = 1.19583922\n",
      "Iteration 322, loss = 1.19530137\n",
      "Iteration 323, loss = 1.19477527\n",
      "Iteration 324, loss = 1.19426048\n",
      "Iteration 325, loss = 1.19375894\n",
      "Iteration 326, loss = 1.19328866\n",
      "Iteration 327, loss = 1.19282844\n",
      "Iteration 328, loss = 1.19237760\n",
      "Iteration 329, loss = 1.19193556\n",
      "Iteration 330, loss = 1.19150174\n",
      "Iteration 331, loss = 1.19107566\n",
      "Iteration 332, loss = 1.19065685\n",
      "Iteration 333, loss = 1.19024491\n",
      "Iteration 334, loss = 1.18983945\n",
      "Iteration 335, loss = 1.18942615\n",
      "Iteration 336, loss = 1.18899201\n",
      "Iteration 337, loss = 1.18854278\n",
      "Iteration 338, loss = 1.18806312\n",
      "Iteration 339, loss = 1.18757841\n",
      "Iteration 340, loss = 1.18709051\n",
      "Iteration 341, loss = 1.18660106\n",
      "Iteration 342, loss = 1.18611152\n",
      "Iteration 343, loss = 1.18562318\n",
      "Iteration 344, loss = 1.18515901\n",
      "Iteration 345, loss = 1.18470628\n",
      "Iteration 346, loss = 1.18426033\n",
      "Iteration 347, loss = 1.18382128\n",
      "Iteration 348, loss = 1.18338921\n",
      "Iteration 349, loss = 1.18296412\n",
      "Iteration 350, loss = 1.18254600\n",
      "Iteration 351, loss = 1.18213478\n",
      "Iteration 352, loss = 1.18173037\n",
      "Iteration 353, loss = 1.18133265\n",
      "Iteration 354, loss = 1.18094148\n",
      "Iteration 355, loss = 1.18055671\n",
      "Iteration 356, loss = 1.18017817\n",
      "Iteration 357, loss = 1.17980570\n",
      "Iteration 358, loss = 1.17943911\n",
      "Iteration 359, loss = 1.17907823\n",
      "Iteration 360, loss = 1.17872287\n",
      "Iteration 361, loss = 1.17837287\n",
      "Iteration 362, loss = 1.17802804\n",
      "Iteration 363, loss = 1.17768822\n",
      "Iteration 364, loss = 1.17735323\n",
      "Iteration 365, loss = 1.17702293\n",
      "Iteration 366, loss = 1.17669716\n",
      "Iteration 367, loss = 1.17637577\n",
      "Iteration 368, loss = 1.17605863\n",
      "Iteration 369, loss = 1.17574559\n",
      "Iteration 370, loss = 1.17543654\n",
      "Iteration 371, loss = 1.17513137\n",
      "Iteration 372, loss = 1.17482994\n",
      "Iteration 373, loss = 1.17453217\n",
      "Iteration 374, loss = 1.17422263\n",
      "Iteration 375, loss = 1.17388583\n",
      "Iteration 376, loss = 1.17354733\n",
      "Iteration 377, loss = 1.17320796\n",
      "Iteration 378, loss = 1.17286842\n",
      "Iteration 379, loss = 1.17252937\n",
      "Iteration 380, loss = 1.17219137\n",
      "Iteration 381, loss = 1.17185493\n",
      "Iteration 382, loss = 1.17152517\n",
      "Iteration 383, loss = 1.17121630\n",
      "Iteration 384, loss = 1.17092535\n",
      "Iteration 385, loss = 1.17063995\n",
      "Iteration 386, loss = 1.17035986\n",
      "Iteration 387, loss = 1.17008486\n",
      "Iteration 388, loss = 1.16981474\n",
      "Iteration 389, loss = 1.16954930\n",
      "Iteration 390, loss = 1.16928836\n",
      "Iteration 391, loss = 1.16907029\n",
      "Iteration 392, loss = 1.16885707\n",
      "Iteration 393, loss = 1.16864419\n",
      "Iteration 394, loss = 1.16843159\n",
      "Iteration 395, loss = 1.16821927\n",
      "Iteration 396, loss = 1.16800729\n",
      "Iteration 397, loss = 1.16779574\n",
      "Iteration 398, loss = 1.16758475\n",
      "Iteration 399, loss = 1.16737445\n",
      "Iteration 400, loss = 1.16716498\n",
      "Iteration 401, loss = 1.16695648\n",
      "Iteration 402, loss = 1.16674908\n",
      "Iteration 403, loss = 1.16654291\n",
      "Iteration 404, loss = 1.16633808\n",
      "Iteration 405, loss = 1.16613469\n",
      "Iteration 406, loss = 1.16593280\n",
      "Iteration 407, loss = 1.16573250\n",
      "Iteration 408, loss = 1.16553686\n",
      "Iteration 409, loss = 1.16534459\n",
      "Iteration 410, loss = 1.16515383\n",
      "Iteration 411, loss = 1.16496452\n",
      "Iteration 412, loss = 1.16477663\n",
      "Iteration 413, loss = 1.16459012\n",
      "Iteration 414, loss = 1.16440492\n",
      "Iteration 415, loss = 1.16422099\n",
      "Iteration 416, loss = 1.16403829\n",
      "Iteration 417, loss = 1.16385676\n",
      "Iteration 418, loss = 1.16367636\n",
      "Iteration 419, loss = 1.16349703\n",
      "Iteration 420, loss = 1.16331875\n",
      "Iteration 421, loss = 1.16314146\n",
      "Iteration 422, loss = 1.16296512\n",
      "Iteration 423, loss = 1.16279456\n",
      "Iteration 424, loss = 1.16262832\n",
      "Iteration 425, loss = 1.16246024\n",
      "Iteration 426, loss = 1.16229061\n",
      "Iteration 427, loss = 1.16211976\n",
      "Iteration 428, loss = 1.16194798\n",
      "Iteration 429, loss = 1.16177560\n",
      "Iteration 430, loss = 1.16160960\n",
      "Iteration 431, loss = 1.16144612\n",
      "Iteration 432, loss = 1.16128295\n",
      "Iteration 433, loss = 1.16112006\n",
      "Iteration 434, loss = 1.16095745\n",
      "Iteration 435, loss = 1.16079512\n",
      "Iteration 436, loss = 1.16063307\n",
      "Iteration 437, loss = 1.16047131\n",
      "Iteration 438, loss = 1.16030985\n",
      "Iteration 439, loss = 1.16014967\n",
      "Iteration 440, loss = 1.15999093\n",
      "Iteration 441, loss = 1.15983614\n",
      "Iteration 442, loss = 1.15968367\n",
      "Iteration 443, loss = 1.15953049\n",
      "Iteration 444, loss = 1.15937680\n",
      "Iteration 445, loss = 1.15922282\n",
      "Iteration 446, loss = 1.15906871\n",
      "Iteration 447, loss = 1.15891640\n",
      "Iteration 448, loss = 1.15876726\n",
      "Iteration 449, loss = 1.15861780\n",
      "Iteration 450, loss = 1.15846806\n",
      "Iteration 451, loss = 1.15831810\n",
      "Iteration 452, loss = 1.15817041\n",
      "Iteration 453, loss = 1.15802426\n",
      "Iteration 454, loss = 1.15787794\n",
      "Iteration 455, loss = 1.15773157\n",
      "Iteration 456, loss = 1.15758529\n",
      "Iteration 457, loss = 1.15744179\n",
      "Iteration 458, loss = 1.15729892\n",
      "Iteration 459, loss = 1.15715548\n",
      "Iteration 460, loss = 1.15701154\n",
      "Iteration 461, loss = 1.15687073\n",
      "Iteration 462, loss = 1.15673061\n",
      "Iteration 463, loss = 1.15659048\n",
      "Iteration 464, loss = 1.15645110\n",
      "Iteration 465, loss = 1.15631165\n",
      "Iteration 466, loss = 1.15617209\n",
      "Iteration 467, loss = 1.15603445\n",
      "Iteration 468, loss = 1.15589793\n",
      "Iteration 469, loss = 1.15576088\n",
      "Iteration 470, loss = 1.15562350\n",
      "Iteration 471, loss = 1.15548868\n",
      "Iteration 472, loss = 1.15535404\n",
      "Iteration 473, loss = 1.15521965\n",
      "Iteration 474, loss = 1.15508558\n",
      "Iteration 475, loss = 1.15495316\n",
      "Iteration 476, loss = 1.15482029\n",
      "Iteration 477, loss = 1.15468814\n",
      "Iteration 478, loss = 1.15455706\n",
      "Iteration 479, loss = 1.15442626\n",
      "Iteration 480, loss = 1.15429594\n",
      "Iteration 481, loss = 1.15416580\n",
      "Iteration 482, loss = 1.15403616\n",
      "Iteration 483, loss = 1.15390724\n",
      "Iteration 484, loss = 1.15377892\n",
      "Iteration 485, loss = 1.15365119\n",
      "Iteration 486, loss = 1.15352388\n",
      "Iteration 487, loss = 1.15339778\n",
      "Iteration 488, loss = 1.15327100\n",
      "Iteration 489, loss = 1.15314537\n",
      "Iteration 490, loss = 1.15302013\n",
      "Iteration 491, loss = 1.15289530\n",
      "Iteration 492, loss = 1.15277091\n",
      "Iteration 493, loss = 1.15264696\n",
      "Iteration 494, loss = 1.15252350\n",
      "Iteration 495, loss = 1.15240047\n",
      "Iteration 496, loss = 1.15227794\n",
      "Iteration 497, loss = 1.15215675\n",
      "Iteration 498, loss = 1.15203438\n",
      "Iteration 499, loss = 1.15191328\n",
      "Iteration 500, loss = 1.15179262\n",
      "Iteration 501, loss = 1.15167232\n",
      "Iteration 502, loss = 1.15155246\n",
      "Iteration 503, loss = 1.15143300\n",
      "Iteration 504, loss = 1.15131393\n",
      "Iteration 505, loss = 1.15119525\n",
      "Iteration 506, loss = 1.15107697\n",
      "Iteration 507, loss = 1.15095906\n",
      "Iteration 508, loss = 1.15084153\n",
      "Iteration 509, loss = 1.15072437\n",
      "Iteration 510, loss = 1.15060756\n",
      "Iteration 511, loss = 1.15049111\n",
      "Iteration 512, loss = 1.15037500\n",
      "Iteration 513, loss = 1.15025932\n",
      "Iteration 514, loss = 1.15014379\n",
      "Iteration 515, loss = 1.15002867\n",
      "Iteration 516, loss = 1.14991385\n",
      "Iteration 517, loss = 1.14979931\n",
      "Iteration 518, loss = 1.14968505\n",
      "Iteration 519, loss = 1.14957106\n",
      "Iteration 520, loss = 1.14945732\n",
      "Iteration 521, loss = 1.14934382\n",
      "Iteration 522, loss = 1.14923056\n",
      "Iteration 523, loss = 1.14911752\n",
      "Iteration 524, loss = 1.14900469\n",
      "Iteration 525, loss = 1.14889207\n",
      "Iteration 526, loss = 1.14877965\n",
      "Iteration 527, loss = 1.14866745\n",
      "Iteration 528, loss = 1.14855536\n",
      "Iteration 529, loss = 1.14844348\n",
      "Iteration 530, loss = 1.14833175\n",
      "Iteration 531, loss = 1.14822016\n",
      "Iteration 532, loss = 1.14810871\n",
      "Iteration 533, loss = 1.14799739\n",
      "Iteration 534, loss = 1.14788619\n",
      "Iteration 535, loss = 1.14777510\n",
      "Iteration 536, loss = 1.14766412\n",
      "Iteration 537, loss = 1.14755323\n",
      "Iteration 538, loss = 1.14744244\n",
      "Iteration 539, loss = 1.14733172\n",
      "Iteration 540, loss = 1.14722109\n",
      "Iteration 541, loss = 1.14711052\n",
      "Iteration 542, loss = 1.14700002\n",
      "Iteration 543, loss = 1.14688957\n",
      "Iteration 544, loss = 1.14677916\n",
      "Iteration 545, loss = 1.14666880\n",
      "Iteration 546, loss = 1.14655848\n",
      "Iteration 547, loss = 1.14644818\n",
      "Iteration 548, loss = 1.14633790\n",
      "Iteration 549, loss = 1.14622764\n",
      "Iteration 550, loss = 1.14611739\n",
      "Iteration 551, loss = 1.14600714\n",
      "Iteration 552, loss = 1.14589689\n",
      "Iteration 553, loss = 1.14578663\n",
      "Iteration 554, loss = 1.14567636\n",
      "Iteration 555, loss = 1.14556606\n",
      "Iteration 556, loss = 1.14545574\n",
      "Iteration 557, loss = 1.14535014\n",
      "Iteration 558, loss = 1.14524759\n",
      "Iteration 559, loss = 1.14514344\n",
      "Iteration 560, loss = 1.14503792\n",
      "Iteration 561, loss = 1.14493128\n",
      "Iteration 562, loss = 1.14482374\n",
      "Iteration 563, loss = 1.14471553\n",
      "Iteration 564, loss = 1.14460686\n",
      "Iteration 565, loss = 1.14449878\n",
      "Iteration 566, loss = 1.14439587\n",
      "Iteration 567, loss = 1.14429236\n",
      "Iteration 568, loss = 1.14418822\n",
      "Iteration 569, loss = 1.14408344\n",
      "Iteration 570, loss = 1.14397802\n",
      "Iteration 571, loss = 1.14387197\n",
      "Iteration 572, loss = 1.14377079\n",
      "Iteration 573, loss = 1.14366905\n",
      "Iteration 574, loss = 1.14356667\n",
      "Iteration 575, loss = 1.14346378\n",
      "Iteration 576, loss = 1.14336053\n",
      "Iteration 577, loss = 1.14325703\n",
      "Iteration 578, loss = 1.14315340\n",
      "Iteration 579, loss = 1.14305136\n",
      "Iteration 580, loss = 1.14295112\n",
      "Iteration 581, loss = 1.14284952\n",
      "Iteration 582, loss = 1.14274660\n",
      "Iteration 583, loss = 1.14264685\n",
      "Iteration 584, loss = 1.14254720\n",
      "Iteration 585, loss = 1.14244726\n",
      "Iteration 586, loss = 1.14234711\n",
      "Iteration 587, loss = 1.14224684\n",
      "Iteration 588, loss = 1.14214651\n",
      "Iteration 589, loss = 1.14204621\n",
      "Iteration 590, loss = 1.14194600\n",
      "Iteration 591, loss = 1.14184851\n",
      "Iteration 592, loss = 1.14175026\n",
      "Iteration 593, loss = 1.14164989\n",
      "Iteration 594, loss = 1.14155158\n",
      "Iteration 595, loss = 1.14145416\n",
      "Iteration 596, loss = 1.14135665\n",
      "Iteration 597, loss = 1.14125909\n",
      "Iteration 598, loss = 1.14116153\n",
      "Iteration 599, loss = 1.14106403\n",
      "Iteration 600, loss = 1.14096660\n",
      "Iteration 601, loss = 1.14086931\n",
      "Iteration 602, loss = 1.14077217\n",
      "Iteration 603, loss = 1.14067521\n",
      "Iteration 604, loss = 1.14057846\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "ScaledMLP: 0.544643 (0.251811)\n",
      "ScaledKNN: 0.475000 (0.214315)\n",
      "ScaledCART: 0.387500 (0.191505)\n",
      "ScaledNB: 0.221429 (0.111575)\n",
      "ScaledSVM: 0.558929 (0.231875)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7) # definindo uma semente global\n",
    "\n",
    "# Padronização do dataset\n",
    "pipelines = []\n",
    "pipelines.append(('ScaledMLP', Pipeline([\n",
    "    ('Scaler', StandardScaler()),\n",
    "    ('MLP', MLPClassifier(max_iter=1000, verbose = True, tol = 0.0001, hidden_layer_sizes = (2,2)))])))\n",
    "pipelines.append(('ScaledKNN', Pipeline([\n",
    "    ('Scaler', StandardScaler()),\n",
    "    ('KNN', KNeighborsClassifier())])))\n",
    "pipelines.append(('ScaledCART', Pipeline([\n",
    "    ('Scaler', StandardScaler()),\n",
    "    ('CART', DecisionTreeClassifier())])))\n",
    "pipelines.append(('ScaledNB', Pipeline([\n",
    "    ('Scaler', StandardScaler()),\n",
    "    ('NB', GaussianNB())])))\n",
    "pipelines.append(('ScaledSVM', Pipeline([\n",
    "    ('Scaler', StandardScaler()),\n",
    "    ('SVM', SVC())])))\n",
    "results = []\n",
    "names = []\n",
    "for name, model in pipelines:\n",
    "    kfold = KFold(n_splits=num_folds)\n",
    "    cv_results = cross_val_score(\n",
    "        model, X_train, Y_train, cv=kfold, scoring=scoring)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor: 0.558929 usando {'metric': 'euclidean', 'n_neighbors': 21}\n",
      "0.439286 (0.172245): {'metric': 'euclidean', 'n_neighbors': 1}\n",
      "0.419643 (0.216101): {'metric': 'euclidean', 'n_neighbors': 3}\n",
      "0.489286 (0.205815): {'metric': 'euclidean', 'n_neighbors': 5}\n",
      "0.503571 (0.227116): {'metric': 'euclidean', 'n_neighbors': 7}\n",
      "0.478571 (0.189117): {'metric': 'euclidean', 'n_neighbors': 9}\n",
      "0.489286 (0.217711): {'metric': 'euclidean', 'n_neighbors': 11}\n",
      "0.478571 (0.226131): {'metric': 'euclidean', 'n_neighbors': 13}\n",
      "0.519643 (0.248779): {'metric': 'euclidean', 'n_neighbors': 15}\n",
      "0.533929 (0.230136): {'metric': 'euclidean', 'n_neighbors': 17}\n",
      "0.533929 (0.230136): {'metric': 'euclidean', 'n_neighbors': 19}\n",
      "0.558929 (0.231875): {'metric': 'euclidean', 'n_neighbors': 21}\n",
      "0.467857 (0.168047): {'metric': 'manhattan', 'n_neighbors': 1}\n",
      "0.382143 (0.194175): {'metric': 'manhattan', 'n_neighbors': 3}\n",
      "0.489286 (0.250535): {'metric': 'manhattan', 'n_neighbors': 5}\n",
      "0.476786 (0.170543): {'metric': 'manhattan', 'n_neighbors': 7}\n",
      "0.489286 (0.176271): {'metric': 'manhattan', 'n_neighbors': 9}\n",
      "0.516071 (0.229997): {'metric': 'manhattan', 'n_neighbors': 11}\n",
      "0.478571 (0.226131): {'metric': 'manhattan', 'n_neighbors': 13}\n",
      "0.532143 (0.250611): {'metric': 'manhattan', 'n_neighbors': 15}\n",
      "0.546429 (0.231345): {'metric': 'manhattan', 'n_neighbors': 17}\n",
      "0.546429 (0.231345): {'metric': 'manhattan', 'n_neighbors': 19}\n",
      "0.546429 (0.231345): {'metric': 'manhattan', 'n_neighbors': 21}\n",
      "0.439286 (0.172245): {'metric': 'minkowski', 'n_neighbors': 1}\n",
      "0.419643 (0.216101): {'metric': 'minkowski', 'n_neighbors': 3}\n",
      "0.489286 (0.205815): {'metric': 'minkowski', 'n_neighbors': 5}\n",
      "0.503571 (0.227116): {'metric': 'minkowski', 'n_neighbors': 7}\n",
      "0.478571 (0.189117): {'metric': 'minkowski', 'n_neighbors': 9}\n",
      "0.489286 (0.217711): {'metric': 'minkowski', 'n_neighbors': 11}\n",
      "0.478571 (0.226131): {'metric': 'minkowski', 'n_neighbors': 13}\n",
      "0.519643 (0.248779): {'metric': 'minkowski', 'n_neighbors': 15}\n",
      "0.533929 (0.230136): {'metric': 'minkowski', 'n_neighbors': 17}\n",
      "0.533929 (0.230136): {'metric': 'minkowski', 'n_neighbors': 19}\n",
      "0.558929 (0.231875): {'metric': 'minkowski', 'n_neighbors': 21}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7) # definindo uma semente global\n",
    "\n",
    "# Tuning do KNN\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "rescaledX = scaler.transform(X_train)\n",
    "\n",
    "k = [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21]\n",
    "distancias = [\"euclidean\", \"manhattan\", \"minkowski\"]\n",
    "param_grid = dict(n_neighbors=k, metric=distancias)\n",
    "\n",
    "model = KNeighborsClassifier()\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Melhor: %f usando %s\" % \n",
    "    (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f): %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Melhor: 0.587500 com {'C': 1.0, 'kernel': 'sigmoid'}\n",
      "0.558929 (0.231875): {'C': 0.1, 'kernel': 'linear'}\n",
      "0.558929 (0.231875): {'C': 0.1, 'kernel': 'poly'}\n",
      "0.558929 (0.231875): {'C': 0.1, 'kernel': 'rbf'}\n",
      "0.558929 (0.231875): {'C': 0.1, 'kernel': 'sigmoid'}\n",
      "0.558929 (0.231875): {'C': 0.5, 'kernel': 'linear'}\n",
      "0.558929 (0.231875): {'C': 0.5, 'kernel': 'poly'}\n",
      "0.558929 (0.231875): {'C': 0.5, 'kernel': 'rbf'}\n",
      "0.558929 (0.231875): {'C': 0.5, 'kernel': 'sigmoid'}\n",
      "0.558929 (0.231875): {'C': 1.0, 'kernel': 'linear'}\n",
      "0.558929 (0.231875): {'C': 1.0, 'kernel': 'poly'}\n",
      "0.558929 (0.231875): {'C': 1.0, 'kernel': 'rbf'}\n",
      "0.587500 (0.203517): {'C': 1.0, 'kernel': 'sigmoid'}\n",
      "0.558929 (0.231875): {'C': 1.5, 'kernel': 'linear'}\n",
      "0.558929 (0.231875): {'C': 1.5, 'kernel': 'poly'}\n",
      "0.558929 (0.231875): {'C': 1.5, 'kernel': 'rbf'}\n",
      "0.573214 (0.218624): {'C': 1.5, 'kernel': 'sigmoid'}\n",
      "0.558929 (0.231875): {'C': 2.0, 'kernel': 'linear'}\n",
      "0.558929 (0.231875): {'C': 2.0, 'kernel': 'poly'}\n",
      "0.558929 (0.231875): {'C': 2.0, 'kernel': 'rbf'}\n",
      "0.573214 (0.218624): {'C': 2.0, 'kernel': 'sigmoid'}\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7) # definindo uma semente global\n",
    "\n",
    "# Tuning do SVM\n",
    "\n",
    "c_values = [0.1, 0.5, 1.0, 1.5, 2.0]\n",
    "kernel_values = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "param_grid = dict(C=c_values, kernel=kernel_values)\n",
    "\n",
    "model = SVC()\n",
    "kfold = KFold(n_splits=num_folds)\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    estimator=model, param_grid=param_grid, scoring=scoring, cv=kfold)\n",
    "grid_result = grid.fit(rescaledX, Y_train)\n",
    "print(\"Melhor: %f com %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f): %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 31.59992034\n",
      "Iteration 2, loss = 31.59992033\n",
      "Iteration 3, loss = 31.59992032\n",
      "Iteration 4, loss = 31.59992032\n",
      "Iteration 5, loss = 31.59992031\n",
      "Iteration 6, loss = 31.59992031\n",
      "Iteration 7, loss = 31.59992030\n",
      "Iteration 8, loss = 31.59992029\n",
      "Iteration 9, loss = 31.59992029\n",
      "Iteration 10, loss = 31.59992028\n",
      "Iteration 11, loss = 31.59992028\n",
      "Iteration 12, loss = 31.59992027\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Accuracy score =  0.10526315789473684\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.00      0.00      0.00        15\n",
      "           C       0.00      0.00      0.00         2\n",
      "           D       0.11      1.00      0.19         2\n",
      "\n",
      "    accuracy                           0.11        19\n",
      "   macro avg       0.04      0.33      0.06        19\n",
      "weighted avg       0.01      0.11      0.02        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\projeto1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\projeto1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\projeto1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7) # definindo uma semente global\n",
    "\n",
    "# Preparação do modelo\n",
    "#model = LogisticRegression(solver='newton-cg')\n",
    "#model.fit(X_train, Y_train)\n",
    "model = MLPClassifier(max_iter=1000, verbose = True, tol = 0.0001, hidden_layer_sizes = (2,2))\n",
    "model.fit(X_train,Y_train)\n",
    "# Estimativa da acurácia no conjunto de teste\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Accuracy score = \", accuracy_score(Y_test, predictions))\n",
    "\n",
    "# Matriz de confusão\n",
    "cm = confusion_matrix(Y_test, predictions)\n",
    "#labels = [\"A\", \"B\", \"C\", \"D\"]\n",
    "#cmd = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "cmd = ConfusionMatrixDisplay(cm)\n",
    "cmd.plot(values_format=\"d\")\n",
    "plt.show()\n",
    "#print(classification_report(Y_test, predictions, target_names=labels))\n",
    "print(classification_report(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score =  0.7368421052631579\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.78      0.93      0.85        15\n",
      "           C       0.00      0.00      0.00         2\n",
      "           D       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.74        19\n",
      "   macro avg       0.26      0.31      0.28        19\n",
      "weighted avg       0.61      0.74      0.67        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\projeto1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\projeto1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\projeto1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7) # definindo uma semente global\n",
    "\n",
    "# Preparação do modelo\n",
    "#model = LogisticRegression(solver='newton-cg')\n",
    "#model.fit(X_train, Y_train)\n",
    "model = SVC(C = 1.0,kernel='sigmoid')\n",
    "model.fit(X_train,Y_train)\n",
    "# Estimativa da acurácia no conjunto de teste\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Accuracy score = \", accuracy_score(Y_test, predictions))\n",
    "\n",
    "# Matriz de confusão\n",
    "cm = confusion_matrix(Y_test, predictions)\n",
    "#labels = [\"Sem diabetes\", \"Com diabetes\"]\n",
    "#cmd = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "cmd = ConfusionMatrixDisplay(cm)\n",
    "cmd.plot(values_format=\"d\")\n",
    "plt.show()\n",
    "#print(classification_report(Y_test, predictions, target_names=labels))\n",
    "print(classification_report(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score =  0.7894736842105263\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           A       0.79      1.00      0.88        15\n",
      "           C       0.00      0.00      0.00         2\n",
      "           D       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.79        19\n",
      "   macro avg       0.26      0.33      0.29        19\n",
      "weighted avg       0.62      0.79      0.70        19\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\projeto1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\projeto1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\USER\\projeto1\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(7) # definindo uma semente global\n",
    "\n",
    "# Preparação do modelo\n",
    "#model = LogisticRegression(solver='newton-cg')\n",
    "#model.fit(X_train, Y_train)\n",
    "model = KNeighborsClassifier(metric = 'euclidean', n_neighbors =21)\n",
    "model.fit(X_train,Y_train)\n",
    "# Estimativa da acurácia no conjunto de teste\n",
    "predictions = model.predict(X_test)\n",
    "print(\"Accuracy score = \", accuracy_score(Y_test, predictions))\n",
    "\n",
    "# Matriz de confusão\n",
    "cm = confusion_matrix(Y_test, predictions)\n",
    "#labels = [\"Sem diabetes\", \"Com diabetes\"]\n",
    "#cmd = ConfusionMatrixDisplay(cm, display_labels=labels)\n",
    "cmd = ConfusionMatrixDisplay(cm)\n",
    "cmd.plot(values_format=\"d\")\n",
    "plt.show()\n",
    "#print(classification_report(Y_test, predictions, target_names=labels))\n",
    "print(classification_report(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multi-class classification with Keras\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# load dataset\n",
    "dataset = pd.read_excel(\"Base_de_dados_IBP.xlsx\",sheet_name = \"classie\") #Regressão,ensemble,pycaret\n",
    "#dataset = litoral.drop('Município',axis=1)\n",
    "array = dataset.values\n",
    "\n",
    "X = array[:, 1:4].astype(float)\n",
    "Y = array[:, 4]\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(Y)\n",
    "encoded_Y = encoder.transform(Y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "Y = np_utils.to_categorical(encoded_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 20)\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Temp\\ipykernel_9488\\3417793941.py:10: DeprecationWarning: KerasClassifier is deprecated, use Sci-Keras (https://github.com/adriangb/scikeras) instead. See https://www.adriangb.com/scikeras/stable/migration.html for help migrating.\n",
      "  estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_test_function.<locals>.test_function at 0x000001620FF87A60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_test_function.<locals>.test_function at 0x00000162112D9E40> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Baseline: 45.71% (19.43%)\n"
     ]
    }
   ],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_dim=3, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu')) \n",
    "    model.add(Dense(4, activation='softmax'))\n",
    " # Compile model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    " \n",
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True)\n",
    "results = cross_val_score(estimator, X_train_scaled, y_train, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "[0 0 0 0 0 0 0 0 0 0 2 0 0 0 2 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "#print(estimator.fit(X_train_scaled,y_train))\n",
    "predictions = estimator.predict(X_test_scaled)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 0., 0., 1.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "history = estimator.fit(X_train_scaled, y_train, validation_split=0.2, epochs =10)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "plt.plot(epochs, acc, 'y', label='Training')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation')\n",
    "plt.title('Training and validation MAE')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
